\chapter{Supplemental Material} \label{sec:supplemental}
\section{Dataset construction}
The datasets IMDB, DBLP and ICEWS were preprocessed into multimodel networks for evaluation of our model.
In the following sections, we discuss the construction of these multimodal networks.

\subsection{IMDB}
IMDB dataset is originally made up of rows detailing movies and their information.
To construct the multimodal we normalize this dataset by splitting listed actors, directors and genres per movie as a separate entity.
Actors and directors are merged as person entity since both sets overlap.
Each movie is associated with a set of keywords. 
By collecting these words into a vocabulary of 80 most frequent keywords, we construct an 80-dimensional one-hot feature vector for each movie.
The Genre and Actor entities have no feature representations.
Each movie is given $[t_v, \infty]$ time range, where $t_v$ is the release data of the respective movie.
Edges are constructed as "person directed movie", "person acted in movie", and "movie has genre" pairs. 

\subsection{DBLP}
The DBLP dataset is constructed in a similar way as IMDB since the dataset consists of Papers with their respective citations, authors, and venues.
To construct the representation vector for each paper, we use pre-trained sentence transformers \cite{wangMiniLMDeepSelfAttention2020} to embed the abstract text.
Authors and venues have no features.
Each paper is given $[t_v, \infty]$ time range, where $t_v$ is the publication date of the respective movie.
Edges are constructed as "paper cites paper", "author wrote paper" and, "paper was published in venue" entity pairs.

\subsection{ICEWS}
The ICEWS datasets consists of triplets between subject, predicate, and object entities.
As ICEWS is a temporal knowledge graph, each triplet is associated with a timestamp.
We model this dataset by combining subject and object into one single entity type.
Between these entities, we create typed edges corresponding to the respective predicate.
The name of each entity is embedded into a feature vector using a pre-trained language transformer \cite{wangMiniLMDeepSelfAttention2020}.
Each edge is given $[t_v, t_v]$ time range, where $t_v$ represents the timestamp when the corresponding triplet was valid.

\section{Model parameter count analysis} \label{sec:exact_params}
In \cref{eq:param1} till \label{eq:param7} we provide a set of equations to calculate the exact number of trainable parameters for the $MGTCOM$ model.
We use parameters $d^{(l)}$ and $h$ to denote latent vector dimensionality at layer $l$ and number of attention heads respectively.
The largest number of parameters come from the Heterogeneous graph transformer layer consisting of four node type specific transformation layers ($A$, $Q$, $K$, $V$) layers and three relation type specific transformation layers ($A$, $P$, $M$). 
The attention $\operatorname{Aux-Emb}$ and $\operatorname{Feat-Lin}$ are dependent on dataset properties such as node count $|\mathcal{V}|$ and the input feature dimensionality $d^{\mathcal{X}(t)}$.
Finally, we include cluster parameters $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$ as learnable parameters, though not through gradient descent.


\begin{align}
    \operatorname{Node-Lin}^{(l)} &=  \sum_{t \in \mathcal{A}} d^{(l-1)}d^{(l)} + d^{(l)} \label{eq:param1} \\
    \operatorname{Rel-Lin}^{(l)} &= \sum_{t \in \mathcal{R}} h^{l}(d^{(l)} / h^{l})^2 + d^{(l)} \\
    \operatorname{HGT}^{(l)} &= 4 \cdot \operatorname{Node-Lin}^{(l)} + 3 \cdot \operatorname{Rel-Lin}^{(l)}\\
    \operatorname{Aux-Emb} &= d^{(0)} |\mathcal{V}|\cdot \operatorname{embed-ratio}\\
    \operatorname{Att-Lin} &= (d^{(L)}d^{(L)} + d^{(L)})h \\
    \operatorname{Feat-Lin} &= \sum_{t \in \mathcal{A}} d^{\mathcal{X}(t)}d^{(0)} + d^{(0)}\\
    \operatorname{Clus} &= \sum_{i=0}^K d^{(L)} + d^{(L)}d^{(L)}\\
    MGTCOM &= \operatorname{Aux-Emb} + \operatorname{Att-Lin} + \operatorname{Feat-Lin} + \operatorname{Clus} + \sum_{l=1}^L \operatorname{HGT}^{(l)} \label{eq:param7}
\end{align}

\section{Exact model parameters}
\input{figs/tab_config}