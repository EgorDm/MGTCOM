\chapter{Experiments} \label{sec:experiments}
In this section, we investigate the effectiveness of the proposed framework $MGTCOM$ (in \cref{sec:approach}) by evaluating its performance on auxiliary tasks related to multimodal networks.
We start by describing our experimental setup, whereafter we compare the performance of our model against baseline methods.

\section{Evaluation metrics}
There are no measures that can assess the quality of communities in multimodal networks.
Therefore, we evaluate our model component-wise by defining related auxiliary tasks. 
On a high level, these tasks evaluate the efficiency of topological and temporal node embeddings and found communities. 
The found communities shall capture important patterns in the data which are useful for further analysis.
%
In order to measure predictive performance over distinct aspects of our data, we first define the following labels for calculating performance metrics, then describe the auxiliary tasks.

\begin{itemize}[leftmargin=*]
    \item \textbf{Ground truth labels $L_y$}. 
    Various datasets include manually selected ground truth labels which capture valuable higher-order relations within data. By measuring prediction performance on this label we gauge the quality of found communities.
    \item \textbf{Node timestamps $L_\mathcal{T}$}. 
    We split the nodes evenly into snapshot labels given the timestamp of their first occurrences. This allows measuring the quality of node embeddings on temporal prediction.  
    \item \textbf{Link-based communities $L_G$}. 
    While other measures such as modularity and link prediction are well-suited for measuring the quality of node embeddings in capturing the structure of a given network, they either require community assignment or measure low-proximity similarity.
    In order to overcome this, we first identify community labels using the Louvain method~\cite{blondelFastUnfoldingCommunities2008}. Then we use those labels to assess the quality of individual node embeddings for community detection. 
    As the Louvain method greedily approximates optimal communities, we don't use this label for formal comparison.
\end{itemize}


\subsection{Classification (CF)}
In the classification experiment, we  evaluate predictive performance given task-related labels.
To elaborate, given a set of node embeddings and their respective ground truth labels, we train a logistic regression model to predict node labels. 
For the predicted node labels, we calculate the average accuracy classification measure.

\subsection{Link prediction (LP)}
In this set of experiments, we evaluate link prediction performance. 
Given a set of positive and negative node pairs, binary classification is used to predict whether an edge exists within the graph.
We use a held-out positive and randomly sampled negative sets of edges to train a logistic regression model.
The inner-product similarity between a pair of node embeddings is used as input for the model.
By repeating this process three times, the average accuracy is calculated.

\subsection{Cluster quality}
Given node embeddings and their respective labeling,  we calculate the silhouette coefficient and Davies-Bouldin index which are helpful to estimate how coherent a clustering is.
In this case, a coherent clustering indicates how well represented the correlated patterns are within the embeddings.

\begin{secDefinition}[\textbf{Davies-Bouldin Index}]\label{def:dbi}
Davies-Bouldin Index (DBI) is the ratio of the sum of the average distance to the distance between the centers of mass of the two clusters. 
In other words, it is defined as a ratio of within-cluster, to the between cluster separation. 
This measure is defined as an average over all the found clusters and is therefore also a good measure to decide how many clusters should be used (See \cref{eq:dbi_r,eq:dbi}).
The $s_i$ refers to the average distance between each point in cluster $i$ to its cluster center $\mu_i$, and $dist(\boldsymbol{\mu}_i, \boldsymbol{\mu}_j)$ refers to the distance between cluster centers $\mu_i$ and $\mu_j$.
Since we use inner-product for node similarity, we define inner-product distance as \cref{eq:dotpdist}.

\begin{align}
    dist(\textbf{Z}_i, \textbf{Z}_j) &= -\sum_{m=0}^d Z_{im}Z_{jm} \label{eq:dotpdist}\\
    R_{i j} &=\frac{s_{i}+s_{j}}{dist(\boldsymbol{\mu}_i, \boldsymbol{\mu}_j)} \label{eq:dbi_r} \\
    DBI &=\frac{1}{k} \sum_{i=1}^{K} \max _{i \neq j} R_{i j} \label{eq:dbi} \\
\end{align}
\end{secDefinition}

\subsection{Link-based Community quality}
In this experiment, we measure link-based community quality.
Girvan and Newman \cite{girvanCommunityStructureSocial2002} defined community structure as a group of nodes where inter-community connectivity is higher than intra-connectivity. 
Following this definition, they introduce a modularity measure to evaluate the quality of found communities in a given network. We make use of this measure in our empirical evaluation. Note that we use modularity (\cref{def:modularity}) to measure the quality of topological communities. 

\begin{secDefinition}[\textbf{Modularity}]\label{def:modularity}
Modularity directly measures the density of links inside a graph and is therefore computed on communities (sets of nodes) individually by weighing edges using community similarity (or exact matching). 
Calculation of modularity is done by aggregating per community $r$ for each pair of nodes $v$ and $w$ the difference between the expected connectivity $\frac{k_{v} k_{w}}{2 m}$ (expected amount of edges between the nodes) and the actual connectivity $A_{vw}$ (existence of an edge) given their degrees ($k_v$ and $k_w$). 
The final result represents the connectivity difference between the current and a random graph, as expected connectivity is determined by random rewirings. 
Because intracommunity pairs are weighted less than intercommunity pairs, the score can vary. 
See \cref{eq:modularity}, where $S_{vr}$ denotes membership of node $v$ to community $r$ (\cref{eq:mod_membership}), and $m$ represents the total edge count.

\begin{align}
    Q &=\frac{1}{2 m}\sum_{v w}\sum_{r}\left[\overbrace{A_{v w}}^{\text{Connectivity}}-\underbrace{\frac{k_{v} k_{w}}{2 m}}_{\text{Expected Connectivity}}\right] \overbrace{S_{v r} S_{w r}}^{\text{Community Similarity}} \label{eq:modularity} \\
    S_{vr} &= \begin{cases}
        1 & \mathbf{z}_v = r \\
        0 & \text{otherwise}
    \end{cases} \label{eq:mod_membership}
\end{align}
\end{secDefinition}

\subsection{Ground-truth community quality (COM)}
Similarly, to measure the quality of detected communities for specific tasks, we measure the Normalized Mutual Information Score (NMI) score given a task-based label (\cref{def:nmi}).

\begin{secDefinition}[\textbf{Normalized Mutual Information Score (NMI)}]\label{def:nmi}
Normalized Mutual Information is a popular measure used to evaluate network partitioning. 
It is a variant of a common measure in information theory called Mutual Information defined by $I(X; Y) = H(X) - H(X| Y)$ and represents a reduction in entropy $H(X)$ of variable $X$ by observing the random variable $Y$ or vice versa.
In the context of ground-truth community evaluation setting this measure is used to quantify the overlap between two sets of partitions.
The Mutual Information score for two sets of partitions $X$ and $Y$ is computed using \cref{eq:mi}, where $|X|$ is the size of set $X$, $X_i$ refers to $i$'th partition of set $X$, and $N$ is the total number of data points.
Finally, the NMI score is computed by normalizing the MI score using the arithmetic mean of entropy of respective partitions \cref{eq:nmi}.

\begin{align}
    MI(X; Y) &= \sum_{i=1}^{|X|} \sum_{j=1}^{|Y|} \frac{\left|X_{i} \cap Y_{j}\right|}{N} \log \frac{N\left|X_{i} \cap Y_{j}\right|}{\left|X_{i}\right|\left|Y_{j}\right|} \label{eq:mi} \\
    NMI(X; Y) &= \frac{MI(X; Y)}{(H(X) + H(Y)) / 2} \label{eq:nmi}
\end{align}

\end{secDefinition}

\section{Experimental setup} \label{sec:exp_setup}
As shown in \cref{tab:comparison_related_work} of \cref{sec:related_work} there are no directly comparable methods to ours in terms of features.
For a fair and coherent comparison, we define three variants of the MGTCOM model for evaluation.
In addition to the complete end-to-end model $MGTCOM$, we split our framework into a temporal model $MGTCOM^{\mathcal{T}}$ and topological model $MGTCOM^{\mathcal{E}}$, by removing $\mathcal{L}_{\mathcal{T}}$ and $\mathcal{L}_{\mathcal{E}}$ from the objective respectively.

For evaluation, we split the network edges into disjoint training (80\%), validation (10\%), and testing (10\%) sets.  
During link prediction, we exclusively use links in the respective set as positive pairs.
Negative pairs are sampled given the full set of edges.
Similarly, the clustering is computed on the training embeddings while cluster-based metrics are calculated using test and validation sets.
During the calculation of predictive metrics such as link prediction and classification, we run logistic regression three times and use the average to get an accurate measurement.

\subsection{Hyperparameters}
The hyperparameters for $MGTCOM$ model can be attributed to either network architecture, topological random walk, temporal random walk or clustering.
In \cref{sec:abl:hyperparam} we explore the sensitivity of our model to these hyperparameters.
In \cref{sec:supplemental} we display a complete overview of all the hyperparameter values used for evaluation.
The most important hyperparameters are specified below.

For primary embedding, we use two HGT layers with neighborhood sampling sizes of 8 and 4.
All the hidden dimensions are equal to the representation dimension, which is 64 ($d=64$).
For temporal and topological context sampling we use walk length $l=10$ with 10 walks per node.
Node2Vec is configured to use $q=0.5$ to favor neighborhood exploration. 
The temporal sampling window $\omega$ for ballroom walk is determined for each dataset by splitting $\mathcal{T}$ into 20 even partitions.
For the clustering module we define prior parameters as $\nu = d + 1, \kappa = 1, \alpha = 10$ and $\Sigma_{scale} = 0.05$.
We set trade-off parameters as $\beta^{\mathcal{E}} = 1, \beta^{\mathcal{T}} = 1, \beta^{C} = 0.01$. 
For max-margin loss we set $\Delta$ to $0.1$.

\subsection{Baselines} 
We use various graph embeddings and community detection algorithms as baselines, covering state-of-art developments in related fields.
For the baselines, we use the hyperparameters reported in their respective papers. 
To keep the results comparable, we use representation dimension $d=64$ throughout.

\begin{itemize}[leftmargin=*]
    \item \textbf{ComE \cite{cavallariLearningCommunityEmbedding2017}} uses Gaussian mixture model to learn homogeneous graph embeddings and cluster parameters jointly while utilizing random walk based context sampling.
    \item \textbf{GEMSEC \cite{rozemberczkiGEMSECGraphEmbedding2019}} uses random walks to learn community structure and embeddings simultaneously on homogeneous graphs.
    \item \textbf{CP-GNN \cite{luoDetectingCommunitiesHeterogeneous2021} } learns node embeddings from a heterogeneous graph by utilizing transformers and k-hop context sampling.
    \item \textbf{CTDNE \cite{nguyenContinuousTimeDynamicNetwork2018} } utilizes time-based biased random walks to learn spatio-temporal node representations from dynamic networks. 
    \item \textbf{GraphSAGE \cite{hamiltonInductiveRepresentationLearning2017} } uses k-hop neighborhood sampling to learn node embeddings from homogeneous graphs. Its unsupervised variant combines contrastive link sampling with hinge loss.  
    \item \textbf{Node2Vec \cite{groverNode2vecScalableFeature2016}} adopts biased random walk and Skip-Gram to learn node embeddings from homogeneous graphs.
\end{itemize}

\subsection{Datasets} 
\input{figs/tab_datasets.tex}

We use four widely used real-world (temporal) datasets for evaluation.
These graphs are of different types and contain information on different modalities.
We applied additional preprocessing on the IMDB, DBLP-HCN, and ICEWS datasets to include the multimodal features present in the datasets but often not included in the graph due to sparsity of temporal or content-based features. 
See \cref{tab:datasets} for a detailed comparison of node features.

\begin{itemize}[leftmargin=*]
    \item \textbf{DBLP} \cite{yangDefiningEvaluatingNetwork2012} is a citation network consisting of Authors, Papers and Venues. Aside from being heterogeneous, the dataset also contains timestamps representing paper publication dates and abstracts. There are thirteen ground-truth communities representing publication venues. The  network contains 10687 nodes and 33066 edges. This dataset includes ground truth labels.
    \item \textbf{ICEWS} \cite{garcia-duranLearningSequenceEncoders2018}
    is a temporal knowledge graph in which nodes represent entities and timestamped edges the relationship between them. We model this data as a highly heterogeneous network consisting of different types of nodes (10463 in total) connected by 915028 timestamped edges. Edges are labeled with relations. 
    \item \textbf{IMDB5000} \cite{IMDB5000Movie} network consists of Actor, Director, Movie, and Genre nodes where each Movie node type has a timestamp denoting the release date. Additionally, each actor node has a set of attributes characterizing information unique to the actor such as age and popularity, while movies have box-office data and keywords encoded as feature vectors. This network has 13560 nodes and 69058 edges. 
    \item \textbf{SocialDistancingStudents (SDS)} \cite{wangPublicSentimentGovernmental2020} represents a small part of the Twitter network around a set of hashtags related to the COVID pandemic. This heterogeneous network models connections between Users, Tweets, and Hashtags where parallel edges are possible due to relations such as tweeted, retweeted, quoted, etc. The tweet nodes contain publication date timestamps and content encoded as feature vectors. 93433 nodes and 7420366 edges are included. 
   \item \textbf{Cora} \cite{yangRevisitingSemisupervisedLearning2016} is a homogeneous citation network. Nodes represent published papers and contain feature vectors representing specific term occurrences in the abstract. Each node is associated with one of the seven ground-truth labels. 
\end{itemize}

%=====================================================================================
\section{Performance comparison}
\input{figs/tab_performance.tex}

In this experiment, we evaluate the performance of learned node embeddings and detected clusters.
In particular, we evaluate the predictive quality of embeddings using classification and link prediction, i.e., link prediction accuracy ($LP_{ACC}$), temporal $L_\mathcal{T}$ and ground truth $L_y$  label classification accuracy $CF_{ACC}$.
We evaluate the quality of detected clusters by calculating their NMI score based on predefined ground-truth communities $L_y$, $L_\mathcal{T}$, $L_G$.
This tells us whether detected clusters approximate user-defined communities $L_y$, temporal partitioning $L_\mathcal{T}$ or the topology $L_G$.
Additionally, we calculate cluster and community quality scores for the learned community assignments, specifically Davies Bouldin score and modularity.

The embeddings obtained from non-community detection methods were clustered using k-means clustering with $K=20$. Similarly, we use $K=20$ for community detection methods (ComE, GEMSEC, CP-GNN) that assume a predefined cluster count.
%
The results are reported in \cref{tab:results_perf}. 
It can be seen that while MGTCOM is competitive on task-specific measures such as link prediction and timestamp prediction, the community detection methods still have an edge on link-based modularity measures. 
A possible explanation for this would be the fact that the DPMM process is more prone to getting stuck in local minima as the clusters split and merge.
Another possibility is that node features do not contain enough information to model very specific network features such as modularity.
In \cref{sec:abl:aux_emb} we further explore this issue by varying the auxiliary embedding ratio.

While CTDNE performs comparatively well in capturing the temporal aspect of the network, we see that it still yields inferior results on datasets where temporal features are weakly correlated with topology.

It is interesting to note that algorithms that rely on pairwise loss measures such as GraphSAGE and CP-GNN perform relatively well on classification-based measures while performing very poorly on cluster quality measures such as DBI and modularity.
A possible explanation for such observation is that the combination of neighborhood sampling and pairwise loss reinforces structural similarity despite having a large receptive field.
Our method successfully overcomes this issue by modifying Hinge loss to work in a context path setting (See \cref{sec:obj_fn}).

We also observe that the MGTCOM model performs well on both topology and temporal prediction tasks in comparison to its task-specific counterparts. 

%=====================================================================================
\section{Qualitative results}
\input{figs/img_embeddings.tex}

We further compare $MGTCOM$ and the baseline models on the DBLP-HCN network.
We apply the T-SNE dimensionality reduction technique to visualize the trained node embeddings in 2D space colored by the ground truth label and the node timestamp (See \cref{fig:embeddings}).

Since in the DBLP-HCN dataset the timestamps are weakly correlated with its topology, we can see that topology-focused embedding (and community detection) methods such as ComE and Node2Vec do not capture temporal relations of nodes. On contrary, we observe distinct patterns emerge when looking at $MGTCOM$ generated embeddings for both of the labels.
%
Similar to that of $ComE$ the community structures are visible in the node embeddings though they are not as distinct.


%=====================================================================================
\section{Inference results}
Because the $MGTCOM$ model operates on sampled neighborhood subgraphs, in contrast to other methods it can operate in an inductive setting.
Meaning that it is not necessary to retrain the model to infer representation vectors for previously unseen nodes.

We evaluate the performance of $MGTCOM$ and its task-specific variants in inductive settings by controlling the ratio of nodes in the training set to the validation set.
The test set remains constant throughout the experiment to accurately assess performance on inferred nodes.
The relevant quality measures are computed exclusively on the test set and can be found in \cref{tab:results_inference}.

In \cref{fig:inference} we see the same measures plotted with the training ratio on the x-axis.
From \cref{fig:inference} (a) we observe that varying training set size does not affect link-prediction tasks as much as node classification tasks (b, c, d).
Throughout the measures, we can see that using only 75\% of the data does not substantially affect the results.
Interestingly, we observe that the variance on the temporal prediction task increases when more data is provided.

\input{figs/img_inference.tex}
\input{figs/tab_inference.tex}


%=====================================================================================
\section{Learnable parameter reduction}
An important goal of our work is to prove that inductive-based community detection is feasible.
We address the structural similarity bias found in many unsupervised inductive algorithms by introducing a custom loss and sampling methodology in \cref{sec:obj_fn}.
While our model still utilizes embeddings to address the incompleteness constraints, we show in \cref{sec:abl:aux_emb} that importance-based pruning is an effective optimization to keep the model scalable.

As result, our model takes advantage of the scalability of inductive representation learning methods.
In \cref{tab:params} we compare the parameter count of the $MGTCOM$ model to the node2vec model which directly learns node embeddings.
Overall $MGTCOM$ has fewer parameters since the model size is bound by meta-topology.
In highly heterogeneous graphs such as the ICEWS dataset, the number of parameters may become larger than expected.
Specifically, the number of parameters is proportional to $|\mathcal{A}| + |\mathcal{R}|$.
For exact analysis on the number of learnable parameters the model uses, we refer the reader to \cref{sec:exact_params}. 

\input{figs/tab_parameters.tex}