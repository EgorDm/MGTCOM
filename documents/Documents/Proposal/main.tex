% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
acmsmall,
nonacm,
screen,
acmthm]{acmart}
\usepackage{amsmath}
% \usepackage{lmodern}
% \DeclareMathDelimiter{(}{\mathopen} {operators}{"28}{largesymbols}{"00}
% \DeclareMathDelimiter{)}{\mathclose}{operators}{"29}{largesymbols}{"01}
\usepackage{iftex}
\ifPDFTeX
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
\usepackage{unicode-math}
\defaultfontfeatures{Scale=MatchLowercase}
\defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
    \usepackage[]{microtype}
    \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
    \IfFileExists{parskip.sty}{%
        \usepackage{parskip}
    }{% else
        \setlength{\parindent}{0pt}
        \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
    \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
        pdftitle={Community Detection through Representation learning in Evolving Heterogenous Networks},
                            hidelinks,
        pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
%\setcounter{secnumdepth}{5}

%% pandoc-theoremnos: required package
\usepackage{amsthm}
\usepackage[capitalise]{cleveref}

%% pandoc-theoremnos: set theorem types
\newtheorem{thm}{Theorem}
\newtheorem{rqq}{Research Question}
\newtheorem{dfn}{Definition}

%% pandoc-eqnos: disable brackets around cleveref numbers
\creflabelformat{equation}{#2#1#3}
\ifLuaTeX
\usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

% Title
\title{Community Detection through Representation learning in Evolving
Heterogenous Networks}

% Subtitle
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
\apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{A Master's Thesis proposal}

% Authors
\author{Egor Dmitriev}
\email{e.dmitriev@students.uu.nl}
\affiliation{%
  \institution{Utrecht University}
  \country{The Netherlands}
}

% Date
\date{}

% Beamer things

\begin{document}
\begin{abstract}
Recent developments in big data and graph representation learning have
allowed researchers to make breakthroughs in social network analysis and
the identification of communities. While opening a lot of research
opportunities, such approaches are highly limited to snapshots of
rapidly evolving social networks. This, in fact, is a great
simplification of the real-world situation which is always evolving and
expanding by the user and/or machine interactions.\\
\strut \\
Relying on novel research of dynamic graph representation learning, the
goal of my thesis project is to build a framework for community
detection and representation in dynamic heterogeneous networks. To
verify the merit of the proposed framework, it will be evaluated against
baselines on static heterogeneous graphs, and analyzed against gathered
twitter dataset on covid measures.
\end{abstract}
\maketitle


% TOC Title

% TOC Beamer
{
\setcounter{tocdepth}{2}
\tableofcontents
\pagebreak
}





\hypertarget{introduction-and-background}{%
\section{Introduction and
Background}\label{introduction-and-background}}

Social Network Analysis (SNA) is a huge part of the Network Science
field and is concerned with the process of investigating social
structures that occur in the real-world using Network and Graph Theory.
These social structures usually include social media networks
\citep{grandjeanSocialNetworkAnalysis2016, hagenCrisisCommunicationsAge2018},
economic transaction networks
\citep{prykeAnalysingConstructionProject2004, kongWhyAreSocial2011, swamynathanSocialNetworksImprove2008},
knowledge networks
\citep{gruberCollectiveKnowledgeSystems2008, brenneckeFirmKnowledgeNetwork2017},
and disease transmission networks
\citep{morrisNetworkEpidemiologyHandbook2004}.

One main issue to address while studying this type of real-world events
lies in the identification of meaningful substructures hidden within the
overall complex system. The SNA is therefore applied to extract patterns
from the data usually in form of information flow, identification of
high throughput nodes and paths, and discovery of communities and
clusters. In this thesis, we are going to focus on the problem of
community discovery.

This thesis proposal is structured as follows: in this section, we are
going to introduce basic concepts and challenges of Dynamic Community
Detection. In \cref{literature-review} a brief literature survey is
conducted, identifying the current state of the art and approaches to
Dynamic Community Detection. In \cref{research-questions} we will
describe the problem we are trying to solve as well as formulate the
research questions. In \cref{approach} we will elaborate on our proposed
methodology for solving the posed problem and answering the research
questions. Finally, in \cref{planning} the concrete planning for the
research project is laid out.

\hypertarget{community-detection}{%
\subsection{Community Detection}\label{community-detection}}

The problem of partitioning a complex network into \emph{communities}
which represent groups of individuals with high interaction density,
while individuals from different communities have comparatively low
interaction density is known as Community Discovery (CD). CD is a task
of fundamental importance within SNA as it discloses deeper properties
of networks. It provides insight into networks' internal structure and
its organizational principles.

Many useful applications of CD have been studied by researchers
including identification of criminal groups
\citep{sarvariConstructingAnalyzingCriminal2014}, social bot detection
\citep{karatasReviewSocialBot2017}, targeted marketing
\citep{mosadeghUsingSocialNetwork2011}, and public health/disease
control \citep{salatheDynamicsControlDiseases2010}.

With the explosion of human- and machine-generated data, often collected
by social platforms, more datasets are emerging having rich temporal
information that can be studied. Most of the CD methods operate only on
static networks. Meaning that their temporal dimension is often omitted,
which often does not yield a good representation of the real world,
where networks constantly evolve. Such networks are often referred to as
dynamic networks as their components such as nodes and edges may appear
and fade from existence. Accordingly community detection on such dynamic
networks is called Dynamic Community Detection (DCD).

DCD algorithms that incorporate additional temporal data are often able
to both outperform their counterpart CD algorithms
\citep{granellBenchmarkModelAssess2015, liuCommunityDetectionMultiPartite2016, faniUserCommunityDetection2020, rossettiANGELEfficientEffective2020},
as well as provide additional information about communities for analysis
\citep{pallaQuantifyingSocialGroup2007}. This additional information
comes in form of community events such as (birth, growth, split,
merging, and death) or in form of the ability to track the membership of
certain individuals over time.

\hypertarget{challenges-in-community-detection}{%
\subsection{Challenges in Community
Detection}\label{challenges-in-community-detection}}

DCD is seen as the hardest problem within Social Network Analysis. The
reason for this is mainly because DCD, unlike CD, also involves tracking
the found communities over time. This tracking relies on the consistency
of the detected communities, as usually slight changes to the network
may cause a different community membership assignment. Not properly
accounting for this uncertainty may cause community and result drift
\citep{dakicheTrackingCommunityEvolution2019}.

Additionally, the increasing richness of the data is not only limited to
temporal data. The real-world data often connects entities of different
modalities. This multi-modality occurs through the fact that the
entities and relations themselves may be of different types (meta
topology-based features). For example users, topics, and documents in a
social network (or vehicles and landmarks in a traffic network). More
complex networks may include asymmetric relationships, and temporal
networks may include appearing, disappearing, or streaming edges/nodes.

Another example of multi-modality in networks comes in the form of node
and relation features (content-based features). These features may come
in the form of structured (numerical, categorical, or vector data) or
unstructured data such as images and text. It is of high importance to
explore this multi-modal data as it may not always be possible to
explain the formation of communities using network structural
information alone.

Finally, a more systematic issue is that there is no common definition
for a community structure. Within networks, it is usually described in
terms of membership assignment, while in more content-based settings
communities are described in terms of modeled topics (that usually
represent interest areas) or distributions over latent similarity space.
Both definitions have their shortcomings as they often fail to account
for more complex community structures (overlapping and hierarchical
communities) and non-linearity of structures often found in the real
world.

The task of community detection is often compared to clustering and
graph clustering, which not always may be a fair comparison as a main
focus point in many CD algorithms is the fact that the amount of
communities is unknown a priori. Communities are never planted in the
real world and the algorithms should detect them in an unsupervised
manner.

\hypertarget{literature-review}{%
\section{Literature Review}\label{literature-review}}

The problem of dynamic community detection was noticed quite early on
within the SNA community and a considerable amount of research has been
made in order to provide a comprehensive a analysis of network. While
the said research was mostly focused on the discovery of communities
using topologically-based features and node connectivity, the covered
methods did research the limitations and challenges posed by a temporal
context.

In recent years, significant developments have been made in the space of
deep learning. Mainly in the development of new deep learning methods
capable of learning graph-structured data
\citep{bronsteinGeometricDeepLearning2017, hamiltonRepresentationLearningGraphs2018, kipfSemiSupervisedClassificationGraph2017}
which is fundamental for SNA. Because of this, various problems within
the field have been revisited, including community detection problems.
The approaches have been expanded by incorporation of more complex
features, solving the problems concerning multi-modality, and the
introduction of unsupervised learning.

Despite this resurgence, the DCD problem has received little attention.
Though a few efforts have been made to incorporate the deep learning
methods by introducing content-based similarity dynamic, the definition
of unified constraints for end-to-end learning, and usage of graph
representation-based CD algorithms within a temporal context, the
current state of the art leaves a lot to be desired.

We structure the literature as follows: first, we describe the various
interpretations of the Community Structure in
\cref{community-structures}. Next, we explore various approaches and
techniques related to Graph Representation Learning in
\cref{graph-representation-learning}. Then, we provide an overview of
the current state-of-the-art approaches for Community Detection and
Dynamic Community Detection tasks in \cref{link-based-approaches} and
\cref{representation-based-approaches}. Finally, we discuss the ways to
evaluate the said algorithms in \cref{evaluation} and the datasets
available in \cref{datasets}.

\hypertarget{community-structures}{%
\subsection{Community Structures}\label{community-structures}}

The goal of this section is to introduce fundamental structures for the
Dynamic Community Detection task. We do this by combining various
definitions used in the relevant literature as well as establishing the
purpose for these structures, before proceeding into approaches for
detecting communities in the following sections.

\hypertarget{communities}{%
\subsubsection{Communities}\label{communities}}

Communities in real-world networks can be of different kinds: disjoint
(students belonging to different educational institutions), overlapping
(person having membership in different social groups) and hierarchical
(components of a car). One of the main reasons behind the complexity of
CD is that there is not one unique definition what a community actually
is.

The \emph{link-based} (also referred to as classic) community detection
methods intuitively describe communities as groups of nodes within a
graph, such that the intra-group connections are denser than the
inter-group ones. This definition is primarily based on the
\emph{homophily} principle, which refers to the assumption that similar
individuals are those that are densely connected together. Therefore,
these kind of methods look for sub-graph structures such as cliques and
components that identify connectedness within the graph structure to
represent the communities.

Unfortunately, in most cases link-based methods fall short to identify
communities of similar individuals. This is mainly due to two facts: (i)
many similar individuals in a social network are not explicitly
connected together, (ii) an explicit connection does not necessarily
indicate similarity, but can explained by sociological processes such as
conformity, friendship or kinship
\citep{diehlRelationshipIdentificationSocial2007, faniUserCommunityDetection2020}.

A more general definition is introduced in
\citep{cosciaClassificationCommunityDiscovery2011} to create an
underlying concept generalizing all variants found in the literature
(\cref{dfn:community}). In link-based methods, a direct connection is
considered as a particular and very important kind of action, while
newer methods also consider content or interest overlap.

\begin{dfn}[Community]\label{dfn:community} 

A community in a complex network is a set of entities that share some
closely correlated sets of actions with the other entities of the
community.

\end{dfn}

\hypertarget{dynamic-communities}{%
\subsubsection{Dynamic Communities}\label{dynamic-communities}}

Similar to how communities can be found in static networks, dynamic
communities extend this definition by utilizing the temporal dimension
to define their life cycle/evolution over a dynamic network. A dynamic
community is characterized by a collection of communities and a set of
transformations on these communities over time.

This persistence of communities across time subjected to progressive
changes is an important problem to tackle. Though, as noted by
\citep{rossettiCommunityDiscoveryDynamic2018} the problem can be
compared to the famous ``the ship of Theseus'' paradox. Because
(verbatim), \emph{deciding if an element composed of several entities at
a given instant is the same or not as another one composed of some---or
even none---of such entities at a later point in time is necessarily
arbitrary and cannot be answered unambiguously}.

Most of the works agree on two atomic transformations on the
communities, including node/edge appearance and vanishing. In other
works such as
\citep{pallaQuantifyingSocialGroup2007, asurEventbasedFrameworkCharacterizing2009, cazabetUsingDynamicCommunity2012}
authors define a more high-level set of transformations (also referred
to as events) built on top of the atomic ones. These transformations
more interesting for analytical purposes and include:

\begin{itemize}
\tightlist
\item
  Birth, when a new community emerges at a given time.~
\item
  Death, when a community disappears. All nodes belonging to this
  community lose their membership.
\item
  Growth, when a community acquires some new members (nodes).
\item
  Contraction, when a community loses some of its members.
\item
  Merging, when several communities merge to form a new community.
\item
  Splitting, when a community is divided into several new ones.
\item
  Resurgence, when a community disappears for a period and reappears.
\end{itemize}

These events/transformations are often not explicitly used during the
definition and/or representation of dynamic communities. Nevertheless,
most of the methods covered in the following sections do define a way in
their algorithm to extract such events from the resulting data.

Finally, it is important to note that dynamic networks can differ in
representation. They can be represented as either a time series of
static networks (also referred to as snapshots) or as a real-time stream
of edges (referred to as temporal networks). Within the global context
of dynamic community detection, they can be seen as equivalent as the
conversion between the two representations can be done in a lossless
way. The latter, temporal networks are often used to handle incremental
changes to the graph and are most commonly applied within real-time
community detection settings.

\hypertarget{graph-representation-learning}{%
\subsection{Graph Representation
Learning}\label{graph-representation-learning}}

The representation-based approaches stem from the field of computational
linguistics which relies heavily on the notion of \emph{distributional
semantics} stating that words occurring in similar contexts are
semantically similar. Therefore the word representations are learned as
dense low-dimensional representation vectors (embeddings) of a word in a
latent similarity space by predicting words based on their context or
vice versa
\citep{mikolovEfficientEstimationWord2013, penningtonGloveGlobalVectors2014}.
Using the learned representations similarity, clustering and other
analytical metrics can be computed.

The success of these representation learning approaches has spread much
farther than just linguistics as similar ideas are also applied to other
fields including graph representation learning. Methods such as deepwalk
\citep{perozziDeepWalkOnlineLearning2014}, LINE
\citep{tangLINELargescaleInformation2015}, and node2vec
\citep{groverNode2vecScalableFeature2016} use random walks to sample the
neighborhood/context in a graph (analogous to sentences in linguistic
methods) and output vector representations (embeddings) that maximize
the likelihood of preserving the topological structure of the nodes
within the graph.

Whereas previously the structural information features of graph entities
had to be hand-engineered, these new approaches are data-driven, save a
lot of time labeling the data, and yield superior feature/representation
vectors. The methods can be trained to optimize for \emph{homophily} on
label prediction or in an unsupervised manner on link prediction tasks.

Newer approaches introduce the possibility for the fusion of different
data types. GraphSAGE
\citep{hamiltonInductiveRepresentationLearning2018} and Author2Vec
\citep{wuAuthor2VecFrameworkGenerating2020} introduce a methodology to
use node and edge features during the representation learning process.
Other approaches explore ways to leverage heterogeneous information
present within the network by using \emph{metapath} based random walks
(path defined by a series of node/link types)
\citep{dongMetapath2vecScalableRepresentation2017} or by representing
and learning relations as translations within the embedding space
\citep{bordesTranslatingEmbeddingsModeling2013}. In
\citet{nguyenContinuousTimeDynamicNetwork2018} the authors introduce a
way to encode temporal information by adding chronological order
constraints to various random walk algorithms. Other relevant
advancements within the field include Graph Convolutional Networks (GCN)
\citep{kipfSemiSupervisedClassificationGraph2017a} and (Variational)
Graph Auto-Encoders (GAE) \citep{kipfVariationalGraphAutoEncoders2016}
which present more effective ways to summarize and represent larger
topological neighborhoods or whole networks.

\hypertarget{link-based-approaches}{%
\subsection{Link-based Approaches}\label{link-based-approaches}}

Link-based approaches to (Dynamic) Community Detection rely on
connection strength to find communities within the network. The main
criteria for communities are the assumed property that intra-group
connections are denser than inter-group ones. The networks are
partitioned in such a way, that optimizes for a defined measure
characterizing this property.

We start this section by covering the fundamentals of link-based
community detection by introducing commonly used community quality
measures and algorithms for optimizing them. Next, we introduce the
link-based DCD problem and the unique challenges that arise as opposed
to CD. Then we proceed to cover the current state of the art by
describing the related works, their solutions to the said challenges,
and possible extensions to the problem.

\hypertarget{community-detection-1}{%
\subsubsection{Community Detection}\label{community-detection-1}}

Different metrics exist quantifying the characteristic
of~\emph{homophily}~over edge strength. The most common metric is
Modularity which measures the strength of the division of a network into
modules (communities). Its popularity stems from the fact that it is
bounded and cheap to compute, though it has other problems such as
resolution limit (making detecting smaller communities difficult). Other
metrics that can be found in the literature include but are not limited
to:

\begin{itemize}
\tightlist
\item
  Conductance: the percentage of edges that cross the cluster border
\item
  Expansion: the number of edges that cross the community border
\item
  Internal Density: the ratio of edges within the cluster with respect
  to all possible edges
\item
  Cut Ratio and Normalized Cut: the fraction of all possible edges
  leaving the cluster
\item
  Maximum/Average ODF (out-degree fraction): the maximum/average
  fraction of nodes' edges crossing the cluster border
\end{itemize}

\hypertarget{modularity}{%
\paragraph{Modularity}\label{modularity}}

Modularity directly measures the density of links inside a graph and is
therefore computed on communities (sets of nodes) individually by
weighing edges based on community similarity (or exact matching).
Calculation of modularity is done by aggregating for each pair of nodes
the difference between the expected connectivity (amount of edges
between the nodes) and the actual connectivity (existence of an edge)
given their degrees (\cref{eq:modularity}). The final result represents
the delta difference by how much the given graph exceeds a random graph
as expected connectivity is determined by a random rewiring graph.
Because intra-community pairs are weighted lower than inter-community
pairs the score can vary.

\begin{equation}
Q=\frac{1}{2 m}\sum_{v w}\sum_{r}\left[\overbrace{A_{v w}}^{\text{Connectivity}}-\underbrace{\frac{k_{v} k_{w}}{2 m}}_{\text{Expected Connectivity}}\right] \overbrace{S_{v r} S_{w r}}^{\text{Community Similarity}}
\label{eq:modularity}\end{equation}

\hypertarget{louvain-method}{%
\paragraph{Louvain Method}\label{louvain-method}}

Finding an optimal partition of a graph into communities is an NP-hard
problem. This is because, while calculating the modularity score can be
done in linear time, all possible node to community assignments still
have to be considered. Therefore heuristic-based methods such as the
Louvain method are usually used.

Louvain method \citep{blondelFastUnfoldingCommunities2008} is a
heuristic-based hierarchical clustering algorithm. It starts by
assigning each node in the graph to its own community. Then it merges
these communities by checking for each node the change in modularity
score produced by assigning it to a neighbor community (based on the
existence of a connection). Once the optimal merges are performed, the
resulting communities are grouped into single nodes and the process is
repeated.

Since modularity changes can be computed incrementally, the complexity
of this method is \(O\left(n \log n\right)\). Additionally, due to the
flexibility of the modularity measure, it allows detecting communities
in graphs with weighted edges.

\hypertarget{label-propagation-algorithm}{%
\paragraph{Label Propagation
algorithm}\label{label-propagation-algorithm}}

Another way to sidestep the complexity issue is using the Label
Propagation algorithm as it uses the network structure directly to
define partitions and doesn't require any priors (quality metrics). The
intuition for the Label Propagation algorithm is as follows: When
propagating a label through connections in the network, a single label
quickly becomes dominant within a group of densely connected nodes, but
these labels usually have trouble crossing sparsely connected regions.

The algorithm starts by assigning each node their own label. After that,
for each iteration, each node updates its label to the majority label
among its neighbors where ties are broken deterministically. The
algorithm stops after a fixed amount of iterations or once it has
converged. An important feature of this algorithm is that a preliminary
solution can be assigned before each run, therefore only updating
existing membership assignments.

\hypertarget{dynamic-community-detection}{%
\subsubsection{Dynamic Community
Detection}\label{dynamic-community-detection}}

Dynamic Community Detection can be seen as an extension to community
detection by the addition of the Community Tracking task. Tracking
relies on the coherency and stability of found communities to define
their evolution through time. The said properties can not be taken for
granted and introduce new challenges when designing DCD methods. The
main issue is in fact that they are competitive with each other causing
a trade-off between community coherency/quality and community temporal
stability.

Various strategies dealing with this trade-off are categorized by
\citet{rossettiCommunityDiscoveryDynamic2018} and
\citet{dakicheTrackingCommunityEvolution2019} where authors reach a
consensus over three main groups. In the following sections, we briefly
introduce these strategies and describe the current state of the art in
similar order.

\hypertarget{independent-community-detection-and-matching}{%
\paragraph{Independent Community Detection and
Matching}\label{independent-community-detection-and-matching}}

Also referred to as the two-stage approach. Works by splitting the DCD
task into two stages. The first stage applies CD directly to every
snapshot of the network. Followed by the second stage matching the
detected communities between the subsequent snapshots.

The advantages of this approach include the fact that it allows for use
of mostly unmodified CD algorithms for the first step and that it is
highly parallelizable as both detection and matching steps can be
applied to each snapshot independently. The main disadvantage is the
instability of underlying CD algorithms which may disrupt the community
matching process. Many CD methods may give drastically different results
in response to slight changes in the network. During the matching, it
becomes difficult to distinguish between this algorithm instability and
the evolution of the network.

In \citet{wangCommunityEvolutionSocial2008} the authors circumvent this
instability issue by looking at the most stable part of the communities,
namely core/leader nodes. In their research, they observe that in
various datasets most of the nodes change dramatically while only a
small portion of the network persists stably. To exploit this feature,
the algorithm CommTracker is introduced which first detects the said
core nodes, and then defines rules to both extract communities as well
as their evolutional events. The community members are assigned based on
their connectivity relative to core nodes.

\citet{rossettiANGELEfficientEffective2020} proposes a way to detect
overlapping communities in dynamic networks. A more robust two-phase
community detection method (ANGEL) is proposed to ensure the stability
of the found communities. The first phase extracts and aggregates local
communities by applying Label Propagation on the Ego-Graph (graph
excluding a single node) for each node in network. The found communities
are biased due to their partial view of the network and are merged in
the second step based on their overlap yielding more stable communities
with the possibility of overlap. During the matching for each snapshot,
a step forward and backward in time is considered where community splits
and merges are detected by reusing the matching criteria of the second
phase of the CD step.

\hypertarget{dependent-community-detection}{%
\paragraph{Dependent Community
Detection}\label{dependent-community-detection}}

Dependent Community Detection strategy works by detecting communities in
each snapshot based on the communities found in the previous snapshots.
This approach introduces temporal smoothness because the previous
solution is reused making the matching process obsolete. Though as part
of the described trade-off it can impact the long-term coherence of the
dynamic communities. Mainly, because each step introduces a certain
amount of error into the results (community drift) which may get
amplified within further iterations (error accumulation). Another
disadvantage is the fact that the strategy has limited possibilities of
parallelization due to its dependent nature.

To lessen the complexity of continuous re-detection of the communities
some algorithms process the changes incrementally by limiting the change
to a local neighborhood. While this approach has many benefits, it is
important to note that these algorithms face a problem where only
applying local changes can cause communities to drift toward invalid
ones in a global context.

\citet{heFastAlgorithmCommunity2015} introduces an efficient algorithm
by modifying the Louvain method. Based on the observation that between
consecutive timesteps only a fraction of connections changes and do not
affect communities dramatically, they argue that if all community nodes
remain unchanged, the community also remains unchanged. With this in
mind, they make a distinction between two types of nodes, ones that
change the connection in a snapshot transition and ones that do not. The
former have to be recomputed, while the latter maintain their community
label. The nodes that maintain their community are merged into community
nodes with edges to other community nodes and changed nodes weighted
proportionally to their real connectivity (amount of edges when
ungrouped). This simplified graph is passed to the Louvain method
algorithm for community detection. By reusing the community assignments
temporal smoothness is maintained and due to the incremental nature of
this algorithm, the overall complexity remains low.

\citet{guoDynamicCommunityDetection2016} envision the target network as
an adaptive dynamical system, where each node interacts with its
neighbors. The interaction will change the distances among nodes, while
the distances will affect the interactions. The intuition is that nodes
sharing the same community move together, and the nodes in different
communities keep far away from each other. This is modeled by defining
the so-called Attractor algorithm, which consists of three interaction
patterns that describe how node connection strength is influenced by
neighboring nodes. The edge weights are initialized using Jaccard
distance and the propagation is run until convergence. The communities
can be extracted by thresholding on edge weight/distance. Thereafter,
all changes are treated as network disturbances. The disturbance can be
limited to a certain area using a disturbance factor which defines a
bound on the possible propagation.

More recently the \citet{yinMultiobjectiveEvolutionaryClustering2021}
has proposed an evolutionary algorithm by looking at the DCD from an
Evolutionary Clustering Perspective. They detect community structure at
the current time under the guidance of one obtained immediately in the
past by simultaneously optimizing for community quality score
(modularity) and community similarity between subsequent time steps
(NMI). In the methodology, a way is proposed to encode a graph
efficiently into a genetic sequence. Additionally, new mutation and
crossover operators are proposed which maximize either of the two
objectives. By using a local search algorithm, building a diverse
initial population, and selecting for dominant candidates the
communities maximizing both objectives are obtained.

\hypertarget{simultaneous-community-detection}{%
\paragraph{Simultaneous community
detection}\label{simultaneous-community-detection}}

The final strategy we consider sidesteps the matching issue by
considering all snapshots of the dynamic network at once. This is done
by flattening the network in the temporal dimension and coupling edges
between the same nodes at different timesteps. These approaches usually
don't suffer from instability or community drift. The disadvantages
include that the standard principle of an unique partition for each time
step can't be applied, as only the combined network is used, therefore
limiting the number of possible algorithms. Handling real-time changes
to the graph are also usually not considered.

\citet{muchaCommunityStructureTimeDependent2009} adopt a simple yet
powerful solution to this problem by connecting identical nodes between
different time steps within the unified network. On this network, they
apply a modified Louvain method algorithm to extract the communities
whose members can be split over different timesteps.

\citet{ghasemianDetectabilityThresholdsOptimal2016} apply stochastic
block model-based approach. They make a distinction between two edge
types: (i) spatial edges (edges between neighbors) and (ii) temporal
edges (edges between nodes in subsequent timesteps). Using this
distinction, they define a Belief Propagation equation to learn marginal
probabilities of node labels over time. Additionally in their research,
they introduce a way to derive a limit to the detectability of
communities. This is, because some communities may not be detectable as
their probability nears that of random chance.

\hypertarget{representation-based-approaches}{%
\subsection{Representation-based
Approaches}\label{representation-based-approaches}}

The main difference between Representation-based approaches and
link-based approaches is the fact that they usually don't directly model
the network based on connections. Instead, they learn an intermediate
representation of the graph or its components to which CD detection can
be applied to. While also relying on the idea of \emph{homophily}, most
methods define additional objectives to improve community quality.

The main reasoning for this is the fact that real-world networks are
non-linear, meaning that there may be no connections when they make
sense and vice versa \citep{wangEvolutionaryAutoencoderDynamic2020}. By
using deep neural networks to learn these embeddings one can address
such non-linearity as they are in general very robust against noise.
Other notable benefits to using representation-based approaches include
the fact that they compress the data efficiently as real-world networks
are very sparse. They can also represent multi-modal features, network
(meta) structure, and temporal dimension by defining them all in a
compatible similarity space or learning mappings to this space. Finally,
representations are naturally easy to compute similarity on.

In this section, we describe representation-based approaches by covering
both CD and DCD approaches. To provide a more cohesive overview of the
methods, we group them by their innovations instead.

\hypertarget{affiliation-graph-models}{%
\subsubsection{Affiliation Graph
Models}\label{affiliation-graph-models}}

While arguably not being representational by itself, Affiliation Graph
Network (AGM) model introduced in
\citet{yangCommunityAffiliationGraphModel2012} is very influential
within the deep learning/representation learning CD field.

The AGM models a network as a bipartite graph with communities as
first-class citizens and is represented by the following equation
\(B(V, C, M, \{p_c\})\), where \(V\) represents nodes, \(C\) set of
communities, \(M\) node-community memberships and \(\{p_c\}\) model
parameters (a single probability \(p_c\) per community). It can model
non-overlapping, overlapping, and hierarchical communities by defining
rules on membership sets in \(M\). AGM can be used in both generative
and discriminative settings.

The generative scenario works as follows: Given an AGM
\(F(V, C, M, {p_c})\), generate links between each pair of nodes
exceeding a baseline probability \(p\). This is done by considering
that, according to AGM, each pair of nodes in community \(C_i\) is
connected with a probability \(p_{c_i}\). Therefore, the probability of
two nodes having a connection is proportional to the number of
communities they share (which can be derived from the model).

The discriminative scenario is defined as: Given a graph \(G\), find a
model \(F(V, C, M, {p_c})\) that may have generated it. By assuming that
the graph was generated using an AGM, the parameters \(M\), number of
communities \(|C|\) and \(\{p_c\}\) have to be found. Process for
finding such a model to the graph involves max likelihood fitting. AGM
is relaxed to have membership strengths \(F_{uC}\), which helps to
define the probability of nodes \(u\) and \(v\) connecting through
community \(C\) as \(P_{C}(u, v)\), and by themselves \(P(u, v)\) (by
marginalizing over communities). Using this, a probability \(P(G|F)\)
can be constructed quantifying how well the model fits the data.
Finally, gradient ascent can be applied to optimize for the model's
parameters.

\hypertarget{graph-reinforcement}{%
\subsubsection{Graph Reinforcement}\label{graph-reinforcement}}

The first class of methods we consider are Graph Reinforcement methods.
These methods use representation-based learning techniques to enhance
the graph by adding valuable edges or reducing the noise by removing
noisy connections. This is usually done by training a model on a
link-prediction task. A notable benefit of this approach is that other
well-known CD methods can be used on the enhanced graph afterward.

\citet{kangCommunityReinforcementEffective2021} present a CD algorithm
agnostic pre-processing method for strengthening the community structure
of a graph by adding non-existing predicted intra-community edges and
deleting existing predicted inter-community edges. Their strategy is to
learn topological embedding using a graph representation learning
algorithm (node2vec) based on the existing link prediction task. The
similarity is computed between different node pairs and put into
buckets. Then with the assumption of \emph{homophily} the buckets with a
higher value can be considered holding intra-community while buckets
with lower inter-community connections. Right buckets are picked from
both extremes to create or delete edges. The preemptive CD is done to
greedily guide pair-wise similarity computation and avoid a high
complexity.

\citet{jiaCommunityGANCommunityDetection2019} solves the issue of
detecting overlapping communities by proposing the CommunityGAN
algorithm which jointly optimizes for node and community
representations. First, they define a method for efficient motif (in
their case clique) sampling from the graph (true/clique, and
false/vertex subset). Then, they define a GAN based structure for
learning representational vectors where the generator \(G\) tries to
learn \(p_{true}(m|\mathbf{v}_c)\) as preference distribution of motifs
to generate most likely vertex subsets most likely to be real motifs.
Discriminator \(D\) tries to learn the probability of a vertex subset
being a real motif, therefore creating a minimax game of progressively
optimizing embeddings to be able to encode rich information about
network topology.

Both components (\(G\) and \(D\)) are implemented as a modified relaxed
AGM model with a more general definition to be able to handle the motif
generation (rather than edge generation). The probability of a set of
vertices being a motif is defined in terms of their probability being a
motif through a community, therefore making them community-aware as they
now represent the affiliation weight between a vertex and a community.
The number of communities is chosen by training and testing part of data
on link prediction task.

\hypertarget{multi-objective-optimization}{%
\subsubsection{Multi-objective
optimization}\label{multi-objective-optimization}}

Another subject where representation-based approaches excel is
multi-objective optimization. Usually, a combined objective is defined
in terms of a community quality, temporal consistency, or homophily
measure. These measures in turn use the proximity between the
representation to be able to back-propagate the combined error and
optimize the representation(function) directly.

In \citet{rozemberczkiGEMSECGraphEmbedding2019} authors propose a method
that learns cluster centers along with node embeddings. They define
objective function as a combination of three terms: normalization term
(ensures embeddings are centered at the origin), proximity term (forces
nodes with similar neighborhoods to be embedded close), cluster quality
term (forces nodes to be close to their nearest cluster). Additionally,
a ``social network cost'' term is added as a regularizer to optimize for
proximity between nodes within the same cluster. During training, the
clustering coefficient is annealed to ensure convergence and negative
sampling is employed to avoid large softmax costs.

\citet{yangGraphClusteringDynamic2017} propose a similar idea of
combining embedding and clustering tasks and solving them in an
end-to-end manner. In their work, they employ a Deep Denoise Autoencoder
(DAE) to learn topological information of the network by optimizing for
reconstruction loss. To learn cluster/community centers they define
GRACE cluster module which first computes soft cluster assignment matrix
\(Q\) by utilizing the embeddings and cluster centers which contains
probabilities \(q_{ik}\) of node \(i\) belonging to cluster \(k\). The
clustering loss is defined as KL-divergence between the soft clustering
\(Q\) and auxiliary target distribution \(P\) which is computed by
squaring and normalizing the soft assignments to reinforce more
confident clustering results while preventing the formation of
excessively large clusters. Both embeddings and clustering are optimized
alternatively until convergence.

\citet{maCommunityawareDynamicNetwork2020} proposed a novel approach to
constructing community-aware dynamic network embeddings by leveraging
multi-objective optimization and extending it into a temporal dimension.
They adopt a Graph Autoencoder structure which works by encoding the
full graph into a lower-dimensional structure and decoding it again into
a graph. Assuming a well-tuned network, this allows authors to encode
the network (and its nodes) into more efficient representation vectors
which characterize the network well

The objective function they use is defined by three terms: the
reconstruction error term minimizing the distance between the
ground-truth and the autoencoder output, local structure/homophily
preservation term minimizing first- and second-order proximity between
connected nodes, and the community evolution preservation term
maximizing temporal smoothness of communities at different granularity
levels given their representation as an aggregation of their members.

The initial community assignment is generated using the Louvain method
for high-level communities and using k-means for fine-grained
communities given a max community size parameter \(w\). After that,
embeddings at each snapshot are optimized by employing a dependent
community detection-like strategy.

\citet{wangEvolutionaryAutoencoderDynamic2020} employs a similar to DCD
detection by utilizing the Graph Autoencoder architecture. As an
addition authors add an additional community score term to the objective
function also minimizing the distance between nodes in the same
community. At last, K-means is run on the representational vectors to
detect communities at different timesteps while reusing the outputs from
the previous step.

\hypertarget{multi-modal-community-detection}{%
\subsubsection{Multi-modal community
detection}\label{multi-modal-community-detection}}

Another way to improve community quality is by incorporating multi-modal
features. These can come in form of node attributes, content-based or
meta-topological data. These representations are incorporated into
learned embedding vectors either by direct learning, incorporating them
into the objective function, or use of pre-trained models.

In \citet{faniUserCommunityDetection2020} the authors describe their
method for identifying user communities through multi-modal feature
learning. First user embeddings are learned based on their temporal
content similarity by looking at topics of interest. Per-user, a heat
map is constructed measuring the user's interest over time and topic
axes. By considering users like-minded if their heat maps overlap enough
they train low-dimensional content embeddings spanning this user
similarity space. Next, they use random walk-based GNN methods to learn
topological similarity embeddings for network nodes. Finally, they
modify the graph by setting edge weights proportionally to node
proximity in this combined embeddings space. After that, the Louvain
method is applied to extract these time and content-aware communities.

\hypertarget{evaluation}{%
\subsection{Evaluation}\label{evaluation}}

In the previous section, we have described the different variations of
community structure definitions as well as the approaches used for
detecting these communities. In this section, we will cover how the
found dynamic community structures can be evaluated in a more general
setting to allow a comparison of different approaches. While dynamic
community detection problems can be seen as two tasks
(resemblance/detection and matching/tracking) these two should separate
evaluation tasks.

In the following sections, we cover both of the evaluation tasks by
classifying approaches used in the literature in three classes. Namely,
annotated, metric-based, and task-specific.

\hypertarget{annotated}{%
\subsubsection{Annotated}\label{annotated}}

Evaluation of detected (dynamic) communities becomes much easier when
the \emph{ground-truth communities} are provided. The evaluation is then
done by comparing the difference between the produced communities and
the effective ones. To perform this comparison, the information
theory-based metric Normalized Mutual Information (NMI) is used which
converts community sets to bit-strings and quantifies the ``amount of
information'' that can be obtained about one community by observing the
other \citep{lancichinettiDetectingOverlappingHierarchical2009}.

A possible drawback of this measure is that its complexity is quadratic
in terms of identified communities. In
\citep{rossettiNovelApproachEvaluate2016} alternative measure (NF1) with
linear complexity is introduced which similarly to the F1 score uses the
trade-off between precision and recall (of the average of harmonic
means) of the matched communities. In the follow-up work
\citep{rossettiANGELEfficientEffective2020} the authors describe a way
to apply this measure within the context of DCD by calculating this
score for all the snapshots and aggregating the results into one single
measure.

Aside from NMI other measures are employed such as Jaccard Coefficient,
Accuracy and Rand-Index measuring community overlap
\citep{yangGraphClusteringDynamic2017, mrabahRethinkingGraphAutoEncoder2021, luoDetectingCommunitiesHeterogeneous2021},
Overlapping-NMI \citep{yeDeepAutoencoderlikeNonnegative2018} and
Omega-Index measuring is the accuracy on estimating the number of
communities that each pair of nodes shares
\citep{yangCommunityAffiliationGraphModel2012},

In the real world, there are usually no ground-truth communities.
Therefore this approach is usually applied on synthetic datasets where
the communities and their dynamicity is sampled from a distribution. An
alternative approach some papers take is by defining ground-truth
communities using the metadata and node attributes present within the
datasets. Some datasets may include annotated communities, but this is
not common within DCD datasets.

\hypertarget{metric-based}{%
\subsubsection{Metric based}\label{metric-based}}

Another way to evaluate and compare different CD algorithms without
knowing ground-truth communities is using a quality function.

\hypertarget{network-based-metrics}{%
\paragraph{Network-based metrics}\label{network-based-metrics}}

The first group of measures we consider operates directly on the network
structure. They are most commonly used to evaluate link-based methods as
their results are network partitioning sets. Modularity is the most
widely used measure
\citep{newmanFastAlgorithmDetecting2004, suComprehensiveSurveyCommunity2021},
since it measures the strength of division of a network into modules.
Networks with high modularity have dense connections between the nodes
within the modules and sparse connections between nodes in different
modules. Other measures are used as well including:

\begin{itemize}
\tightlist
\item
  \textbf{Conductance}: the percentage of edges that cross the cluster
  border
\item
  \textbf{Expansion}: the number of edges that cross the community
  border
\item
  \textbf{Internal Density}: the ratio of edges within the cluster with
  respect to all possible edges
\item
  \textbf{Cut Ratio and Normalized Cut}: the fraction of all possible
  edges leaving the cluster
\item
  \textbf{Maximum/Average ODF}: the maximum/average fraction of nodes'
  edges crossing the cluster border
\item
  \textbf{Triangle Participation Ratio TPR}: measures fraction of triads
  within the community. A higher TPR indicates a denser community
  structure
\end{itemize}

\hypertarget{proximity-based-measures}{%
\paragraph{Proximity-based measures}\label{proximity-based-measures}}

Proximity-based measures are often used to evaluate clustering tasks but
are also often employed for representation-based CD methods since there
is a large overlap in their methodology. Additionally,
representation-based approaches have the benefit of being able to
quantify both entities and communities as a d-dimensional vector
enabling a more direct comparison of the two
\citep{wangVehicleTrajectoryClustering2020}. The most common measures
include:

\begin{itemize}
\tightlist
\item
  \textbf{Silhouette Coefficient}: Is defined as
  \(S(i)=\frac{b(i)-a(i)}{\max \{a(i), b(i)\}}\) where \(a(i)\) defines
  the mean distance from node \(i\) to other nodes in the same cluster
  while \(b(i)\) is mean distance to any node not in the same cluster.
  It measures cohesion of a cluster/community and indicates how well a
  node is matched to its own cluster.
\item
  \textbf{Davies-Bouldin Index}: Is the ratio of the sum of the average
  distance to the distance between the centers of mass of the two
  clusters. In other words, it is defined as a ratio of within-cluster,
  to the between cluster separation. The index is defined as an average
  over all the found clusters and is therefore also a good measure to
  deciding how many clusters should be used.
\item
  \textbf{Calinski-Harabasz Index}: Is similarly the ratio of the
  between-cluster to the within-cluster variance. Therefore it measures
  both cohesion (how well its members fit the cluster) as well as
  compares it to other clusters (separation).
\end{itemize}

\hypertarget{stability-based-measures}{%
\paragraph{Stability-based measures}\label{stability-based-measures}}

To evaluate temporal stability and consistency of the node and community
structures, measures based on temporal smoothness are proposed in
\citet{maCommunityawareDynamicNetwork2020}. These measures compare the
evolution rate of network structures against the evolution rate of the
whole network given node embeddings between two consecutive snapshots.
The intuition is that rapidly evolving structures relative to the global
evolution rate are temporally unstable and thus of low quality. Metrics
proposed include:

\begin{itemize}
\tightlist
\item
  \textbf{Network Stability}: Is defined as \cref{eq:networkstability}
  and evaluates the evolution ratio of the low-dimensional node
  representations to the network representations between snapshots at
  \(a\)-th time stamp.
\item
  \textbf{Community Stability}: Is defined as
  \cref{eq:communitystability} and computes stability of communities in
  dynamic networks on the embedded low-dimensional representations. It
  is represented as evolution ratio of the low-dimensional community
  representations to the network representations between snapshots at
  \(a\)-th time stamp.
\end{itemize}

\begin{equation}
p_{s}^{a}=\frac{\left(\left\|\mathrm{H}^{a+1}-\mathrm{H}^{a}\right\|_{2}^{2}\right) /\left\|\mathrm{H}^{a}\right\|_{2}^{2}}{\left(\left\|\mathrm{~A}^{a+1}-\mathrm{A}^{a}\right\|_{2}^{2}\right) /\left\|\mathrm{A}^{a}\right\|_{2}^{2}}
\label{eq:networkstability}\end{equation}

\begin{equation}
p_{c}^{a}=\sum_{k=1}^{q}\left(\frac{\left(\left\|\mathrm{H}_{c_{k}}^{a+1}-\mathrm{H}_{c_{k}}^{a}\right\|_{2}^{2}\right) /\left\|\mathrm{H}_{c_{k}}^{a}\right\|_{2}^{2}}{\left(\left\|\mathrm{~A}_{c_{k}}^{a+1}-\mathrm{A}_{c_{k}}^{a}\right\|_{2}^{2}\right) /\left\|\mathrm{A}_{c_{k}}^{a}\right\|_{2}^{2}}\right) / q
\label{eq:communitystability}\end{equation}

In this case
\(\left\|\mathrm{H}_{c_{k}}^{a+1}-\mathrm{H}_{c_{k}}^{a}\right\|_{2}^{2}\)
represents the normalized euclidean distance between the two clusters
while
\(\left\|\mathrm{~A}_{c_{k}}^{a+1}-\mathrm{A}_{c_{k}}^{a}\right\|_{2}^{2}\)
represents the normalized euclidean distance between the adjacency
matrices of the two clusters. For network stability no such grouping is
done and evaluation is performed on global representation and adjacency
matrices.

\hypertarget{task-specific}{%
\subsubsection{Task specific}\label{task-specific}}

In \citep{peelGroundTruthMetadata2017} the authors criticize annotation
and metric-based CD evaluation approaches by proving that they introduce
severe theoretical and practical problems. For one, they prove the
no-free lunch theorem for CD, ie. they prove that algorithmic biases
that improve performance on one class of networks must reduce
performance on others. Therefore, there can be no algorithm that is
optimal for all possible community detection tasks, as the quality of
communities may differ by the optimized metrics. Additionally, they
demonstrate that when a CD algorithm fails, the poor performance is
indistinguishable from any of the three alternative possibilities: (i)
the metadata is irrelevant to the network structure, (ii) the metadata
and communities capture different aspects of network structure, (iii)
the network itself lacks structure. Therefore, which community is
optimal should depend on its subsequent use cases and not a single
measure.

To address this issue, it is common to evaluate the algorithm on both
earlier described approaches as auxiliary tasks. The general sentiment
behind it is, that communities have better quality if they improve an
underlying application. In the following sections, we describe a few
commonly used auxiliary tasks in literature.

\hypertarget{link-prediction}{%
\paragraph{Link-prediction}\label{link-prediction}}

A common way to evaluate the quality of extracted node embeddings within
Graph Representation learning is using the link-prediction task. As
link-prediction can be done in an unsupervised manner, it does not
require additional labeling for evaluation. The edge set of the input
network is split into a test set on which the model is trained, and a
test set on which is used to compare the predicted links against. For
node representation learning the predictions are defined by proximity
between the nodes and a threshold. To quantify the quality of the
results classification metrics are usually employed such as accuracy,
precision, recall, and f-score.

In the context of CD and DCD, the link prediction is modified to measure
the predicting capability of the community embeddings.
\citet{faniUserCommunityDetection2020} defines a user prediction task to
predict which users posted a certain news article at a certain time.
Their methodology is, given a news item at time \(t\), find the closest
community to the article in representational similarity space. All
members of the given community are seen as predicted users over which
the classification metrics are calculated. Similarly,
\citet{maCommunityawareDynamicNetwork2020} modify the task by predicting
whether the edges will still exist within the next timestamp to also
quantify the temporal prediction capability of the trained embeddings.

\hypertarget{recommendation-tasks}{%
\paragraph{Recommendation Tasks}\label{recommendation-tasks}}

Another way the quality of node representations can be evaluated is by
using recommendation tasks. Here the idea is, instead of predicting a
single item like in link-prediction, to rank the items based on their
recommendation confidence. Using the ranked list, standard information
retrieval metrics such as precision at rank, mean reciprocal rank, and
success at rank k can be computed. This approach is applied to CD
\citep{rozemberczkiGEMSECGraphEmbedding2019, huangInformationFusionOriented2022, faniUserCommunityDetection2020}
by ranking recommendations for per community instead of on an individual
basis. Communities with higher scores therefore would indicate high
similarity between their members.

\hypertarget{datasets}{%
\subsection{Datasets}\label{datasets}}

In this section, we curate popular datasets used in literature for CD
and DCD tasks. First, we discuss methods for generating synthetic
datasets containing ground-truth communities. We conclude this section
with an overview of real-world dynamic networks.

\hypertarget{synthetic-datasets}{%
\subsubsection{Synthetic Datasets}\label{synthetic-datasets}}

Synthetic Datasets are datasets generated by specific models on manually
designed rules. These datasets are generated with communities in mind
and therefore most of the time contain ground-truth communities which
can be used for the evaluation of CD algorithms. In this section, we
describe a selection of popular synthetic data set generation
techniques.

The most common benchmark dataset is was introduced in
\citet{lancichinettiBenchmarkGraphsTesting2008}. Their method for
generating the network generates communities and their nodes with size
and degree drawn from a power-law distribution. After that node edge
rewiring is applied that enforces mixture parameter \(\mu\) which
determines the ratio of in- to between-community connections. In
follow-up \citep{lancichinettiBenchmarksTestingCommunity2009} the
authors extend their method to work for overlapping communities.

In \citet{greeneTrackingEvolutionCommunities2010} the authors modify the
\citet{lancichinettiBenchmarkGraphsTesting2008} method to generate
dynamic community networks. They do this by defining a set of community
events that can be used as seeds for the temporal evolution of the
network. The subsequent snapshots of the graph are defined by this set
of events.

\citet{granellBenchmarkModelAssess2015} propose a model for generating
simple dynamic networks based on stochastic block models (SBM). The time
evolution consists of periodic oscillation of the system's structure
between configurations. Once the initial network is created, it is
divided into subgraphs each having certain inter-and intra- connectivity
probability. The edges between the timestamps are drawn from binomial
distributions which themselves are modified by stochastic
community-based events such as shrinking, growing, merging, and
splitting.

\citet{ghalebiDynamicNetworkModel2019} similarly proposes an SBM
model-based approach to generate dynamic community networks.
Additionally, they propose a methodology to learn model parameters from
existing real-world networks.

\hypertarget{real-world-datasets}{%
\subsubsection{Real World Datasets}\label{real-world-datasets}}

Real-world network datasets are often categorized based on the source
they model such as social networks, biological networks, and webpage
networks. In \cref{tbl:realworlddatasets} we present an overview of the
most commonly used datasets in the literature. For each of the datasets,
we describe their scale in terms of their node and edge counts.
Additionally, we specify the presence of meta-topological information in
form of node types, whether the network is dynamic, whether it includes
content-based (unstructured/textual) information, and the availability
of node features.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2706}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2147}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1735}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1206}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2206}}@{}}
\caption{Summary of real-world data sets. Nodes column contains single
character combinations representing the node types available (one
character per distinct type). Node attributes column indicates the
presence of temporal data using character ``T'' and the presence of
content-based (unstructured) data with character ``C''.
\label{tbl:realworlddatasets}}\tabularnewline
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Data set
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Nodes~~~~~~~~~
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Edges~~~~~~
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Node Attributes~~~~~~~~~~
\end{minipage} \\
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Data set
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Nodes~~~~~~~~~
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Edges~~~~~~
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Node Attributes~~~~~~~~~~
\end{minipage} \\
\midrule
\endhead
\href{http://konect.cc/networks/ucidata-zachary/}{Karate} &
\citep{zacharyInformationFlowModel1976} & 34 (U) & 78 & - \\
\href{https://networkrepository.com/misc-football.php}{Football} &
\citep{girvanCommunityStructureSocial2002} & 115 (C) & 613 & - \\
\href{https://www.aminer.org/citation}{DBLP} &
\citetext{\citealp{tangArnetMinerExtractionMining2008}; \citealp[~][]{yangDefiningEvaluatingNetwork2012}}
& 0.42M (PACT) & 1.34M & 33 (CT) \\
\href{http://www.wise2012.cs.ucy.ac.cy/challenge.html}{Weibo} & - & 8.3M
(UT) & 49M & 33 (T) \\
\href{http://socialnetworks.mpi-sws.org/datasets.html}{FB-wosn} &
\citep{viswanathEvolutionUserInteraction2009} & 64K (UP) & 1.3M & -
(T) \\
\href{https://www.cs.cmu.edu/~enron/}{Enron} &
\citep{vanburenEnronDatasetResearch2009} & 1.15M (UE) & 298K & 2 (CT) \\
\href{https://grouplens.org/datasets/movielens/25m/}{MovieLens} &
\citep{MovieLens25MDataset2019} & 224K (UM) & 25M & 7 (CT) \\
\href{http://ocelma.net/MusicRecommendationDataset/lastfm-1K.html}{LastFM}
& \citep{celmaMusicRecommendationDiscovery2008} & 272K (UAS) & 350K & 2
(CT) \\
\href{https://zenodo.org/record/3608135}{Reddit} &
\citep{baumgartnerPushshiftRedditDataset2020} & 61M (USPC) & 1.2B & 39
(CT) \\
\href{https://www.isi.edu/~lerman/downloads/digg2009.html}{Digg} &
\citep{hoggSocialDynamicsDigg2012} & 142K (US) & 3.7M & 3 (T) \\
\href{https://github.com/Jhy1993/HAN}{ACM} &
\citep{wangHeterogeneousGraphAttention2021a} & 3,025 (APS) & 2M & 1830
(CT) \\
\href{https://github.com/Jhy1993/HAN}{IMDB} &
\citep{wangHeterogeneousGraphAttention2021a} & 4,780 (MAD) & 119K & 1232
(CT) \\
\href{https://snap.stanford.edu/data/wiki-RfA.html}{Wiki-RFA} &
\citep{westExploitingSocialNetwork2014} & 10K (U) & 159K & 5 (CT) \\
\href{https://snap.stanford.edu/data/soc-sign-bitcoin-otc.html}{Bitcoin}
& \citep{kumarEdgeWeightPrediction2016} & 5K (U) & 35K & 2 (T) \\
\href{https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0168344}{Rumor
Detection} & \citep{kwonRumorDetectionVarying2017} & 54M (UT) & 1.9B &
22 (T) \\
\bottomrule
\end{longtable}

\hypertarget{research-questions}{%
\section{Research Questions}\label{research-questions}}

The main goal of my thesis is to build a framework for community
detection and representation in dynamic heterogeneous networks.

This is, to enable dynamic community analysis on the datasets proposed
in \citet{wangPublicSentimentGovernmental2020}. The data described is
collected from the Twitter social platform and is dynamic,
heterogeneous, and rich in contentual (unstructured text) data. To the
best of our knowledge, there are currently no dynamic community
detection algorithms that can handle this data without relaxing its rich
data representation (data loss).

As there are no alike algorithms, direct comparison is not possible. To
both validate the merit of our methods as well as the quality of the
results, we spread our research over four research questions.

\begin{rqq}[Information]\label{rqq:rq1} 

\emph{Does addition of meta-topological and/or content-based information
improve quality of detected communities?}

\end{rqq}

While focusing on link-based features, CD algorithms treat all nodes
alike. Therefore ignoring arguably most important structural features,
namely node types. Additionally, supplementary node types can be
constructed from categorical features enhancing network topology and
solving issues requiring topic modeling.

Similarly, recent improvements in natural text processing allow for
efficient representation of natural text which is proven to improve the
quality of node embeddings. As the formation of communities is not
purely a sociological process, the CD problem should benefit from the
incorporation of such content-based features.

\begin{rqq}[Scale]\label{rqq:rq2} 

\emph{Does usage of graph representation function learning techniques
improve scale of CD beyond current state-of-the-art?}

\end{rqq}

The previously mentioned representation-based DCD method use spectral
graph representation methods which operate on the whole network at once.
More recent graph representation approaches instead learn a graph
representation function by sampling the network using random walks or
convolutions.

This has a two-fold positive effect on the scalability of the
algorithms. Computation can be done more efficiently as opposed to
spectral methods which rely on adjacency matrices. By learning a
representation function, embeddings are computed on-demand instead of
being held in memory for the whole network, therefore, limiting the
impact of big networks. Other benefits may include the fact that they
would be a suitable choice for processing streaming edge network
variants.

\begin{rqq}[Modelling]\label{rqq:rq3} 

\emph{Does making temporal features implicit in node representations
provide better quality communities as opposed to making them explicit?}

\end{rqq}

Throughout the literature, various ways are used to incorporate temporal
aspects into node embeddings. The implicit approach aims to make the
embeddings temporally aware while the explicit approach creates a
separate embedding for each snapshot. While methods using either
approach have presented good results in the literature, it is important
to analyze the potential trade-off and benefits of both.

\begin{rqq}[Results]\label{rqq:rq4} 

\emph{Do community-aware node embeddings perform well on both node as
well as community evaluation based tasks?}

\end{rqq}

While the main task of the algorithm is to find high-quality dynamic
communities, the result also includes community-aware dynamic
embeddings. Aside from testing the quality of the communities, it is
important to compare how this community awareness influences the
embeddings on dynamic node representation tasks such as link prediction
and node classification.

\hypertarget{approach}{%
\section{Approach}\label{approach}}

To find an answer to our research questions, the methodology is split
into multiple parts which address the selection of baselines, evaluation
benchmark set up, the architecture of the algorithm, building an
appropriate objective function, and elaborating on DCD result extraction
respectively. While split into parts, it is important to note that these
tasks have a large overlap and won't be considered in isolation.

The final result of my thesis will consist of the said framework for DCD
within dynamic heterogeneous graphs, a set of results comparing the
algorithm to the current state-of-the-art approaches on various related
tasks, and a set of ablation tests providing empirical corroboration for
important design choices.

\hypertarget{benchmarks-and-baselines}{%
\subsection{Benchmarks and Baselines}\label{benchmarks-and-baselines}}

As the research questions posed in \cref{research-questions} are all
mostly of a quantitative nature, it is very important to set up
appropriate benchmarks in order to provide a valid answer. As a direct
comparison between methods is not always possible, we define auxiliary
task benchmarks testing algorithms on desired properties as well as
reuse benchmarks used in previous literature to provide a fair
comparison.

\hypertarget{benchmarks-and-evaluation}{%
\subsubsection{Benchmarks and
Evaluation}\label{benchmarks-and-evaluation}}

To provide an answer for \cref{rqq:rq1}, the quality of the algorithm on
both static and dynamic communities needs to be compared against the
benchmarks for various configurations (considering the content and/or
meta-topological data). As our baselines include both representation- as
well as link-based approaches, the benchmarks should cover measures used
in both groups. To evaluate the quality of the communities,
annotation-based approaches (computing NMI and NF1) and quality
metric-based evaluation approaches will be employed (See
\ref{evaluation}). Since our definition of community slightly differs
from the literature as it encompasses network external information
(content) we will also employ task-based evaluation such as
recommendation tasks (follower recommendation, hashtag recommendation -
depending on the dataset).

A similar evaluation methodology will be employed for \cref{rqq:rq3},
though the question is of a more exploratory nature concerning the
modeling of temporal information, and therefore will be conducted as an
ablation test with a focus on stability measures.

The \cref{rqq:rq2} aims to compare the scalability of our approach to
the current representation-based approaches. Therefore a rough
complexity analysis, as well as performance benchmarking (computation
time), shall be conducted.

Finally, \cref{rqq:rq4} addresses the usability of our dynamic community
detection results to other tasks concerning dynamic node representation
learning. Here we, make use of defined auxiliary tasks (recommendation
and link-prediction) to compare our method against more popular dynamic
node representation learning algorithms.

\hypertarget{baselines}{%
\subsubsection{Baselines}\label{baselines}}

To give a fair representation of the state-of-the-art the following
methods are selected as baselines. The selection is based on the
category of communities they learn, diversification of techniques, and
competitiveness with the ideas introduced as part of our framework.

\hypertarget{static-community-detection}{%
\paragraph{Static Community
Detection:}\label{static-community-detection}}

\begin{itemize}
\tightlist
\item
  Link-based:

  \begin{itemize}
  \tightlist
  \item
    \citet{heFastAlgorithmCommunity2015}
  \item
    \citet{rossettiANGELEfficientEffective2020}
  \end{itemize}
\item
  Representation-based:

  \begin{itemize}
  \tightlist
  \item
    \citet{rozemberczkiGEMSECGraphEmbedding2019}~~~~
  \item
    \citet{cavallariLearningCommunityEmbedding2017}
  \item
    \citet{jiaCommunityGANCommunityDetection2019}
  \end{itemize}
\end{itemize}

\hypertarget{dynamic-community-detection-1}{%
\paragraph{Dynamic Community
Detection:}\label{dynamic-community-detection-1}}

\begin{itemize}
\tightlist
\item
  Link-based:

  \begin{itemize}
  \tightlist
  \item
    \citet{rossettiANGELEfficientEffective2020}
  \end{itemize}
\item
  Representation-based:

  \begin{itemize}
  \tightlist
  \item
    \citet{wangEvolutionaryAutoencoderDynamic2020}
  \item
    \citet{maCommunityawareDynamicNetwork2020}
  \end{itemize}
\end{itemize}

\hypertarget{dynamic-representation-learning}{%
\paragraph{Dynamic Representation
Learning:}\label{dynamic-representation-learning}}

\begin{itemize}
\tightlist
\item
  \citet{nguyenContinuousTimeDynamicNetwork2018}
\item
  \citet{wuSageDyNovelSampling2021}
\end{itemize}

\hypertarget{representation-learning}{%
\subsection{Representation learning}\label{representation-learning}}

The core part of the proposed framework is the representation learning
algorithm as it is responsible for both random walk sampling of the
graph, as well as provides the architecture for the deep neural network
used to learn or map the node representation. The representation
learning can be split into three main components. Each of the components
respectively has sufficient prior work and has been used in literature.
The challenge lays in combining them into an efficient architecture for
representation learning capable of maximizing set objective function.

\hypertarget{graph-sampling}{%
\subsubsection{Graph Sampling}\label{graph-sampling}}

The first component concerns efficient graph sampling. In the
literature, various ways to sample graphs can be found a way that
enforces learning desired topological properties of the network. In this
work, the choice lies between random walk approaches
\citep{perozziDeepWalkOnlineLearning2014, groverNode2vecScalableFeature2016, groverNode2vecScalableFeature2016}
which traverse the graph in a depth-first search manner which is known
for favoring homophily and convolution-based approaches
(\citet{kipfSemiSupervisedClassificationGraph2017a};
\citet{hamiltonInductiveRepresentationLearning2018}) which sample the
graph in a breadth-first search manner favoring structural equivalence.

Both methods support heterogeneous graph sampling
\citep{wuAuthor2VecFrameworkGenerating2020, yingGraphConvolutionalNeural2018, yangHeterogeneousNetworkRepresentation2020, dongMetapath2vecScalableRepresentation2017, wangHeterogeneousGraphAttention2021},
and temporally aware sampling
\citep{nguyenContinuousTimeDynamicNetwork2018, wuSageDyNovelSampling2021, dasguptaHyTEHyperplanebasedTemporally2018}.

\hypertarget{dnn-architecture}{%
\subsubsection{DNN Architecture}\label{dnn-architecture}}

The architecture and training strategy of the underlying neural network
matter. Many methods introduced in the previous section already outline
an effective architecture suitable for training on the introduced
sampling method. Though it is important to note that as both data and
requirements for our algorithms differ, they can not be used as it. The
architecture should allow for heterogeneous graph samples, temporally
aware samples, and node features.

Similarly, related literature can be used as inspiration as various
approaches already utilize content-rich networks
\citep{wuAuthor2VecFrameworkGenerating2020, yingGraphConvolutionalNeural2018},
attention-based mechanisms
\citep{abu-el-haijaWatchYourStep2018, sankarDynamicGraphRepresentation2019, wangHeterogeneousGraphAttention2021},
and alternative training strategies such (variational or diffusion)
auto-encoders, GAN
\citep{liVariationalDiffusionAutoencoders2020, kipfVariationalGraphAutoEncoders2016}.

\hypertarget{feature-fusion}{%
\subsubsection{Feature Fusion}\label{feature-fusion}}

Many of the presented datasets are feature-rich. While for some feature
types using them is as simple as pre-processing the values and passing
them to the DNN, some features require additional attention. The natural
text features can be aggregated into a single representation vector
using pre-trained embeddings
\citep{devlinBERTPretrainingDeep2019, penningtonGloveGlobalVectors2014},
and (large) categorical features may be transformed into network nodes,
thus moving the information into topological domain
\citep{chenCatGCNGraphConvolutional2021, wuTopologicalMachineLearning2020}.

\hypertarget{objective-function}{%
\subsection{Objective Function}\label{objective-function}}

A very important part of representation-based DCD methods is the
objective function. By utilizing node (and community) representation
vectors one can optimize the network to maximize a multi-objective
function using back-propagation. During the training process, the focus
can be shifted between the different objectives. By focusing on defining
necessary criteria for dynamic community detection which will give a
rough overview of the multi-objective function we will use.

\hypertarget{cohesion}{%
\paragraph{Cohesion}\label{cohesion}}

The most common definition states that communities are characterized by
more dense inter-community connections compared to intra-community
density. Representation methods extend this definition by noting that
the density of the connections is can be represented by the topological
similarity measure of two nodes. Clustering methods further extend this
definition by defining similarity on multi-modal embeddings, therefore
keeping the definition consistent for attributed networks. Therefore a
viable choice for community cohesion would be the Silhouette Coefficient
(See \cref{evaluation})

\hypertarget{homophily}{%
\paragraph{Homophily}\label{homophily}}

To ensure reliable computation of the cohesion measure, the
representation vectors need to be accurate. A way to train these
representations is by assuming homophily which states that the more two
nodes occur in the same context, the more similar they are. This is
translated to network problems by using node neighborhood as context.
Either using first-order proximity where nodes should occur in their
counterpart's context or by utilizing second-order proximity where two
nodes are similar if they share the same context. Hybrid approaches
exist which optimize for both as they both model different semantics.
This idea can also be extended to content-based features and attributed
networks, therefore, extending the definition of a community through
transitivity when the above definition for cohesion is utilized.

\hypertarget{temporal-smoothness}{%
\paragraph{Temporal Smoothness}\label{temporal-smoothness}}

When talking about dynamic networks and communities, temporal smoothness
should also be considered. Between subsequent timesteps, the dynamic
networks often evolve, but not by a large amount. While individual nodes
may change drastically within a single timestep, the communities are
seen as more stable structures within the networks. Therefore the
evolution of the communities should not exceed to global (network) or
individual (node) evolution rate.

In most of the literature, this temporal smoothness is indirectly
handled by result matching or reuse of results from previous timesteps.
Within representation-based approaches, this property can be quantified
and optimized for. A similar approach is employed to keep the embedding
space temporally stable while only individual nodes may change.

\hypertarget{community-detection-2}{%
\subsection{Community Detection}\label{community-detection-2}}

As the final step of the framework, the viable dynamic communities need
to be extracted. This may be done by simultaneously training community
embeddings along with the node embeddings
\citep{maCommunityawareDynamicNetwork2020, limBlackHoleRobustCommunity2016, wangEvolutionaryAutoencoderDynamic2020},
therefore having the advantage that objective function can directly
influence the resulting communities. Other approaches instead operate on
the resulting embedding space or the augmented graphs to extract the
resulting communities using link-based methods such as the Louvain
method or density-based clustering algorithms such as K-means, BIRCH
\citep{zhangBIRCHEfficientData1996}, or OPTICS
\citep{ankerstOPTICSOrderingPoints1999} yielding the benefit of losing
the community count assumption.

In our approach, we plan to focus on direct community optimization,
while avoiding hard-coding the model to specific assumptions using
spectral clustering-based techniques and soft assignment clustering
\citep[
]{liDivideandconquerBasedLargeScale2021, maCommunityawareDynamicNetwork2020}.

\hypertarget{extensions}{%
\subsection{Extensions}\label{extensions}}

If the timing of the research project permits, while not the main focus
of the research, additional extensions of the algorithms for future work
may be explored. These extensions may encompass exploring evolutional
event detection within the extracted dynamic communities.

\hypertarget{planning}{%
\section{Planning}\label{planning}}

In this section, the approximate planning for my thesis is discussed.
The research phase of the thesis is broken down into two-week blocks.
Below a timeline of these sprints can be found along with goals expected
to achieve by the end of the block. The final deadline for the thesis is
June 13th, 2022.

\hypertarget{timeline}{%
\subsection{Timeline}\label{timeline}}

\hypertarget{january-24---2-weeks}{%
\subsubsection{January 24 - 2 weeks}\label{january-24---2-weeks}}

Preparing the datasets, and setting up benchmarks for the baseline
methods. Implementation of baseline if the implementation is not
publicly available.

\hypertarget{february-5---2-weeks}{%
\subsubsection{February 5 - 2 weeks}\label{february-5---2-weeks}}

Finishing up the benchmark implementation. Conducting the evaluation and
collection of the results on the baselines for the RQ's.

\hypertarget{february-19---2-weeks}{%
\subsubsection{February 19 - 2 weeks}\label{february-19---2-weeks}}

Setting up graph sampling pipeline and experimentation with various
representation-based approaches. Collecting performance results for
ablation tests.

\hypertarget{march-3---2-weeks}{%
\subsubsection{March 3 - 2 weeks}\label{march-3---2-weeks}}

Experimenting and implementing incorporation of community-based
objectives into the framework. Conducting experiments with temporal
modeling for \cref{rqq:rq3}.

\hypertarget{march-19---2-weeks}{%
\subsubsection{March 19 - 2 weeks}\label{march-19---2-weeks}}

Finishing up experimenting and implementing incorporation of
community-based objectives into the framework. Experimenting with
community extraction and sub-sampling.

\hypertarget{april-2---2-weeks}{%
\subsubsection{April 2 - 2 weeks}\label{april-2---2-weeks}}

Evaluation and tuning of the algorithm parameters to collect results for
\cref{rqq:rq1} and \cref{rqq:rq4}

\hypertarget{april-10---2-weeks}{%
\subsubsection{April 10 - 2 weeks}\label{april-10---2-weeks}}

Evaluation and optimization of the algorithm for scalability. Collecting
data for \cref{rqq:rq2}.

\hypertarget{april-30---2-weeks}{%
\subsubsection{April 30 - 2 weeks}\label{april-30---2-weeks}}

Writing thesis: describing the full methodology

\hypertarget{may-14---2-weeks}{%
\subsubsection{May 14 - 2 weeks}\label{may-14---2-weeks}}

Writing thesis: describing evaluation and summarizing the results

\hypertarget{may-28---2-weeks}{%
\subsubsection{May 28 - 2 weeks}\label{may-28---2-weeks}}

Completing the thesis and ensuring everything is ready for delivery.

\bibliography{refs.bib}

\end{document}
