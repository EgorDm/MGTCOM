% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
acmsmall,
nonacm,
screen,
acmthm]{../../scripts/pandoc/templates/acmart}
\usepackage{amsmath}
% \usepackage{lmodern}
% \DeclareMathDelimiter{(}{\mathopen} {operators}{"28}{largesymbols}{"00}
% \DeclareMathDelimiter{)}{\mathclose}{operators}{"29}{largesymbols}{"01}
\usepackage{iftex}
\ifPDFTeX
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
\usepackage{unicode-math}
\defaultfontfeatures{Scale=MatchLowercase}
\defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
    \usepackage[]{microtype}
    \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
    \IfFileExists{parskip.sty}{%
        \usepackage{parskip}
    }{% else
        \setlength{\parindent}{0pt}
        \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
    \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
        pdftitle={Community Detection through Representation learning in Evolving Heterogenous Networks},
                            hidelinks,
        pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
%\setcounter{secnumdepth}{5}

%% pandoc-eqnos: required package
\usepackage{cleveref}

%% pandoc-eqnos: disable brackets around cleveref numbers
\creflabelformat{equation}{#2#1#3}

%% pandoc-tablenos: required package
\usepackage{caption}

%% pandoc-tablenos: environment to disable table caption prefixes
\makeatletter
\newcounter{tableno}
\newenvironment{tablenos:no-prefix-table-caption}{
  \caption@ifcompatibility{}{
    \let\oldthetable\thetable
    \let\oldtheHtable\theHtable
    \renewcommand{\thetable}{tableno:\thetableno}
    \renewcommand{\theHtable}{tableno:\thetableno}
    \stepcounter{tableno}
    \captionsetup{labelformat=empty}
  }
}{
  \caption@ifcompatibility{}{
    \captionsetup{labelformat=default}
    \let\thetable\oldthetable
    \let\theHtable\oldtheHtable
    \addtocounter{table}{-1}
  }
}
\makeatother
\ifLuaTeX
\usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

% Title
\title{Community Detection through Representation learning in Evolving
Heterogenous Networks}

% Subtitle
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
\apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{A Master's Thesis proposal}

% Authors
\author{Egor Dmitriev}
\email{e.dmitriev@students.uu.nl}
\affiliation{%
  \institution{Utrecht University}
  \country{The Netherlands}
}

% Date
\date{}

% Beamer things

\begin{document}
\begin{abstract}
Recent developments in big data and graph representation learning have
allowed researchers to make breakthroughs in social network analysis and
the identification of communities. While opening a lot of research
opportunities, such approaches are highly limited to snapshots of
rapidly evolving social networks. This, in fact, is a great
simplification of the real-world situation which is always evolving and
expanding by the user and/or machine interactions.\\
\strut \\
Relying on novel research of dynamic graph representation learning, the
goal of my thesis project is to build a framework for community
detection and representation in evolving heterogeneous networks. To
verify the merit of the proposed framework, it will be evaluated against
baselines on static heterogeneous graphs, and analyzed against gathered
twitter dataset on covid measures.
\end{abstract}
\maketitle


% TOC Title

% TOC Beamer
{
\setcounter{tocdepth}{2}
\tableofcontents
\pagebreak
}





\hypertarget{introduction-and-background}{%
\section{Introduction and
Background}\label{introduction-and-background}}

Social Network Analysis (SNA) is a huge part of the Network Science
field and is concerned with the process of investigating social
structures that occur in the real-world using Network and Graph Theory.
These social structures usually include social media networks, economic
transaction networks, knowledge networks, and disease transmission
networks.

One main issue to address while studying this type of real-world events
lies in the identification of meaningful substructures hidden within the
overall complex system. The SNA is therefore applied to extract patterns
from the data usually in form of information flow, identification of
high throughput nodes and paths, and discovery of communities and
clusters. In this thesis, we are going to focus on the problem of
community discovery.

This thesis proposal is structured as follows: in this section, we are
going to introduce basic concepts and challenges of Dynamic Community
Detection. In \cref{literature-review} a brief literature survey is
conducted on identifying the current state of the art and approaches to
Dynamic Community Detection. In \cref{research-questions} we will
describe the problem we are trying to solve as well as formulate the
research questions. In \cref{approach} we will elaborate on our proposed
methodology for solving the posed problem and answering the research
questions. Finally, in \cref{planning} the concrete planning for the
research project is laid out.

\hypertarget{community-detection}{%
\subsection{Community Detection}\label{community-detection}}

The problem of partitioning a complex network into \emph{communities}
which represent groups of individuals with high interaction density,
while individuals from different communities have comparatively low
interaction density is known as Community Discovery (CD). CD is a task
of fundamental importance within SNA as it discloses deeper properties
of networks. It provides insight into networks' internal structure and
its organizational principles.

Many useful applications of CD have been studied by researchers
including identification of criminal groups
\citep{sarvariConstructingAnalyzingCriminal2014}, social bot detection
\citep{karatasReviewSocialBot2017}, targeted marketing
\citep{mosadeghUsingSocialNetwork2011}, and public health/disease
control \citep{salatheDynamicsControlDiseases2010}.

With the explosion of human- and machine-generated data, often collected
by social platforms, more datasets are emerging having rich temporal
information that can be studied. CD operates only on static networks.
Meaning that their temporal dimension is often omitted, which often does
not yield a good representation of the real world, where networks
constantly evolve. Such networks are often referred to as dynamic
networks as their components such as nodes and edges may appear and fade
from existence. Accordingly community detection on such dynamic networks
is called Dynamic Community Detection (DCD).

DCD algorithms incorporate additional temporal data are often able to
both outperform their counterpart CD algorithms
\citep{granellBenchmarkModelAssess2015, liuCommunityDetectionMultiPartite2016, faniUserCommunityDetection2020, rossettiANGELEfficientEffective2020},
as well as provide additional information about communities for analysis
\citep{pallaQuantifyingSocialGroup2007}. This additional information
comes in form of community events such as (birth, growth, split,
merging, and death) or in form of the ability to track the membership of
certain individuals over time.

\hypertarget{challenges-in-community-detection}{%
\subsection{Challenges in Community
Detection}\label{challenges-in-community-detection}}

DCD is seen as the hardest problem within Social Network Analysis. The
reason for this is mainly because DCD, unlike CD, also involves tracking
the found communities over time. This tracking relies on the consistency
of the detected communities, as usually slight changes to the network
may cause a different community membership assignment. Not properly
accounting for this uncertainty may cause community and result drift
\citep{dakicheTrackingCommunityEvolution2019}.

Additionally, the increasing richness of the data is not only limited to
temporal data. The real-world data often connects entities of different
modalities. This multi-modality occurs through the fact that the
entities and relations themselves may be of different types (meta
topology-based features). For example users, topics, and documents in a
social network (or vehicles and landmarks in a traffic network). More
complex networks may include asymmetric relationships, and temporal
networks may include appearing, disappearing, or streaming edges/nodes.

Another example of multi-modality in networks comes in form of node and
relation features (content-based features). These features may come in
form of structured (numerical, categorical, or vector data) or
unstructured data such as images and text. It is of high importance to
explore this multi-modal data as it may not always be possible to
explain the formation of communities using network structural
information alone.

Finally, a more systematic issue is that there is no common definition
for a community structure. Within networks, it is usually described in
terms of membership assignment, while in more content-based settings
communities are described in terms of modeled topics (that usually
represent interest areas) or distributions over latent similarity space.
Both definitions have their shortcomings as they often fail to account
for more complex community structures (such as overlapping and
hierarchical communities) and non-linearity of structures often found in
the real world.

Task of community detection is often compared to clustering and graph
clustering, which not always may be a fair comparison as a main focus
point in many CD algorithms is the fact that the amount of communities
is unknown a priori. Communities are not are never planted in the real
world and the algorithms should detect them in an unsupervised manner.

\hypertarget{literature-review}{%
\section{Literature Review}\label{literature-review}}

The problem of dynamic community detection was noticed quite early on
within the SNA community and a considerable amount of research has been
made in order to provide a comprehensive analysis of the network. While
the said research was mostly focused on the discovery of communities
using topologically-based features and node connectivity, the covered
methods did research the limitations and challenges posed by a temporal
context.

In recent years, significant developments have been made in the space of
deep learning. Mainly in the development of new deep learning methods
capable of learning graph-structured data
\citep{bronsteinGeometricDeepLearning2017, hamiltonRepresentationLearningGraphs2018, kipfSemiSupervisedClassificationGraph2017}
which is fundamental for SNA. Because of this, various problems within
the field have been revisited, including community detection problems.
The approaches have been expanded by incorporation of more complex
features, solving the problems concerning multi-modality, and the
introduction of unsupervised learning.

Despite this resurgence, the DCD problem has received little attention.
Though a few efforts have been made to incorporate the deep learning
methods by introducing content-based similarity dynamic, the definition
of unified constraints for end-to-end learning, and usage of graph
representation-based CD algorithms within a temporal context, the
current state of the art leaves a lot to be desired.

We structure the literature as follows: first, we describe the various
interpretations of the Community Structure in
\cref{community-structures}. Next, we explore various approaches and
techniques related to Graph Representation Learning in
\cref{graph-representation-learning}. Then, we provide an overview of
the current state-of-the-art approaches for Community Detection and
Dynamic Community Detection tasks in \cref{link-based-approaches} and
\cref{representation-based-approaches}. Finally, we discuss the ways to
evaluate the said algorithms in \cref{evaluation} and the datasets
available in \cref{datasets}.

\hypertarget{community-structures}{%
\subsection{Community Structures}\label{community-structures}}

The goal of this section is to introduce fundamental structures for the
Dynamic Community Detection task. We do this by combining various
definitions used in the relevant literature as well as establishing the
purpose for these structures, before proceeding into approaches for
detecting communities in the following sections.

\hypertarget{communities}{%
\subsubsection{Communities}\label{communities}}

Communities in real-world networks can be of different kinds: disjoint
(students belonging to different educational institutions), overlapping
(person having membership in different social groups) and hierarchical
(components of a car). One of the main reasons behind the complexity of
CD is that there is not one unique definition what a community actually
is.

The \emph{link-based} (also referred to as classic) community detection
methods intuitively describe communities as groups of nodes within a
graph, such that the intra-group connections are denser than the
inter-group ones. This definition is primarily based on the
\emph{homophily} principle, which refers to the assumption that similar
individuals are those that are densely connected together. Therefore,
these kind of methods look for sub-graph structures such as cliques and
components that identify connectedness within the graph structure to
represent the communities.

Unfortunately, in most cases link-based methods fall short to identify
communities of similar individuals. This is mainly due to two facts: (i)
many similar individuals in a social network are not explicitly
connected together, (ii) an explicit connection does not necessarily
indicate similarity, but may explained by sociological processes such as
conformity, friendship or kinship
\citep{diehlRelationshipIdentificationSocial2007, faniUserCommunityDetection2020}.

A more general definition is introduced in
\citep{cosciaClassificationCommunityDiscovery2011} to create an
underlying concept generalizing all variants found in the literature
(+\citet{thm:community}). In link-based methods, a direct connection is
considered as a particular and very important kind of action, while
newer methods also consider content or interest overlap.

\begin{description}
\item[\protect\hypertarget{thm:community}{}{Community}]
A community in a complex network is a set of entities that share some
closely correlated sets of actions with the other entities of the
community.
\end{description}

\hypertarget{dynamic-communities}{%
\subsubsection{Dynamic Communities}\label{dynamic-communities}}

Similar to how communities can be found in static networks, dynamic
communities extend this definition by utilizing the temporal dimension
to define their life cycle/evolution over a dynamic network. A dynamic
community is characterized by a collection of communities and a set of
transformations on these communities over time.

This persistence of communities across time subjected to progressive
changes is an important problem to tackle. Though, as noted by
\citep{rossettiCommunityDiscoveryDynamic2018} the problem can be
compared to the famous ``the ship of Theseus'' paradox. Because
(verbatim), \emph{deciding if an element composed of several entities at
a given instant is the same or not as another one composed of some---or
even none---of such entities at a later point in time is necessarily
arbitrary and cannot be answered unambiguously}.

Most of the works agree on two atomic transformations on the
communities, including node/edge appearance and vanishing. While some
such as
\citep[\citet{cazabetUsingDynamicCommunity2012}]{pallaQuantifyingSocialGroup2007, asurEventbasedFrameworkCharacterizing2009}
define a more extensive set of transformations (also referred to as
events) which may be more interesting for analytical purposes:

\begin{itemize}
\tightlist
\item
  Birth, when a new community emerges at a given time.~
\item
  Death, when a community disappears. All nodes belonging to this
  community lose their membership.
\item
  Growth, when a community acquires some new members (nodes).
\item
  Contraction, when a community loses some of its members.
\item
  Merging, when several communities merge to form a new community.
\item
  Splitting, when a community is divided into several new ones.
\item
  Resurgence, when a community disappears for a period and reappears.
\end{itemize}

These events/transformations are often not explicitly used during the
definition and/or representation of dynamic communities. Nevertheless,
most of the methods covered in the following sections do define a way in
their algorithm to extract such events from the resulting data.

Finally, it is important to note that dynamic networks can differ in
representation. They can be represented as either a time series of
static networks (also referred to as snapshots) or as a real-time stream
of edges (referred to as temporal networks). Within the global context
of dynamic community detection, they can be seen as equivalent as the
conversion between the two representations can be done in a lossless
way. The latter, temporal networks are often used to handle incremental
changes to the graph and are most commonly applied within real-time
community detection settings.

\hypertarget{graph-representation-learning}{%
\subsection{Graph Representation
Learning}\label{graph-representation-learning}}

The representation-based approaches stem from the field of computational
linguistics which relies heavily on the notion of \emph{distributional
semantics} stating that words occurring in similar contexts are
semantically similar. Therefore the word representations are learned as
dense low-dimensional representation vectors (embeddings) of a word in a
latent similarity space by predicting words based on their context or
vice versa
\citep{mikolovEfficientEstimationWord2013, penningtonGloveGlobalVectors2014}.
Using the learned representations similarity, clustering and other
analytical metrics can be computed.

The success of these representation learning approaches has spread much
farther than just linguistics as similar ideas are also applied to other
fields including graph representation learning. Methods such as deepwalk
\citep{perozziDeepWalkOnlineLearning2014}, LINE
\citep{tangLINELargescaleInformation2015}, and node2vec
\citep{groverNode2vecScalableFeature2016} use random walks to sample the
neighborhood/context in a graph (analogous to sentences in linguistic
methods) and output vector representations (embeddings) that maximize
the likelihood of preserving the topological structure of the nodes
within the graph.

Whereas previously the structural information features of graph entities
had to be hand-engineered, these new approaches are data-driven, save a
lot of time labeling the data, and yield superior feature/representation
vectors. The methods can be trained to optimize for \emph{homophily} on
label prediction or in an unsupervised manner on link prediction tasks.

Newer approaches introduce the possibility for the fusion of different
data types. GraphSAGE
\citep{hamiltonInductiveRepresentationLearning2018} and Author2Vec
\citep{wuAuthor2VecFrameworkGenerating2020} introduce a methodology to
use node and edge features during the representation learning process.
Other approaches explore ways to leverage heterogeneous information
present within the network by using \emph{metapath} based random walks
(path defined by a series of node/link types)
\citep{dongMetapath2vecScalableRepresentation2017} or by representing
and learning relations as translations within the embedding space
\citep{bordesTranslatingEmbeddingsModeling2013}. In
\citet{nguyenContinuousTimeDynamicNetwork2018} the authors introduce a
way to encode temporal information by adding chronological order
constraints to various random walk algorithms. Other relevant
advancements within the field include Graph Convolutional Networks (GCN)
\citep{kipfSemiSupervisedClassificationGraph2017a} and (Variational)
Graph Auto-Encoders (GAE) \citep{kipfVariationalGraphAutoEncoders2016}
which present more effective ways to summarize and represent larger
topological neighborhoods or whole networks.

\hypertarget{link-based-approaches}{%
\subsection{Link-based Approaches}\label{link-based-approaches}}

Link-based approaches to (Dynamic) Community Detection rely on
connection strength to find communities within the network. The main
criteria for communities are the assumed property that intra-group
connections are denser than inter-group ones. The networks are
partitioned in such a way, that optimizes for a defined measure
characterizing this property.

We start this section by covering the fundamentals of link-based
community detection by introducing commonly used community quality
measures and algorithms for optimizing them. Next, we introduce the
link-based DCD problem and the unique challenges that arise as opposed
to CD. Then we proceed to cover the current state of the art by
describing the related works, their solutions to the said challenges,
and possible extensions to the problem.

\hypertarget{community-detection-1}{%
\subsubsection{Community Detection}\label{community-detection-1}}

Different metrics exist quantifying the characteristic
of~\emph{homophily}~over edge strength. The most common metric is
Modularity which measures the strength of the division of a network into
modules (communities). Its popularity stems from the fact that it is
bounded and cheap to compute, though it has other problems such as
resolution limit (making detecting smaller communities difficult). Other
metrics that can be found in the literature include but are not limited
to:

\begin{itemize}
\tightlist
\item
  Conductance: the percentage of edges that cross the cluster border
\item
  Expansion: the number of edges that cross the community border
\item
  Internal Density: the ratio of edges within the cluster with respect
  to all possible edges
\item
  Cut Ratio and Normalized Cut: the fraction of all possible edges
  leaving the cluster
\item
  Maximum/Average ODF: the maximum/average fraction of nodes' edges
  crossing the cluster border
\end{itemize}

\hypertarget{modularity}{%
\paragraph{Modularity}\label{modularity}}

Modularity directly measures the density of links inside a graph and is
therefore computed on communities (sets of nodes) individually by
weighing edges based on community similarity (or exact matching).
Calculation of modularity is done by aggregating for each pair of nodes
the difference between the expected connectivity (amount of edges
between the nodes) and the actual connectivity (existence of an edge)
given their degrees (\cref{eq:modularity}). The final result represents
the delta difference by how much the given graph exceeds a random graph
as expected connectivity is determined by a random rewiring graph.
Because intra-community pairs are weighted lower than inter-community
pairs the score can vary.

\begin{equation}
Q=\frac{1}{2 m}\sum_{v w}\sum_{r}\left[\overbrace{A_{v w}}^{\text{Connectivity}}-\underbrace{\frac{k_{v} k_{w}}{2 m}}_{\text{Expected Connectivity}}\right] \overbrace{S_{v r} S_{w r}}^{\text{Community Similarity}}
\label{eq:modularity}\end{equation}

\hypertarget{louvain-method}{%
\paragraph{Louvain Method}\label{louvain-method}}

Finding an optimal partition of a graph into communities is an NP-hard
problem. This is because, while calculating the modularity score can be
done in a timely manner, still all possible node to community
assignments have to be considered. Therefore heuristic-based methods
such as the Louvain method are usually used.

Louvain method \citep{blondelFastUnfoldingCommunities2008} is a
heuristic-based hierarchical clustering algorithm. It starts by
assigning each node in the graph to its own community. Then it merges
these communities by checking for each node the change in modularity
score produced by assigning it to a neighbor community (based on the
existence of a connection). Once the optimal merges are performed, the
resulting communities are grouped into single nodes and the process is
repeated.

Since modularity changes can be computed incrementally, the complexity
of this method is \(O\left(n \log n\right)\). Additionally, due to the
flexibility of the modularity measure, it allows detecting communities
in graphs with weighted edges.

\hypertarget{label-propagation-algorithm}{%
\paragraph{Label Propagation
algorithm}\label{label-propagation-algorithm}}

Another way to sidestep the complexity issue is using the Label
Propagation algorithm as it uses the network structure directly to
define partitions and doesn't require any priors (quality metrics). The
intuition for the Label Propagation algorithm is as follows: When
propagating a label through connections in the network, a single label
quickly becomes dominant within a group of densely connected nodes, but
these labels usually have trouble crossing sparsely connected regions.

The algorithm starts by assigning each node their own label. After that,
for each iteration, each node updates its label to the majority label
among its neighbors where ties are broken deterministically. The
algorithm stops after a fixed amount of iterations or once it has
converged. An important feature of this algorithm is that a preliminary
solution can be assigned before each run, therefore only updating
existing membership assignments.

\hypertarget{dynamic-community-detection}{%
\subsubsection{Dynamic Community
Detection}\label{dynamic-community-detection}}

Dynamic Community Detection can be seen as an extension to community
detection by the addition of the Community Tracking task. Tracking
relies on the coherency and stability of found communities to define
their evolution through time. The said properties are not be taken for
granted and introduce new challenges when designing DCD methods. The
main issue is in fact that they are competitive with each other causing
a trade-off between community coherency/quality and community temporal
stability.

Various strategies dealing with this trade-off are categorized by
\citet{rossettiCommunityDiscoveryDynamic2018} and
\citet{dakicheTrackingCommunityEvolution2019} where authors reach a
consensus over three main groups. In the following sections, we briefly
introduce these strategies and describe the current state of the art in
similar order.

\hypertarget{independent-community-detection-and-matching}{%
\paragraph{Independent Community Detection and
Matching}\label{independent-community-detection-and-matching}}

Also referred to as the two-stage approach. Works by splitting the DCD
task into two stages. The first stage applies CD directly to every
snapshot of the network. Followed by the second stage matching the
detected communities between the subsequent snapshots.

The advantages of this approach include the fact that it allows for use
of mostly unmodified CD algorithms for the first step and that it is
highly parallelizable as both detection and matching steps can be
applied to each snapshot independently. The main disadvantage is the
instability of underlying CD algorithms which may disrupt the community
matching process. Many CD methods may give drastically different results
in response to slight changes in the network. During the matching, it
becomes difficult to distinguish between this algorithm instability and
the evolution of the network.

In \citet{wangCommunityEvolutionSocial2008} the authors circumvent this
instability issue by looking at the most stable part of the communities,
namely core/leader nodes. In their research, they observe that in
various datasets most of the nodes change dramatically while only a
small portion of the network persists stably. To exploit this feature,
the algorithm CommTracker is introduced which first detects the said
core nodes, and then defines rules to both extract communities as well
as their evolutional events. The community members are assigned based on
their connectivity relative to core nodes.

\citet{rossettiANGELEfficientEffective2020} proposes a way to detect
overlapping communities in dynamic networks. A more robust two-phase
community detection method (ANGEL) is proposed to ensure the stability
of the found communities. The first phase extracts and aggregates local
communities for each node by applying Label Propagation on their
Ego-Graph (graph with the said node removed). Because the found
communities are biased due to a partial view of the network, they are
merged in the second step based on their overlap yielding more stable
communities with the possibility of overlap. During the matching for
each snapshot, a step forward and backward in time is considered where
community splits and merges are detected by reusing the matching
criteria of the second phase of the CD step.

\hypertarget{dependent-community-detection}{%
\paragraph{Dependent Community
Detection}\label{dependent-community-detection}}

Dependent Community Detection strategy works detecting communities in
each snapshot based on the communities found in the previous snapshots.
This approach introduces temporal smoothness because the previous
solution is reused making the matching process obsolete. Though as part
of the described trade-off it can impact the long-term coherence of the
dynamic communities. Mainly, because each step introduces a certain
amount of error into the results (community drift) which may get
amplified within further iterations (error accumulation). Another
disadvantage is the fact that the strategy has limited possibilities of
parallelization due to its dependent nature.

To lessen the complexity of continuous re-detection of the communities
some algorithms process the changes incrementally by limiting the change
to a local neighborhood. While this approach has many benefits, it is
important to note that these algorithms face a problem where only
applying local changes can cause communities to drift toward invalid
ones in a global context.

\citet{heFastAlgorithmCommunity2015} introduces an efficient algorithm
by modifying the Louvain method algorithm. Based on the observation that
between consecutive timesteps only a fraction of connections changes and
do not affect communities dramatically, they argue that if all community
nodes remain unchanged, the community also remains unchanged. With this
in mind, they make a distinction between two types of nodes, ones that
change the connection in a snapshot transition and ones that do not. The
former have to be recomputed, while the latter maintain their community
label. The nodes that maintain their community are merged into community
nodes with edges to other community nodes and changed nodes weighted
proportionally to their real connectivity (amount of edges when
ungrouped). This simplified graph is passed to the Louvain method
algorithm for community detection. By reusing the community assignments
temporal smoothness is maintained and due to the incremental nature of
this algorithm, the overall complexity remains low.

\citet{guoDynamicCommunityDetection2016} envision the target network as
an adaptive dynamical system, where each node interacts with its
neighbors. The interaction will change the distances among nodes, while
the distances will affect the interactions. The intuition is that nodes
sharing the same community move together, and the nodes in different
communities keep far away from each other. This is modeled by defining
the so-called Attractor algorithm, which consists of three interaction
patterns that describe how node connection strength is influenced by
neighboring nodes. The edge weights are initialized using Jaccard
distance and the propagation is run until convergence. The communities
can be extracted by thresholding on edge weight/distance. Thereafter,
all changes are treated as network disturbances. The disturbance can be
limited to a certain area using a disturbance factor which defines a
bound on the possible propagation.

More recently the \citet{yinMultiobjectiveEvolutionaryClustering2021}
has proposed an evolutionary algorithm by looking at the DCD from an
Evolutionary Clustering Perspective. They detect community structure at
the current time under the guidance of one obtained immediately in the
past by simultaneously optimizing for community quality score
(modularity) and community similarity between subsequent time steps
(NMI). In the methodology, a way is proposed to encode a graph
efficiently into a genetic sequence. Additionally, new mutation and
crossover operators are proposed which maximize either of the two
objectives. By using a local search algorithm, building a diverse
initial population, and selecting for dominant candidates the
communities maximizing both objectives are obtained.

\hypertarget{simultaneous-community-detection}{%
\paragraph{Simultaneous community
detection}\label{simultaneous-community-detection}}

The final strategy we consider sidesteps the matching issue by
considering all snapshots of the dynamic network at once. This is done
by flattening the network in the temporal dimension and coupling edges
between the same nodes at different timesteps. These approaches usually
don't suffer from instability or community drift. The disadvantages
include that the standard principle of a unique partition for each time
step can not be applied, limiting the number of possible algorithms.
Handling real-time changes to the graph are also usually not considered.

\citet{muchaCommunityStructureTimeDependent2009} adopt a simple yet
powerful solution to this problem by connecting identical nodes between
different time steps within the unified network. On this network, they
apply a modified Louvain method algorithm to extract the communities
whose members can be split over different timesteps.

\citet{ghasemianDetectabilityThresholdsOptimal2016} apply stochastic
block model-based approach. They make a distinction between two edge
types: (i) spatial edges (edges between neighbors) and (ii) temporal
edges (edges connection nodes in different timesteps). Using this
distinction, they define a Belief Propagation equations to learn
marginal probabilities of node labels (\(\mu^i_s(t)\)) over time.
Additionally in their research, they introduce a way to derive a limit
to the detectability of communities. This is, because some communities
may not be detectable as their probability nears that of chance.

\hypertarget{representation-based-approaches}{%
\subsection{Representation-based
Approaches}\label{representation-based-approaches}}

The main difference between Representation-based approaches and
link-based approaches is the fact that they usually don't directly model
the network based on connections. Instead they learn an intermediate
representation of the graph or its components to which CD detection can
be applied to. While also relying on the idea of \emph{homophily}, most
of methods define additional objectives to improve community quality.

The main reasoning for this is the fact that real-world networks are
non-linear, meaning that there may be no connections when then make
sense and vice versa \citep{wangEvolutionaryAutoencoderDynamic2020}. By
using deep neural networks to learn these embeddings one can address
such non-linearity as they are in general very robust against noise.
Other notable benefits to using representation-based approaches include
the fact that they compress the data efficiently as real-world networks
are very sparse. They can also represent multi-modal features, network
(meta) structure and temporal dimension by defining them all in a
compatible similarity space or learning mappings to this space. Finally,
representations are naturally easy to compute similarity on.

In this section we describe representation based approaches by covering
both CD and DCD approaches. To provide a more cohesive overview of the
methods, we group them by their innovations instead.

\hypertarget{affiliation-graph-models}{%
\paragraph{Affiliation Graph Models}\label{affiliation-graph-models}}

While arguably not being representational by itself, Affiliation Graph
Network (AGM) model introduced in
\citet{yangCommunityAffiliationGraphModel2012} is very influential
within deep learning / representation learning CD field.

The AGM models a network as a bipartite graph with communities as first
class citizens and is represented by the following equation
\(B(V, C, M, \{p_c\})\), where \(V\) represents nodes, \(C\) set of
communities, \(M\) node-community memberships and \(\{p_c\}\) model
parameters (a single probability \(p_c\) per community). It can model
non-overlapping, overlapping and hierarchical communities by defining
rules on membership sets in \(M\). AGM can used in both generative and
discriminative settings.

The generative scenario goes as follows: Given an AGM \(F\) generate
links between each pair of nodes exceeding a baseline probability \(p\).
The can be done by considering that according to AGM, each pair of nodes
in community \(A\) is connected with a probability \(p_A\). Therefore,
the probability of two nodes having a connection is proportional to the
amount of communities they share (which is defined in the model).

The discriminative scenario is defined as: Given a graph \(G\) and ,
find a model \(F\) that may have generated it. By assuming that the
graph was generated using an AGM, the parameters \(M\), number of
communities \(|C|\) and \(\{p_c\}\) have to be found. Process for
finding such a model to the graph usually max likelihood fitting. AGM is
relaxed to have membership strengths \(F_{uC}\), which helps to define
probability of nodes \(u\) and \(v\) connecting through community \(C\)
(\(P_{C}(u, v)\)), and in terms of that by themselves \(P(u, v)\). Using
this a probability \(P(G|F)\) can be constructed quantifying how well
the model fits the data. Finally, gradient ascent can be applied
optimizing the model parameters.

\hypertarget{graph-reinforcement}{%
\subsubsection{Graph Reinforcement}\label{graph-reinforcement}}

The first class of methods we consider are Graph Reinforcement methods.
These methods use representation based learning techniques to to enhance
the graph by adding valuable edges or reduce the noise by removing noisy
connections. This is usually by training a model on a link-prediction
task. A notable benefit of this approach is that other well known CD
methods can be used on the enhanced graph afterwards.

\citet{kangCommunityReinforcementEffective2021} present a CD algorithm
agnostic pre-processing method for strengthening community structure of
a graph by adding non-existing predicted intra-community edges and
deleting existing predicted inter-community edges. Their strategy is to
learn topological embedding using a graph representation learning
algorithm (node2vec) based on existing link prediction task. The
similarity is computed between different node pairs and put into
buckets. Then with assumption of \emph{homophily} the buckets with
higher value can be considered holding intra-community while buckets
with lower inter-community connections. Right buckets are picked from
both extremes to create or delete edges. Preemptive CD is done to
greedily guide pair-wise similarity computation and avoid a high
complexity.

\citet{jiaCommunityGANCommunityDetection2019} solves issue of detecting
overlapping communities by proposing CommunityGAN algorithm which
jointly optimizes for node and community representations. First they
define a method for efficient motif (in their case clique) sampling from
the graph (true/clique, and false/vertex subset). Then, they define a
GAN based structure for learning representational vectors where the
generator \(G\) tries to learn \(p_{true}(m|\mathbf{v}_c)\) as
preference distribution of motifs to generate most likely vertex subsets
most likely to be real motifs. Discriminator \(D\) tries to learn
probability of a vertex subset being a real motif, therefore creating a
minimax game of progressively optimizing embeddings to be able to encode
rich information about network topology.

Both components (\(G\) and \(D\)) are implemented as a modified the
relaxed AGM model with a to a more general definition be able to handle
the motif generation (rather than edge generation). The probability of a
set of vertices being a motif is defined in terms of their probability
being a motif through a community, therefore making them community-aware
as they now represent the affiliation weight between a vertex and a
community. Number of communities are chosen by training and testing part
of data on link prediction task.

\hypertarget{multi-objective-optimization}{%
\subsubsection{Multi-objective
optimization}\label{multi-objective-optimization}}

Another subject where representation based approaches excel is
multi-objective optimization. Usually a combined objective is defined in
terms of a community quality, temporal consistency or homophily measure.
These measures in turn use the proximity between the representation to
be able to back-propagate the combined error and optimize the
representation(function) directly.

In \citet{rozemberczkiGEMSECGraphEmbedding2019} authors propose a method
which learns cluster centers along with node embeddings. They define
objective function as a combination of three terms: normalization term
(ensures embeddings are centered at the origin), proximity term (forces
nodes with similar neighborhoods to be embedded close), cluster quality
term (forces nodes to be close to their nearest cluster). Additionally,
a ``social network cost'' or \emph{homophily} term is added as a
regularizer to optimize for proximity between nodes within the same
cluster. During training the clustering coefficient is annealed to
ensure convergence and negative sampling is employed to avoid large
softmax cost.

\citet{yangGraphClusteringDynamic2017} propose a similar idea of
combining embedding and clustering tasks and solving them in an
end-to-end manner. In their work they employ an Deep Denoise Autoencoder
(DAE) to learn topological information of the network by optimizing for
reconstruction loss. To learn cluster/community centers they define
their own GRACE cluster module which first computes soft cluster
assignment matrix \(Q\) by utilizing the embeddings and cluster centers
which contains probabilities \(q_{ik}\) of node \(i\) belonging to
cluster \(k\) . The clustering loss is defined as KL-divergence between
the soft clustering \(Q\) and auxiliary target distribution \(P\) which
is defined by squaring and normalizing the soft-assignments to reinforce
more confident clustering results while preventing formation of too
large clusters. Both embeddings and clustering are optimized
alternatively until convergence.

\citet{maCommunityawareDynamicNetwork2020} proposed a novel approach to
constructing community-aware dynamic network embeddings by leveraging
multi-objective optimization and extending it into a temporal dimension.
They a adopt a Graph Autoencoder structure which works by encoding the
full graph into a lower-dimensional structure and decoding it again into
a graph. Assuming a well tuned network, this allows authors to encode
the network (and its nodes) into more efficient representation vectors
which characterize the network well

The objective function they use is defined by three terms: the
reconstruction error term minimizing the distance between the
ground-truth and the autoencoder output, local structure/homophily
preservation term minimizing first- and second-order proximity between
connected nodes, and the community evolution preservation term
maximizing temporal smoothness of communities at different granularity
levels given their representation as aggregation of their members.

The inital community assignment is generated using Louvain method for
high level communities and using k-means for fine grained communities
given a max community size parameter \(w\). After that, embeddings at
each snapshot are optimized by employing dependent community detection
like strategy.

\citet{wangEvolutionaryAutoencoderDynamic2020} employ a similar to DCD
detection by utilizing the Graph Autoencoder architecture. As an
addition authors add an additional community score term to the objective
function also minimizing the distance between nodes in the same
community. At last K-means is run on the representational vectors to
detect communities at different timesteps while reusing the outputs from
the previous step.

\hypertarget{multi-modal-community-detection}{%
\subsubsection{Multi-modal community
detection}\label{multi-modal-community-detection}}

In \citet{faniUserCommunityDetection2020} the authors describe their
method to identifying user communities through multi-modal feature
learning. First user embeddings are learned based on their temporal
content similarity by looking topics of interest. Per user a heat map is
constructed measuring user's interest over time and topic axes. By
considering users like-minded if their heat maps overlap enough they
train low-dimensional content embeddings spanning this user similarity
space. Next, they use random walk based GNN methods to learn topological
similarity embeddings for network nodes. Finally, they modify the graph
by setting edge weights proportionally to node proximity in this
combined embeddings space. After that, Louvain method is applied to
extract these time and content aware communities.

\hypertarget{evaluation}{%
\subsection{Evaluation}\label{evaluation}}

As described in the previous sections, the definition for both community
and dynamic community may be quite ambiguous. In this section we will
cover how detection and tracking results can be evaluated in a lesser
ambiguous setting to compare various approaches. To disambiguate the
process a little, during evaluation, the resemblance/detection and
matching/tracking tasks are evaluated separately.

\hypertarget{annotated}{%
\subsubsection{Annotated}\label{annotated}}

Evaluation of detected (dynamic) communities becomes much easier when
the \emph{ground truth communities} are provided. The evaluation is then
done by comparing the difference between the produced communities and
the effective ones. To perform this comparison, information theory based
metric Normalized Mutual Information (NMI) is used which converts
community sets to bit-strings and quantifies the ``amount of
information'' can be obtained about one community by observing the other
\citep{lancichinettiDetectingOverlappingHierarchical2009}.

A possible drawback of this measure is that its complexity is quadratic
in terms of identified communities. In
\citep{rossettiNovelApproachEvaluate2016} alternative measure (NF1) with
linear complexity is introduced which similarly to F1 score uses the
trade-off between precision and recall (of the average of harmonic
means) of the matched communities. In the follow-up work
\citep{rossettiANGELEfficientEffective2020} the authors describe a way
to apply this measure within the context of DCD by calculating this
score for all the snapshots and aggregating the results into one single
measure.

In real-world there are usually no ground truth communities. Therefore
this approach is usually applied on synthetic datasets where the
communities and their dynamicity is sampled from a distribution.
Alternative approach some papers take is by defining ground truth
communities using the metadata and node attributes present within the
datasets. Some datasets may include annotated communities, but this is
not common within DCD datasets.

\hypertarget{metric-based}{%
\subsubsection{Metric based}\label{metric-based}}

Evaluation of detected (dynamic) communities becomes much easier when
the \emph{ground truth communities} are provided. The evaluation is then
done by comparing the difference between the produced communities and
the effective ones. To perform this comparison, information theory based
metric Normalized Mutual Information (NMI) is used which converts
community sets to bit-strings and quantifies the ``amount of
information'' can be obtained about one community by observing the other
\citep{lancichinettiDetectingOverlappingHierarchical2009}.

A possible drawback of this measure is that its complexity is quadratic
in terms of identified communities. In
\citep{rossettiNovelApproachEvaluate2016} alternative measure (NF1) with
linear complexity is introduced which similarly to F1 score uses the
trade-off between precision and recall (of the average of harmonic
means) of the matched communities. In the follow-up work
\citep{rossettiANGELEfficientEffective2020} the authors describe a way
to apply this measure within the context of DCD by calculating this
score for all the snapshots and aggregating the results into one single
measure.

In real-world there are usually no ground truth communities. Therefore
this approach is usually applied on synthetic datasets where the
communities and their dynamicity is sampled from a distribution.
Alternative approach some papers take is by defining ground truth
communities using the metadata and node attributes present within the
datasets. Some datasets may include annotated communities, but this is
not common within DCD datasets.

\hypertarget{task-specific}{%
\subsubsection{Task specific}\label{task-specific}}

In \citep{peelGroundTruthMetadata2017} the authors criticize these
evaluation approaches by proving that they introduce severe theoretical
and practical problems. For one, they prove the no free lunch theorem
for CD, ie. they prove that algorithmic biases that improve performance
on one class of networks must reduce performance on others. Therefore,
there can be no algorithm that is optimal for all possible community
detection tasks, as quality of communities may differ by the optimized
metrics. Additionally, they demonstrate that when a CD algorithm fails,
the poor performance is indistinguishable from any of the three
alternative possibilities: (i) the metadata is irrelevant to the network
structure, (ii) the metadata and communities capture different aspects
of network structure, (iii) the network itself lacks structure.
Therefore, which community is optimal should depend on it's subsequent
use cases and not a single measure.

\hypertarget{datasets}{%
\subsection{Datasets}\label{datasets}}

\hypertarget{synthetic-datasets}{%
\subsubsection{Synthetic Datasets}\label{synthetic-datasets}}

\begin{tablenos:no-prefix-table-caption}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3247}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6753}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Paper
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule
\endhead
\citet{lancichinettiBenchmarkGraphsTesting2008} & Static networks
(widely used) \\
\citet{greeneTrackingEvolutionCommunities2010} & Generate Graphs based
on Modularity measure \\
\citet{granellBenchmarkModelAssess2015} & \\
\citet{hamiltonRepresentationLearningGraphs2018} & Generate Time
dependent Heterogeneous graphs using modularity optimization and
multi-dependency sampling \\
SYN - \citet{ghalebiDynamicNetworkModel2019} & \\
SBM - \citet{lancichinettiBenchmarksTestingCommunity2009} & extracted
from the dynamic Stochastic Block Model \\
\bottomrule
\end{longtable}

\end{tablenos:no-prefix-table-caption}

\hypertarget{real-world-datasets}{%
\subsubsection{Real World Datasets}\label{real-world-datasets}}

\begin{tablenos:no-prefix-table-caption}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4894}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5106}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Dataset
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule
\endhead
\href{https://www.cs.cmu.edu/~./enron/}{Enron} & Includes: Persons,
Email Categories, Sentiment, Email Content \\
\href{https://i11www.iti.kit.edu/en/projects/spp1307/emaildata}{KIT}
(dead) & \\
\href{http://www.wise2012.cs.ucy.ac.cy/challenge.html}{Weibo} &
Includes: Persons, Tweets, Followers; \textbf{Excludes: Tweet
Content} \\
\href{https://www.isi.edu/~lerman/downloads/digg2009.html}{Digg} &
Includes: Persons, Stores, Followers, Votes; \textbf{Excludes:
Content} \\
\href{http://snap.stanford.edu/data/soc-sign-Slashdot090221.html}{Slashdot}
& Includes: Persons, Votes; \textbf{Excludes: Content} \\
\href{https://paperswithcode.com/dataset/imdb-binary}{IMDB} & Actor
movie network; Content is implicitly defined \\
\href{https://snap.stanford.edu/data/wiki-RfA.html}{WIKI-RFA} &
Wikipedia Adminitrator Election; Network of Voters and Votees. Links are
votes and vote comments \\
\href{http://socialnetworks.mpi-sws.org/data-wosn2009.html}{FB-wosn} &
User friendship links and User posts on users walls; \textbf{Excludes:
Content} \\
\href{https://wis.st.ewi.tudelft.nl/research/tweetum/}{TweetUM} (dead) &
Twitter Tweets, User Profiles and Followers; Includes: Content \\
\href{https://arxiv.org/abs/2001.08435}{Reddit Pushift} & User
Submissions and Posts on Subreddits; With timestamps \\
\href{https://snap.stanford.edu/data/soc-sign-bitcoin-otc.html}{Bitcoin
Trust Network} & Network Nodes and peer Ratings; With timestamps \\
\href{http://ocelma.net/MusicRecommendationDataset/lastfm-1K.html}{LastFM1k}
& User - Song Listen histories; With timestamps \\
\href{https://grouplens.org/datasets/movielens/25m/}{MovieLens25M} &
Users and Movie Ratings; With timestamps \\
\href{https://snap.stanford.edu/data/memetracker9.html}{Memetracker}
& \\
\href{https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0168344}{Rumor
Detection} & Rumor Detection over Varying Time Windows; Twitter data;
With timestamps \\
\bottomrule
\end{longtable}

\end{tablenos:no-prefix-table-caption}

\hypertarget{research-questions}{%
\section{Research Questions}\label{research-questions}}

\hypertarget{approach}{%
\section{Approach}\label{approach}}

\renewcommand\refname{Planning}
\bibliography{../../refs.bib}

\end{document}
