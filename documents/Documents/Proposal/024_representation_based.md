## Representation-based Approaches

% * Work similarly as link-based methods
%   * Based on the idea of homophily (connected nodes are similar)
% * Instead of directly modelling based on connection
%   * They learn an intermediate representation
%   * Benefits is that it solves non-linearity problem real-world networks have
%   * Representations are more robust against noise
% * The representations are beneficial because
%   * They can encode auxiliary objectives
%   * Multi-modal features such as content
%   * Network (meta) structure and Temporal dimension
%   * Are naturally easy to compute similarity on

The main difference between Representation-based approaches and link-based approaches is the fact that they usually don't directly model the network based on connections. Instead, they learn an intermediate representation of the graph or its components to which CD detection can be applied to. While also relying on the idea of *homophily*, most methods define additional objectives to improve community quality.

The main reasoning for this is the fact that real-world networks are non-linear, meaning that there may be no connections when they make sense and vice versa [@wangEvolutionaryAutoencoderDynamic2020]. By using deep neural networks to learn these embeddings one can address such non-linearity as they are in general very robust against noise. Other notable benefits to using representation-based approaches include the fact that they compress the data efficiently as real-world networks are very sparse. They can also represent multi-modal features, network (meta) structure, and temporal dimension by defining them all in a compatible similarity space or learning mappings to this space. Finally, representations are naturally easy to compute similarity on.

In this section, we describe representation-based approaches by covering both CD and DCD approaches. To provide a more cohesive overview of the methods, we group them by their innovations instead.



#### Affiliation Graph Models

% @yangCommunityAffiliationGraphModel2012
% 
% AGM: Affiliation Graph Model: a generative model $B(V, C, M, \{p_c\})$ for graphs
% 
% * $V$ vertices, $C$ communities, $M$ community memberships,
% * ${p_c}$ model parameters (probability $p_c$ per community $c$ indicating a connection strength.)
% * Can model variety of community structures: (all have distinct characteristics in M)
%   * Non-overlapping, Overlapping and hierarchical (TODO: add figure?)
% * Usage: Generative Model: (generating edges)
%   * generates links for each pair of nodes in community $A$ with probability $p_a$ 
%   * The more communities they have in common, means higher probability for a connection
%   * If no overlap, then $P(u, v) = \epsilon$
% * Usage: Discriminative Model: Given a graph, find a model $F$ that may have generated it
%   * Assume a model was generated by AGM
%   * Parameters $M$, number of communities $C$, $p_c$
%   * Graph Fitting: using max likelihood estimation
%     * Given a model, it can be compared to data using log likelihood
%     * AGM is relaxed to have edge strength
%     * Using the relaxed model memberships now have strengths $F_uC$
%     * Probability of two nodes connecting is defined in terms of them connecting through a community
%     * Gradient descent is used to find the optimal parameters

While arguably not being representational by itself, Affiliation Graph Network (AGM) model introduced in @yangCommunityAffiliationGraphModel2012 is very influential within the deep learning/representation learning CD field.

The AGM models a network as a bipartite graph with communities as first-class citizens and is represented by the following equation $B(V, C, M, \{p_c\})$, where $V$ represents nodes, $C$ set of communities, $M$ node-community memberships and $\{p_c\}$ model parameters (a single probability $p_c$ per community). It can model non-overlapping, overlapping, and hierarchical communities by defining rules on membership sets in $M$. AGM can be used in both generative and discriminative settings.

The generative scenario goes as follows: Given an AGM $F$ generates links between each pair of nodes exceeding a baseline probability $p$. The can be done by considering that according to AGM, each pair of nodes in community $A$ is connected with a probability $p_A$. Therefore, the probability of two nodes having a connection is proportional to the number of communities they share (which is defined in the model).

The discriminative scenario is defined as: Given a graph $G$ and, find a model $F$ that may have generated it. By assuming that the graph was generated using an AGM, the parameters $M$, number of communities $|C|$ and $\{p_c\}$ have to be found. Process for finding such a model to the graph usually max likelihood fitting. AGM is relaxed to have membership strengths $F_{uC}$, which helps to define the probability of nodes $u$ and $v$ connecting through community $C$ ($P_{C}(u, v)$), and in terms of that by themselves $P(u, v)$. Using this a probability $P(G|F)$ can be constructed quantifying how well the model fits the data. Finally, gradient ascent can be applied to optimize the model parameters.



### Graph Reinforcement

% Class of methods for Community Detection: Graph Augmentation
% 
% * These type of method use representation learning techniques to 
%   * Enhance the graph (augmentation) by adding valuable edges
%   * Remove noise by adding missing connections or removing noisy ones
% * TODO: non-linearity, sparsity of real-world graphs

The first class of methods we consider are Graph Reinforcement methods. These methods use representation-based learning techniques to enhance the graph by adding valuable edges or reducing the noise by removing noisy connections. This is usually done by training a model on a link-prediction task. A notable benefit of this approach is that other well-known CD methods can be used on the enhanced graph afterward.



% @kangCommunityReinforcementEffective2021
% 
% * Present a **Community Reinforcement** approach
% * Is CD algorithm agnostic
%   * Shown in experiments - therefore the graph itself benefits
% * Reinforces the Graph by
%   * Creating inter-community edges
%   * Deleting intra-community edges
%   * Determines the appropriate amount of reinforcement a graph needs
% * Which helps the dense inter-community and sparse intra-community property of the graph
%   * Can effectively turn difficult-to-detect community situation into that of easy-to-detect communities
% * Challenges:
%   * Needs to be unsupervised (doesn't need community annotations to work)
%   * Appropriate amount of reinforcement needs to be determined (otherwise noise is introduced)
%   * Needs to be fast, checking every possible edge is infeasible
% * Methodology:
%   * Edge Addition / Deletion
%     * Based on node similarity of connected pairs
%       * Similar nodes are likely to be in a community (intra edges)
%       * Dissimilar ones are less likely to be in a community (inter edges)
%     * Employ graph embedding to generate topological embeddings
%       * Adamic/Adar (local link-based)
%       * SimRank (gloabl link-based)
%       * Node2vec graph embedding based
%     * Predict similarities and bucket them
%       * Use similarity buckets to select intra and inter edges - and to tune the model
%       * Buckets are selected on how well they predict current edges
%   * Detect the right amount of addition
%     * Use a gradual reinforcement strategy by generating a series of graphs (adding top x inter and removing intra edges)
%     * Pick the best graph using a scoring function (modularity)
%     * Simply run CD over the graph and see
%   * Reducing computational overhead
%     * Using a greedy similarity computation
%     * Prefer nodes which are likely to be in same community of inter similarity detection

@kangCommunityReinforcementEffective2021 present a CD algorithm agnostic pre-processing method for strengthening the community structure of a graph by adding non-existing predicted intra-community edges and deleting existing predicted inter-community edges. Their strategy is to learn topological embedding using a graph representation learning algorithm (node2vec) based on the existing link prediction task. The similarity is computed between different node pairs and put into buckets. Then with the assumption of *homophily* the buckets with a higher value can be considered holding intra-community while buckets with lower inter-community connections. Right buckets are picked from both extremes to create or delete edges. The preemptive CD is done to greedily guide pair-wise similarity computation and avoid a high complexity.



% @jiaCommunityGANCommunityDetection2019
% 
% * Has some info in related work to extend on graph representation learning
% * Solves issue of detecting overlapping communities:
%   * K-means and Gaussian Mixture Model cant do that
% * Proposes CommunityGAN:
%   * Solves graph representation learning and community detection jointly
%   * Embeddings indicate the membership strength of vertices to communities
%   * Make use of Affiliation Graph Model AGM for community (detection) assignment
%   * Uses GAN structure to:
%     * Generate most likely vertex subset s to compose a specified kind of motif
%       * "Graph AGM" motif generation model
%     * Discriminate whether vertex subset s is a real motif or not
%     * Motif = in this case is an n-clique
%   * Study motif distribution among ground truth communities to analyse how they impact quality of detected communities
% * Methodology:
%   * Define a method to efficiently random walk such cliques / motifs
%   * Generator tries to learn $p_{true}(m|v_c)$ as preference distribution of motifs containing $v_c$ vs all the motifs
%     * To be able to generate most likely motifs (vertex subsets) similar to real motifs covering $v_c$
%   * Discriminator tries to learn probability of a vertex subset being a real motif
%     * Tries to discriminate between ground truth motifs and not
%   * AGM:
%     * Can define a measure to check whether two nodes are affiliated through a specific (or any) community
%     * Usually applied for edge generation
%     * In this case, extended to motif generation (edge is a 2-clique)
%       * The affiliation is defined now in form of a motif in community
%   * Amount of communities are chosen by hyperparameter tuning

@jiaCommunityGANCommunityDetection2019 solves the issue of detecting overlapping communities by proposing the CommunityGAN algorithm which jointly optimizes for node and community representations. First, they define a method for efficient motif (in their case clique) sampling from the graph (true/clique, and false/vertex subset). Then, they define a GAN based structure for learning representational vectors where the generator $G$ tries to learn $p_{true}(m|\mathbf{v}_c)$ as preference distribution of motifs to generate most likely vertex subsets most likely to be real motifs. Discriminator $D$ tries to learn the probability of a vertex subset being a real motif, therefore creating a minimax game of progressively optimizing embeddings to be able to encode rich information about network topology.

Both components ($G$ and $D$) are implemented as a modified relaxed AGM model with a more general definition to be able to handle the motif generation (rather than edge generation). The probability of a set of vertices being a motif is defined in terms of their probability being a motif through a community, therefore making them community-aware as they now represent the affiliation weight between a vertex and a community. The number of communities is chosen by training and testing part of data on link prediction task.



### Multi-objective optimization

% * Is another subject where representation based approaches excel
% * Usually multiple objectives are defined such as:
%   * Homophily
%   * Community Quality
%   * Temporal Consistency
% * in terms of given representations to be able to back-propagate the combined error

Another subject where representation-based approaches excel is multi-objective optimization. Usually, a combined objective is defined in terms of a community quality, temporal consistency, or homophily measure. These measures in turn use the proximity between the representation to be able to back-propagate the combined error and optimize the representation(function) directly.



% @rozemberczkiGEMSECGraphEmbedding2019
% 
% * Learns clustering (centers) simultaneously with computing embeddings
% * Objective functions includes:
%   * Term to embed around the origin
%   * Term to force nodes with similar sampled neighborhoods to be embedded close
%   * Term to force nodes to be close to the nearest cluster (weighted by clustering coefficient)
% * Weights are randomly initialized
% * Clustering coefficient is annealed (changes overtime)
% * Uses negative sampling to avoid huge cost of softmax
% * Adds regularizer "social network cost" to optimize for homophily
%   * weighs distance in latent space by neighborhood overlap
%   * Makes algorithm more robust to changes in hyperparameters
% * Evaluate cluster quality by modularity
% * Evaluate embeddings by genre prediction / recommendation

In @rozemberczkiGEMSECGraphEmbedding2019 authors propose a method that learns cluster centers along with node embeddings. They define objective function as a combination of three terms: normalization term (ensures embeddings are centered at the origin), proximity term (forces nodes with similar neighborhoods to be embedded close), cluster quality term (forces nodes to be close to their nearest cluster). Additionally, a "social network cost" or *homophily* term is added as a regularizer to optimize for proximity between nodes within the same cluster. During training, the clustering coefficient is annealed to ensure convergence and negative sampling is employed to avoid large softmax costs.



% @yangGraphClusteringDynamic2017
% 
% * Goal: unsupervised clustering on networks with contents
%   * Propose a way to utilize deep embedding for graph clustering
% * Simultaneously solve node representation problem and find optimal clustering in a e2e manner
%   * Jointly learns embeddings X and soft clustering $q_i \in Q$
%   * $\mathcal{J}*{2}=K L(\mathcal{P} | Q)=\sum*{i} \sum_{k} p_{i k} \log \frac{p_{i k}}{q_{i k}}$: probablility of node $v_i$ belonging to kth cluster
%   * K is known a-priori
% * Employ Deep Denoise Autoencoder (DAE) - good for features with high-dimensional sparse noisy inputs
% * Use stable influence propagation technique (for computing embeddings)
%   * Use a transition matrix for a single step embedding propagation
%   * Because:
%     * Random walk requires more tuning
%     * Their transition matrix is very similar to a spectral method (symmetric Laplacian matrix)
%     * Influence propagation is like kipf and welling - doenst require matrix decomposition
%   * Embedding loss: $\mathcal{J}*{2}=K L(\mathcal{P} | Q)=\sum*{i} \sum_{k} p_{i k} \log \frac{p_{i k}}{q_{i k}}$
% * Introduce GRACE cluster module:
%   * Computes soft clustering Q from: $q_{i k}=\frac{\left(1+\left|\mathbf{x}*{i}-\mathbf{u}*{k}\right|^{2}\right)^{-1}}{\sum_{j}\left(1+\left|\mathbf{x}*{i}-\mathbf{u}*{j}\right|^{2}\right)^{-1}}$
%   * Learn clustering results by learning distribuition P where $p_{i k}=\frac{q_{i k}^{2} / f_{k}}{\sum_{j} q_{i j}^{2} / f_{j}}$
%     * and $f_{k}=\sum_{i} q_{i k}$ total number of nodes softly assigned to kth cluster
%   * Clustering Loss: $\mathcal{J}*{2}=K L(\mathcal{P} | Q)=\sum*{i} \sum_{k} p_{i k} \log \frac{p_{i k}}{q_{i k}}$
%   * Training is done in alternating steps:
%     * Macrostep: Compute: P and fix it
%     * S Microsteps: Update node embeddings S and cluster centers U
%       * Tries to make Q catch up with P

@yangGraphClusteringDynamic2017 propose a similar idea of combining embedding and clustering tasks and solving them in an end-to-end manner. In their work, they employ a Deep Denoise Autoencoder (DAE) to learn topological information of the network by optimizing for reconstruction loss. To learn cluster/community centers they define GRACE cluster module which first computes soft cluster assignment matrix $Q$ by utilizing the embeddings and cluster centers which contains probabilities $q_{ik}$ of node $i$ belonging to cluster $k$. The clustering loss is defined as KL-divergence between the soft clustering $Q$ and auxiliary target distribution $P$ which is defined by squaring and normalizing the soft assignments to reinforce more confident clustering results while preventing the formation of excessively large clusters. Both embeddings and clustering are optimized alternatively until convergence.



% @maCommunityawareDynamicNetwork2020 (use as baseline?)
% 
% * Define communities in terms of large and small scale communities
% * They propose a method for dynamic *community aware* network representation learning
%   * By creating a unified objective optimizing stability of communities, temporal stability and structure representation
%   * Uses both first-order as well as second order proximity for node representation learning
% * They define community representations as average of their members
%   * Adopt a stacked autoencoder to learn low-dimensional (generic) representations of nodes
% * They define loss in terms of:
%   * Reconstruction Error: How well the graph can be reconstructed from the representation
%   * Local structure preservation: Preservation of homophiliy - connected nodes are similar
%   * Community evolution preservation: Preservation of smoothness of communities in time at multiple granularity levels
% * The communities are initialized using classical methods:
%   * First large communities are detected using Genlouvin (fast and doesnt require priors)
%   * Then small scale communities are detected using k-means by defining a max community size w
%     * Which provides more fine tuned communities
% * Using the initial embeddings the temporal embeddings are optimized
%   * Done by optimizing all at once - therefore maintaining the stability
%   * And use of the mentioned combined objective
% * Though they present / evaluate their algorithm in terms of Dynamic Representation Algorithms
%   * Therefore the actual quality of communities remains to be known

@maCommunityawareDynamicNetwork2020 proposed a novel approach to constructing community-aware dynamic network embeddings by leveraging multi-objective optimization and extending it into a temporal dimension. They adopt a Graph Autoencoder structure which works by encoding the full graph into a lower-dimensional structure and decoding it again into a graph. Assuming a well-tuned network, this allows authors to encode the network (and its nodes) into more efficient representation vectors which characterize the network well

The objective function they use is defined by three terms: the reconstruction error term minimizing the distance between the ground-truth and the autoencoder output, local structure/homophily preservation term minimizing first- and second-order proximity between connected nodes, and the community evolution preservation term maximizing temporal smoothness of communities at different granularity levels given their representation as an aggregation of their members.

The initial community assignment is generated using the Louvain method for high-level communities and using k-means for fine-grained communities given a max community size parameter $w$. After that, embeddings at each snapshot are optimized by employing a dependent community detection-like strategy.



% @wangEvolutionaryAutoencoderDynamic2020
% 
% * Approach is similar to to @maCommunityawareDynamicNetwork2020
% * Defines a unified objective where
%   * community characteristics
%   * previous clustering
%   * are incorporated as a regularization term
% * **They argue that real world networks are non-linear** in nature and **classical approaches can not capture this**
%   * Autoencoders can though
% * Methodology:
%   * Construct a similarity matrix using Dice Coefficient (handles varying degrees well)
%   * Apply stacked (deep) autoencoders to learn the low-dimensional representation
%   * Characterizes tradeoff between two costs:
%     * Snapshot cost (SC):
%       * Reconstruction loss
%       * Node embedding similarity (homophiliy) between connected nodes
%       * Node embedding similarity (homophiliy) between nodes in same community
%     * Temporal cost (TC)
%       * Temporal smoothness of node embeddings
%   * Adopt K-means to discover community structure

@wangEvolutionaryAutoencoderDynamic2020 employs a similar to DCD detection by utilizing the Graph Autoencoder architecture. As an addition authors add an additional community score term to the objective function also minimizing the distance between nodes in the same community. At last, K-means is run on the representational vectors to detect communities at different timesteps while reusing the outputs from the previous step.



### Multi-modal community detection

% TODO: Describe and maybe more refs
% 
% * Improves community quality
% * Comes in form of Content or Meta-topological -based data
% * Represented separately and aggregated into a single representation vector
% * Can calculate similarity over these
% * e2e optimization makes these additions viable

Another way to improve community quality is by incorporating multi-modal features. These can come in form of node attributes, content-based or meta-topological data. These representations are incorporated into learned embedding vectors either by direct learning, incorporating them into the objective function, or use of pre-trained models.



% @faniUserCommunityDetection2020
% 
% * Propose a new method of identifying user communities through multi-modal feature learning:
%   * learn user embeddings based on their **temporal content similarity**
%     * Base on topics of interest
%     * Users are considered like-minded if they are interested in similar topics at similar times
%     * Learn embeddings using a context modelling approach
%   * learn user embeddings based on their **social network connections**
%     * Use GNN which works as a skip-gram like approach by generating context using random walks
%   * **interpolate** temporal content-based embeddings and social link-based embeddings
% * Then they use these multi-modal embeddings to detect dynamic communities\
%   * Communities are detected on a modified graph
%     * Weights are set given embedding similarity
%     * Communities are detected using Louvain methods
%   * Then test their approach on specific tasks such as
%   * News recommendation
%   * User for content prediction
% * Note: **This approach detects static communities**
%   * But the communities implicitly take time into account

In @faniUserCommunityDetection2020 the authors describe their method for identifying user communities through multi-modal feature learning. First user embeddings are learned based on their temporal content similarity by looking at topics of interest. Per-user, a heat map is constructed measuring the user's interest over time and topic axes. By considering users like-minded if their heat maps overlap enough they train low-dimensional content embeddings spanning this user similarity space. Next, they use random walk-based GNN methods to learn topological similarity embeddings for network nodes. Finally, they modify the graph by setting edge weights proportionally to node proximity in this combined embeddings space. After that, the Louvain method is applied to extract these time and content-aware communities.



% @wangVehicleTrajectoryClustering2020
% 
% * Transform task of trajectory clustering into one of Dynamic Community Detection
%   * discretion the trajectories by recording entity their current neigbors at each time interval
%   * Edge streaming network is created
% * Use representation learning to learn node their embeddings
%   * Use dyn walks to perform random walks in time dimenstion
%   * Use negative sampling to avoid the softmax cost
% * Then use K-means to find the communities
%   * Try K-means, K-medioids and GMM (Gaussian Mixture Models)
%   * Initalize the centers at the previous timestamp centers
% * Use quality measures to establish quality of results



% Important drawback:
% 
% * Require representation for each node



% **Resources:**
% 
% * https://paperswithcode.com/paper/a-comprehensive-survey-on-community-detection/review/


