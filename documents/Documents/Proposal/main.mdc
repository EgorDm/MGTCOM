
# Introduction and Background

Social Network Analysis (SNA) is a huge part of the Network Science field and is concerned with the process of investigating social structures that occur in the real-world using Network and Graph Theory. These social structures usually include social media networks [@grandjeanSocialNetworkAnalysis2016; @hagenCrisisCommunicationsAge2018], economic transaction networks [@prykeAnalysingConstructionProject2004; @kongWhyAreSocial2011; @swamynathanSocialNetworksImprove2008], knowledge networks [@gruberCollectiveKnowledgeSystems2008; @brenneckeFirmKnowledgeNetwork2017], and disease transmission networks [@morrisNetworkEpidemiologyHandbook2004; @stattnerSocialNetworkAnalysis2011].

One main issue to address while studying this type of real-world events lies in the identification of meaningful substructures hidden within the overall complex system. The SNA is therefore applied to extract patterns from the data usually in form of information flow, identification of high throughput nodes and paths, and discovery of communities and clusters. In this thesis, we are going to focus on the problem of community discovery.

This thesis proposal is structured as follows: in this section, we are going to introduce basic concepts and challenges of Dynamic Community Detection. In +@literature-review a brief literature survey is conducted, identifying the current state of the art and approaches to Dynamic Community Detection. In +@research-questions we will describe the problem we are trying to solve as well as formulate the research questions. In +@approach we will elaborate on our proposed methodology for solving the posed problem and answering the research questions. Finally, in +@planning the concrete planning for the research project is laid out.

## Community Detection

The problem of partitioning a complex network into *communities* which represent groups of individuals with high interaction density, while individuals from different communities have comparatively low interaction density is known as Community Discovery (CD). CD is a task of fundamental importance within SNA as it discloses deeper properties of networks. It provides insight into networks’ internal structure and its organizational principles.

Many useful applications of CD have been studied by researchers including identification of criminal groups [@sarvariConstructingAnalyzingCriminal2014], social bot detection [@karatasReviewSocialBot2017], targeted marketing [@mosadeghUsingSocialNetwork2011], and public health/disease control [@salatheDynamicsControlDiseases2010].

With the explosion of human- and machine-generated data, often collected by social platforms, more datasets are emerging having rich temporal information that can be studied. Most of the CD methods operate only on static networks. Meaning that their temporal dimension is often omitted, which often does not yield a good representation of the real world, where networks constantly evolve. Such networks are often referred to as dynamic networks as their components such as nodes and edges may appear and fade from existence. Accordingly community detection on such dynamic networks is called Dynamic Community Detection (DCD).

DCD algorithms that incorporate additional temporal data are often able to both outperform their counterpart CD algorithms [@granellBenchmarkModelAssess2015; @liuCommunityDetectionMultiPartite2016; @faniUserCommunityDetection2020; @rossettiANGELEfficientEffective2020], as well as provide additional information about communities for analysis [@pallaQuantifyingSocialGroup2007]. This additional information comes in form of community events such as (birth, growth, split, merging, and death) or in form of the ability to track the membership of certain individuals over time.

## Challenges in Community Detection

DCD is seen as the hardest problem within Social Network Analysis. The reason for this is mainly because DCD, unlike CD, also involves tracking the found communities over time. This tracking relies on the consistency of the detected communities, as usually slight changes to the network may cause a different community membership assignment. Not properly accounting for this uncertainty may cause community and result drift [@dakicheTrackingCommunityEvolution2019].

Additionally, the increasing richness of the data is not only limited to temporal data. The real-world data often connects entities of different modalities. This multi-modality occurs through the fact that the entities and relations themselves may be of different types (meta topology-based features). For example users, topics, and documents in a social network (or vehicles and landmarks in a traffic network). More complex networks may include asymmetric relationships, and temporal networks may include appearing, disappearing, or streaming edges/nodes.

Another example of multi-modality in networks comes in the form of node and relation features (content-based features). These features may come in the form of structured (numerical, categorical, or vector data) or unstructured data such as images and text. It is of high importance to explore this multi-modal data as it may not always be possible to explain the formation of communities using network structural information alone.

Finally, a more systematic issue is that there is no common definition for a community structure. Within networks, it is usually described in terms of membership assignment, while in more content-based settings communities are described in terms of modeled topics (that usually represent interest areas) or distributions over latent similarity space. Both definitions have their shortcomings as they often fail to account for more complex community structures (overlapping and hierarchical communities) and non-linearity of structures often found in the real world.

The task of community detection is often compared to clustering and graph clustering, which not always may be a fair comparison as a main focus point in many CD algorithms is the fact that the amount of communities is unknown a priori. Communities are never planted in the real world and the algorithms should detect them in an unsupervised manner.



# Preliminaries

% Goals:
% 
% * Introduce notation and important concepts that we use throughout this proposal
% * Notation is mostly the same as one used in the literature the next chapter is based on

In this section we introduce the notation as well as important concepts that we use throughout this proposal. The notation is mostly the same as one used in the literature the next chapter is based on.



#### Graphs

% * G (V, E)
%   * Attributed edges
%   * Parallel edges

A graph $G$ is a tuple $G = (V, E)$ consisting of $n := |V|$ sequentially numbered nodes $V = \{v_1, ..., v_n\}$ and $m := |E|$ edges. Edges can be directed or undirected, represented as either ordered tuples $(u, v) \in E$ or unordered sets $\{u, v\} \in E$ respectively. Unless stated otherwise we assume that graphs are undirected. An edge $\{u, v\} \in E$ is *incident* to a node $v_i$, if $v_i \in \{u, v\}$. Two nodes $v_i$ and $v_j$ are *adjacent if they are connected by an edge, i.e., $\{v_i, v_j\} \in E$.  The neighbors*  $\mathcal{N}(v_i) = \{v_j|\{v_i, v_j\} \in E\}$ of a node $v_i$ are nodes that are adjacent to it. The *degree* $k_v := |\mathcal{N}(v)|$ of a node $v$ is its number of neighbors. In more mathematical context a graph can represented as an *adjacency matrix* $A$ where $A_{ij}$-th cell is valued $1$ iff edge $\{v_i, v_j\} \in E$. In some cases we also consider *weighted graphs* $G = (V, E, w)$ with weights $w(v_i, v_j) \in \mathbb{R}$. 

#### Multidimensional Networks

% * Parallel edges

Multidimensional network is an edge-labeled extension of multigraphs which allow for multiple edges between same nodes (referred to as *parallel edges*). A multidimensional is defined as $G = (V, E, D)$, where $D$ is a set of dimensions. Each link is a triple $(u, v, d) \in E$ where $u, v \in V$ and $d \in D$.



#### Heterogeneous Graph

A heterogeneous graph $G = (V, E, O)$ extends the notion of a multidimensional network by defining a node type mapping function $\phi: V \rightarrow O_V$ and an edge type mapping function $\psi: E \rightarrow O_E$. Where $O_V$ and $O_E$ denote sets of predefined node and edge types, where $|O_V| + |O_E| > 2$. A *meta-path* $\mathcal{P}$ of length $l$ within a heterogeneous graph is defined in form of $V_{1} \stackrel{R_{1}}{\longrightarrow} V_{2} \stackrel{R_{2}}{\longrightarrow} \cdots \stackrel{R_{l}}{\longrightarrow} V_{l+1}$  (abbreviated as $V_1V_2...V_{l+1}$) which describes a composite relation $\mathcal{R} = R_1 \circ R_2 \circ \cdots \circ R_{l+1}$ between node types $V_1$ till $V_l$. Meta-path based neighbor set $\mathcal{N}^\mathcal{P}(v_i)$ is defned as a set of nodes which connect node $v_i$ via meta-path $\mathcal{P}$.



% * Knowledge Graph
% * $KG = (E, R, A, T^R, T^A)$
% * 



#### Temporal Network

A temporal network [@rossettiCommunityDiscoveryDynamic2018] is a graph $G = (V, E, T)$, where $V$ is a set of triplets in form $(v, t_s, t_e)$, with $v$ a node of graph and $t_s, t_e \in T$ respectively being the birth and death timestamps of the corresponding node (with $t_s \leq t_e$); $E$ is a set of quadruplets $(u, v, t_s, t_e)$ with $u, v \in V$ being vertices of the graph, and $t_s, t_e \in T$ respectively being the birth and death timestamps of the corresponding edge (with $t_s \leq t_e$). Networks without edge durations ($t_e = \infty$) are often referred to as *contact sequences* and those with duration as *interval graphs*. Differently, a distinction is made between two types of temporal network, *interaction networks* and *relation networks*. The former model iterations that can repeat as time goes by, while the latter model more stable relationships (friendship, coworker, belonging to same group, etc.).



#### Snapshot Network

A snapshot network $\mathcal{G}_\tau$ [@dakicheTrackingCommunityEvolution2019] is defined by an ordered set $\langle G_1, ... G_\tau \rangle$ of $\tau$ consecutive snapshots, where $G_i = (V_i, E_i)$ represents a graph with only the set of nodes and edges that appear in the interval $(t_i, t_{i+1})$. It is worth noting that a temporal network can be discretized into a snapshot network by partitioning it into a series of snapshots at desired resolution.



#### Complex Network

A complex network is a graph with non-trivial topological features, which do not occur in simple networks such as lattices or random graphs, but often occur in networks representing real systems. A few common features occurring in complex networks include, scale-free networks where the degree distribution follows the power law (implying that degree distribution has no characteristic scale); small-world networks which exhibit small-worldness phenomenen where diameter the network (degree of separation) of is usually small while the clustering coefficient is high; multidimenssional networks; and spatial networks where nodes are embedded in space.





#### Normalized Mutual Information (NMI)

Normalized Mutual Information is a popular measure used to evaluate network partitioning. It is a variant of a common measure in information theory called Mutual Information defined by $I(X; Y) = H(X) - H(X| Y)$ and representing reduction in entropy of variable $X$ by observing the random variable $Y$ or vice versa. In context of network partitioning it therefore quantifies the information shared between two partitions. A lower value represents a higher quality partitioning. NMI is defined as $NMI(X; Y) = \frac{I(X; Y)}{H(X) + H(Y)}$ and ensures the resulting value is within a $[0, 1]$ range, therefore allowing for comparison of different size partitions. 

# Literature Review

The problem of dynamic community detection was noticed quite early on within the SNA community and a considerable amount of research has been made in order to provide a comprehensive analysis [@tamimiLiteratureSurveyDynamic2015; @rossettiCommunityDiscoveryDynamic2018; @dakicheTrackingCommunityEvolution2019]. While the said research was mostly focused on the discovery of communities using topologically-based features and node connectivity, the covered methods did research the limitations and challenges posed by a temporal context.

In recent years, significant developments have been made in the space of deep learning. Mainly in the development of new deep learning methods capable of learning graph-structured data [@bronsteinGeometricDeepLearning2017; @hamiltonRepresentationLearningGraphs2018; @kipfSemiSupervisedClassificationGraph2017] which is fundamental for SNA. Because of this, various problems within the field have been revisited, including community detection problems. The approaches have been expanded by incorporation of more complex features, solving the problems concerning multi-modality, and the introduction of unsupervised learning.

Despite this resurgence, the DCD problem has received little attention. Though a few efforts have been made to incorporate the deep learning methods by introducing content-based similarity [@faniUserCommunityDetection2020; @cazabetUsingDynamicCommunity2012; @huangInformationFusionOriented2022], the definition of unified constraints for end-to-end learning [@maCommunityawareDynamicNetwork2020; @wangEvolutionaryAutoencoderDynamic2020; @cavallariLearningCommunityEmbedding2017; @jiaCommunityGANCommunityDetection2019], and usage of graph representation-based CD algorithms [@wangVehicleTrajectoryClustering2020; @limBlackHoleRobustCommunity2016] within a temporal context, the current state of the art leaves room for improvement.

% 

We structure the literature as follows: first, we describe the various interpretations of the Community Structure in +@community-structures. Next, we explore various approaches and techniques related to Graph Representation Learning in +@graph-representation-learning. Then, we provide an overview of the current state-of-the-art approaches for Community Detection and Dynamic Community Detection tasks in +@link-based-approaches and +@representation-based-approaches. Finally, we discuss the ways to evaluate the said algorithms in +@evaluation and the datasets available in +@datasets.







## Community Structures

The goal of this section is to introduce fundamental structures for the Dynamic Community Detection task. We do this by combining various definitions used in the relevant literature as well as establishing the purpose for these structures, before proceeding into approaches for detecting communities in the following sections.



% Goals:
% 
% * Give a general definition of important structures



### Communities

Communities in real-world networks can be of different kinds: disjoint (students belonging to different educational institutions), overlapping (person having membership in different social groups) and hierarchical (components of a car). One of the main reasons behind the complexity of CD is that there is not one unique definition what a community actually is.

The *link-based* (also referred to as classic) community detection methods intuitively describe communities as groups of nodes within a graph, such that the intra-group connections are denser than the inter-group ones. This definition is primarily based on the *homophily* principle, which refers to the assumption that similar individuals are those that are densely connected together. Therefore, these kind of methods look for sub-graph structures such as cliques and components that identify connectedness within the graph structure to represent the communities.

Unfortunately, in most cases link-based methods fall short to identify communities of similar individuals. This is mainly due to two facts: (i) many similar individuals in a social network are not explicitly connected together, (ii) an explicit connection does not necessarily indicate similarity, but can explained by sociological processes such as conformity, friendship or kinship [@diehlRelationshipIdentificationSocial2007; @faniUserCommunityDetection2020].

A more general definition is introduced in [@cosciaClassificationCommunityDiscovery2011] to create an underlying concept generalizing all variants found in the literature (+@dfn:community). In link-based methods, a direct connection is considered as a particular and very important kind of action, while newer methods also consider content or interest overlap.

[Community]{#dfn:community}

: A community in a complex network is a set of entities that share some closely correlated sets of actions with the other entities of the community.



% Feedback: Define what a "network" and a "dynamic network" is. (Probably should go into preliminaries)



### Dynamic Communities

Similar to how communities can be found in static networks, dynamic communities extend this definition by utilizing the temporal dimension to define their life cycle/evolution over a dynamic network. A dynamic community is characterized by a collection of communities and a set of transformations on these communities over time.

This persistence of communities across time subjected to progressive changes is an important problem to tackle. Though, as noted by [@rossettiCommunityDiscoveryDynamic2018] the problem can be compared to the famous “the ship of Theseus” paradox. Because (verbatim), *deciding if an element composed of several entities at a given instant is the same or not as another one composed of some—or even none—of such entities at a later point in time is necessarily arbitrary and cannot be answered unambiguously*.

Most of the works agree on two atomic transformations on the communities, including node/edge appearance and vanishing. In  other works such as [@pallaQuantifyingSocialGroup2007; @asurEventbasedFrameworkCharacterizing2009; @cazabetUsingDynamicCommunity2012] authors define a more high-level set of transformations (also referred to as events) built on top of the atomic ones. These transformations more interesting for analytical purposes and include:

* Birth, when a new community emerges at a given time. 
* Death, when a community disappears. All nodes belonging to this community lose their membership.
* Growth, when a community acquires some new members (nodes).
* Contraction, when a community loses some of its members.
* Merging, when several communities merge to form a new community.
* Splitting, when a community is divided into several new ones.
* Resurgence, when a community disappears for a period and reappears.

These events/transformations are often not explicitly used during the definition and/or representation of dynamic communities. Nevertheless, most of the methods covered in the following sections do define a way in their algorithm to extract such events from the resulting data.

Finally, it is important to note that dynamic networks can differ in representation. They can be represented as either a time series of static networks (also referred to as snapshots) or as a real-time stream of edges (referred to as temporal networks). Within the global context of dynamic community detection, they can be seen as equivalent as the conversion between the two representations can be done in a lossless way. The latter, temporal networks are often used to handle incremental changes to the graph and are most commonly applied within real-time community detection settings.





## Graph Representation Learning

The representation-based approaches stem from the field of computational linguistics which relies heavily on the notion of *distributional semantics* stating that words occurring in similar contexts are semantically similar. Therefore the word representations are learned as dense low-dimensional representation vectors (embeddings) of a word in a latent similarity space by predicting words based on their context or vice versa [@mikolovEfficientEstimationWord2013; @penningtonGloveGlobalVectors2014]. Using the learned representations similarity, clustering and other analytical metrics can be computed.

The success of these representation learning approaches has spread much farther than just linguistics as similar ideas are also applied to other fields including graph representation learning. Methods such as deepwalk [@perozziDeepWalkOnlineLearning2014], LINE [@tangLINELargescaleInformation2015], and node2vec [@groverNode2vecScalableFeature2016] use random walks to sample the neighborhood/context in a graph (analogous to sentences in linguistic methods) and output vector representations (embeddings) that maximize the likelihood of preserving the topological structure of the nodes within the graph.

Whereas previously the structural information features of graph entities had to be hand-engineered, these new approaches are data-driven, save a lot of time labeling the data, and yield superior feature/representation vectors. The methods can be trained to optimize for *homophily* on label prediction or in an unsupervised manner on link prediction tasks.

Newer approaches introduce the possibility for the fusion of different data types. GraphSAGE [@hamiltonInductiveRepresentationLearning2018] and Author2Vec [@wuAuthor2VecFrameworkGenerating2020] introduce a methodology to use node and edge features during the representation learning process. Other approaches explore ways to leverage heterogeneous information present within the network by using *metapath* based random walks (path defined by a series of node/link types) [@dongMetapath2vecScalableRepresentation2017] or by representing and learning relations as translations within the embedding space [@bordesTranslatingEmbeddingsModeling2013]. In @nguyenContinuousTimeDynamicNetwork2018 the authors introduce a way to encode temporal information by adding chronological order constraints to various random walk algorithms. Other relevant advancements within the field include Graph Convolutional Networks (GCN) [@kipfSemiSupervisedClassificationGraph2017a] and (Variational) Graph Auto-Encoders (GAE) [@kipfVariationalGraphAutoEncoders2016] which present more effective ways to summarize and represent larger topological neighborhoods or whole networks.



% * Goals:
%   * Introduce common graph represntation learning techniques
%   * By covering influential 

In the remainder of this section we briefly describe working of a few selected influential representation learning algorithms. 



### Negative Sampling

Negative sampling is a technique used for reducing calculation complexity of loss for link prediction tasks. Originally introduced in @mikolovDistributedRepresentationsWords2013 this technique was used to optimize the *skipgram* algorithm which predicts context words based on a single input word (See +@eq:skipgram). Computing result of this softmax function very expensive as the vocabulary may become very large and the negative samples outnumber the positive ones by a lot. Negative sampling (+@eq:negativesampling) is introduced as an approximation where $w_O$ is the output context word, given the $w_I$ the input word, $v_{w_O}, v_{w_I}$ as their representation vectors respectively, sigmoid function $\sigma$, and a sample of $n$ words sampled from a random distribution weighted by word frequency $P_n(w)$.

Due to its efficiency, negative sampling is also embraced in graph representation learning as the link prediction task is synonymous to the context word prediction.

$$
p\left(w_{O} \mid w_{I}\right)=\frac{\exp \left(v_{w_{O}}^{\prime}{ }^{\top} v_{w_{I}}\right)}{\sum_{w=1}^{W} \exp \left(v_{w}^{\prime} v_{w_{I}}\right)} 
$$ {#eq:skipgram}

$$
\log P(w_O|w_I) \approx \log \sigma\left(v_{w_{O}}^{\prime}{ }^{\top} v_{w_{I}}\right)+\sum_{i=1}^{k} \mathbb{E}_{w_{i} \sim P_{n}(w)}\left[\log \sigma\left(-v_{w_{i}}^{\prime}{ }^{\top} v_{w_{I}}\right)\right]
$$ {#eq:negativesampling}

### Node2Vec

% * @groverNode2vecScalableFeature2016
%   Learns representation vectors in graph via
%   * 2nd order random walk (node similarity depends of connected *through* node )  (dfs)
%   * 1st order random walk - learning from direct neighbors  (bfs)
% * Features bias parameter $\alpha$ that makes the bfs and dfs tradeoff
% * Random walks start from a random node and span a length of l
%   * Starting at node $v$, transition probability is calculated for each of the neighbors using
%     * 1st order (context 0): $p(u \mid v)=\frac{w(u, v)}{\sum_{u^{\prime} \in \mathcal{N} v} w\left(u^{\prime}, v\right)}=\frac{w(u, v)}{d(v)}$ 
%       * Thus prob is $1/p$ of picking a node
%     * 2nd order (context 1 node) $p(u)$ where
%       * $1/q$  probability of leaving the neighborhood
%         * where $q$ is in-out degree ratio parameter
%       * $\alpha_{p q}(t, x)= \begin{cases}\frac{1}{p} & \text { if } d_{t x}=0 \\ 1 & \text { if } d_{t x}=1 \\ \frac{1}{q} & \text { if } d_{t x}=2\end{cases}$
%       * $d_{tx}$ denotes shortest path distance between two nodes
%         * $p$ and $q$ control how fast the walk leaves a neighborhood
%   * And as such the path is chosen
% * Similar to language modelling, negative sampling is used for loss
% * Random walks are efficient in terms of space complexity
% * Also in time complexity since you are effectively sampling
% * @wuAuthor2VecFrameworkGenerating2020
%   * introduces an extension that maps different types of node / edge embeddings to a common space
%   * (for heterogenous networks)

Introduced in @groverNode2vecScalableFeature2016, node2vec learns node representations within a graph by introducing a hybrid *second order* random walk technique. Once random walks are constructed, the *skipgram* approach along with negative sampling is used to learn the node representations. Since, random walks can be efficiently sampled, this helps avoiding memory and runtime complexity issues faced by formerly leading approaches such as spectral/matrix factorization methods.

The random walks start from a random node $u$ and span a length of $k$ nodes. The next node in the walk is chosen by calculating the transition probability for each of the neighbors of the current node. See +@eq:randomwalk1o for calculation of transition probability for first order random walks where next node $u$ is chosen given only the current node $v$ and the corresponding edge weights $w(u, v)$. 

$$
p(u \mid v)=\frac{w(u, v)}{\sum_{u^{\prime} \in \mathcal{N} v} w\left(u^{\prime}, v\right)}
$$ {#eq:randomwalk1o}

Second order random walk (+@eq:randomwalk2o) instead considers last two visited nodes $v, t$ and introduces an additional tradeoff weight $\alpha_{pq}(t,x)$ parametrized by $p$ (*in-out bias*) and $q$ (*return ratio*). See +@eq:node2vectradeoff for the trade-off weight where $d_{tx}$ represents the hop distance between two nodes $t$ and $x$. Given the distance of the two nodes the first order random walk is therefore additionally biased towards two cases: (i) returning to the same node $d_{tx} = 0$, therefore reinforcing exploration of a single neighborhood (BFS), (ii) exiting the neighborhood of a single node (DFS) (See @fig:randomwalk). Their experiments have shown that setting a higher $q$ parameter optimize embeddings for *homophily* and yield better clustering results, while setting a higher $p$ value promotes structural equivalence which aids in node classification tasks.

$$
\alpha_{p q}(t, x)= \begin{cases}\frac{1}{p} & \text { if } d_{t x}=0 \\ 1 & \text { if } d_{t x}=1 \\ \frac{1}{q} & \text { if } d_{t x}=2\end{cases}
$$ {#eq:node2vectradeoff}

$$
p(u \mid v, t)=\frac{\alpha_{p q}(t, u) w(u, v)}{\sum_{u^{\prime} \in \mathcal{N} v} \alpha_{p q}\left(t, u^{\prime}\right) w\left(u^{\prime}, v\right)}
$$ {#eq:randomwalk2o}

![BFS and DFS search strategies from node $u$ ($k = 3$).](/home/egordm/.config/marktext/images/7488500d2d1a170449bad6632dee81276b2ab8bd.png){#fig:randomwalk width=250px}



### Graph Autoencoder (GAE)

% * Applies the idea of Variational Autoencoders to the graph structure
% * Traditionally autoconcoders consist of
%   * An encoder encoding the input $X$ to a low dimensional latent space
%   * A decoder decoding the low dimensional vector back to the input space $\hat{X}$
% * Reconstruction error is calculated $L(X, \hat{X}) = ||X - \hat{X}||^2$
% * Provides robustness as input can be easily augmented and changes can be measured
%   * Variational autoencoders produce a distribution instead (to be able to generate new samples)
% * Their architecture is similar to spectral GCN which work on
%   * Whole graph laplacian matrix - which provides a mathematical representation of the graph (often in a normalized way)
%   * Decoder reconsturucts this laplacian matrix by calculating inner product of the latent matrices (stack of low-dim repr vectors)

In @kipfVariationalGraphAutoEncoders2016 authors describe the application of autoencoder idea for graph representation learning. Traditionally autoencoders consist of an encoder $E: X \rightarrow Z$ translating the input $X$ to a low dimensional latent space vector $Z$, and a decoder $D: Z \rightarrow \hat{X}$ translating the low dimensional vector $Z$ back to the input space as $\hat{X}$. To optimize both components *reconstruction error* $L(X, \hat{X}) = ||X - \hat{X}||^2$ and its variants are employed. This ensures cohesion of the latent space.

The architecture for graph autoencoders is similar to the one used in spectral graph convolution networks. The whole graph is represented as a graph Laplacian matrix $S \in \mathbb{R}^{N\times N}$ (mathematical representation of a graph; can be adjacency, similarity matrix or similar). During encoding an efficient factorization approximation of this matrix is produced as $Z \in \mathbb{R}^{N \times d}$, while during decoding the inner product is calculated $ZZ^T$ reconstructing the graph Laplacian.

Employing this approach for graph representation learning yields many benefits such as flexibility for objective definition and robustness against noise. The main downside of this approach is that working on full graph is expensive and doesn't scale well.



![Graph Autoencoder architecture](/home/egordm/.config/marktext/images/d1d5cd4d01bc507a198256ec0a4e2b9982e6a10b.png){width=380px}



### GraphSAGE

% * @hamiltonInductiveRepresentationLearning2018
%   Was made to tackle issues in existing approaches are *transductive*  
%   * ie. needs whole graph to learn embedding of a node
%   * generalize poorly since addition of a node requires rerun of the algorithm
% * Representation function learning method - referred to as aggergator functions
%   * These functions can be used to predict the embedding rather than statically learning it
% * Needs a little bit different sampling approach than random walks
%   * Due to the fact aggregator functions need to be stacked (which need regular input counts)
%   * Therefore a neighborhood of a node is sampled to the depth $k$ (with $k$ being the amount of aggregator functions)
% * Works by initializing all node embeddings to node features
%   * For each node its embedding is concatenated with aggregated representation of its neighbors
%   * And passed to a deep neural network yielding a representation vector at given layer
% * Loss is defined similarly using negative sampling
% * TODO: Talk about PinSAGE extension
%   * Introduces a methodology for more efficient sampling and importance weighing
%   * Introduces a way to learn representations of heterogenous networks
%     * By using type specific aggregation functions mostly for metapaths

Existing graph representation learning approaches are *transductive*, meaning the algorithm needs the whole graph to learn embedding of a node. This approach generalized poorly, since addition of a single node requires rerun of the algorithm. In @hamiltonInductiveRepresentationLearning2018 the authors introduce GraphSAGE representation function learning method which solves this issue by learning a set of aggregator functions to predict the embedding rather than statically learning it (also referred to as *inductive learning*).

First, layer count $k$ parameter is specified defining amount of aggregations used, and therefore the aggregation neighborhood size (namely $k$-hop neighborhoods). A forward pass for a "mean" operation based aggregator is defined as +@#eq:graphsageagg. Where $h^{k-1}_v$ is the representation vector of a node $v$ at $k-1$ th layer, $\mathbf{W}$ is the the set of weights, and $\sigma$ is the activation function. By initializing $h^0_v$ using the node's feature vector and applying aggregator functions recursively given a nodes neighborhood $\mathcal{N}(v)$ the final representation vector $h^k_v$ is calculated (See +@fig:graphsage).

In follow-up work @yingGraphConvolutionalNeural2018 the authors introduce a random walk simulating sampling based aggregation approach which in combination with negative sampling is capable of learning on web scale graphs (> billion nodes).  

$$
\mathbf{h}_{v}^{k} \leftarrow \sigma\left(\mathbf{W} \cdot \operatorname{MEAN}\left(\left\{\mathbf{h}_{v}^{k-1}\right\} \cup\left\{\mathbf{h}_{u}^{k-1}, \forall u \in \mathcal{N}(v)\right\}\right)\right.
$$ {#eq:graphsageagg}

![Overview of GraphSAGE model architecture using depth-2 convolutions ($k=2)](/home/egordm/.config/marktext/images/f844cf41b511a806c4af337a2c48db8da70b89ac.png){#fig:graphsage width=380px}







## Link-based Approaches

% Goals:
% 
% * Describe the intuition of link based approaches
%   * Inter-connection density vs intra connection density
% * Cover the current state of the are
% * Cover different approaches and problems that may arise within the link based approaches
% * Start with community detection
% * Expand by covering different **strategies to tackle tracking instability**

Link-based approaches to (Dynamic) Community Detection rely on connection strength to find communities within the network. The main criteria for communities are the assumed property that intra-group connections are denser than inter-group ones. The networks are partitioned in such a way, that optimizes for a defined measure characterizing this property.

We start this section by covering the fundamentals of link-based community detection by introducing commonly used community quality measures and algorithms for optimizing them. Next, we introduce the link-based DCD problem and the unique challenges that arise as opposed to CD. Then we proceed to cover the current state of the art by describing the related works, their solutions to the said challenges, and possible extensions to the problem.



### Community Detection

% * Talk about basic and common CD techniques
% * Introduce notion of modularity

Different metrics exist quantifying the characteristic of *homophily* over edge strength. The most common metric is Modularity which measures the strength of the division of a network into modules (communities). Its popularity stems from the fact that it is bounded and cheap to compute, though it has other problems such as resolution limit (making detecting smaller communities difficult). Other metrics that can be found in the literature include but are not limited to:

* Conductance: the percentage of edges that cross the cluster border
* Expansion: the number of edges that cross the community border
* Internal Density: the ratio of edges within the cluster with respect to all possible edges
* Cut Ratio and Normalized Cut: the fraction of all possible edges leaving the cluster
* Maximum/Average ODF (out-degree fraction): the maximum/average fraction of nodes’ edges crossing the cluster border

#### Modularity

% * Measures the density of links inside communities compared to links between communities
% * Value: Calculated over a set of nodes
%   * Ranges between [-1/2, 1]
%   * expected number of edges in computed using a configuration model concept
%     * Edges are split into two stubs and each is randomly rewired with any other stub
%     * Based on node degrees pairwise expected number of edges can be computed
%   * It is positive if number of edges within a group exceeds expected number on basis of chance
%   * [Explanations of terms in equations](https://latex.org/forum/viewtopic.php?t=11318)

Modularity directly measures the density of links inside a graph and is therefore computed on communities (sets of nodes) individually by weighing edges based on community similarity (or exact matching). Calculation of modularity is done by aggregating for each community $r$ and for each pair of nodes $vw$ the difference between the expected connectivity $\frac{k_{v} k_{w}}{2 m}$ (amount of edges between the nodes) and the actual connectivity $A_{vw}$ (existence of an edge) given their degrees ($k_v$ and $k_w$). The final result represents the delta connectivity difference by how much the given graph exceeds a random graph as expected connectivity is determined by a random rewiring graph. Because intra-community pairs are weighted lower than inter-community pairs the score can vary. See +@eq:modularity where $S_{vr}$ indicates membership of node $v$ for community $r$ and $m$ represents the total edge count.

$$
Q=\frac{1}{2 m}\sum_{v w}\sum_{r}\left[\overbrace{A_{v w}}^{\text{Connectivity}}-\underbrace{\frac{k_{v} k_{w}}{2 m}}_{\text{Expected Connectivity}}\right] \overbrace{S_{v r} S_{w r}}^{\text{Community Similarity}}
$$ {#eq:modularity}

#### Louvain Method

% * Hierarchical algorithm
%   * Starts with each node assigned to it’s own community
%     * First small communities are found
%       * For each node i change in modularity is calculated for removing i from its own community
%       * And adding it to a neighbor community
%       * Modularity change can be calculated incrementally (local)
%   * Then produces condensed graph by merging communities (their nodes) into a single node
%     * Repeats this process
% * Optimizes for modularity
%   * Is a heuristic algorithm
%     * Since going through all possible assignments maximizing modularity is impractical

Finding an optimal partition of a graph into communities is an NP-hard problem. This is because, while calculating the modularity score can be done in loglinear time, all possible node to community assignments still have to be considered. Therefore heuristic-based methods such as the Louvain method are usually used.

% Feedback: Cite NP-hard CD

Louvain method [@blondelFastUnfoldingCommunities2008] is a heuristic-based hierarchical clustering algorithm. It starts by assigning each node in the graph to its own community. Then it merges these communities by checking for each node the change in modularity score produced by assigning it to a neighbor community (based on the existence of a connection). Once the optimal merges are performed, the resulting communities are grouped into single nodes and the process is repeated.

Since modularity changes can be computed incrementally, the complexity of this method is $O\left(n \log n\right)$. Additionally, due to the flexibility of the modularity measure, it allows detecting communities in graphs with weighted edges.



#### Label Propagation algorithm

% * Algorithm to find communities in graph (very fast)
% * Uses only network structure as guide
% * Doesn’t require any priors (metrics)
% * Intuition:
%   * Single label quickly becomes dominant in a group of densely connected nodes
%   * But these labels have trouble crossing sparsely connected regions
%   * Nodes that end up with same label can be considered part of same community
% * Algorithm:
%   * Initialize each node to their own label
%   * Propagate the labels, per iteration:
%     * Each node updates its label to one that majority of its neighbors belong
%     * Ties are broken deterministically
%   * Stops when convergence is reached, or max iter
% * Preliminary solution can be assigned before run

Another way to sidestep the complexity issue is using the Label Propagation algorithm as it uses the network structure directly to define partitions and doesn't require any priors (quality metrics). The intuition for the Label Propagation algorithm is as follows: When propagating a label through connections in the network, a single label quickly becomes dominant within a group of densely connected nodes, but these labels usually have trouble crossing sparsely connected regions.

The algorithm starts by assigning each node their own label. After that, for each iteration, each node updates its label to the majority label among its neighbors where ties are broken deterministically. The algorithm stops after a fixed amount of iterations or once it has converged. An important feature of this algorithm is that a preliminary solution can be assigned before each run, therefore only updating existing membership assignments.



### Dynamic Community Detection

% * Expand by covering different **strategies to tackle tracking instability**
% * Extension to Community Detection
%   * Adds community tracking to the equation
%   * Instability of community detection methods becomes a problem (tracking relies on their consistency)
% * Community Drift: 
%   * Caused by relying on the previously found communities
%   * Can cause avalanche of wrong detection

Dynamic Community Detection can be seen as an extension to community detection by the addition of the Community Tracking task. Tracking relies on the coherency and stability of found communities to define their evolution through time. The said properties can not be taken for granted and introduce new challenges when designing DCD methods. The main issue is in fact that they are competitive with each other causing a trade-off between community coherency/quality and community temporal stability.

Various strategies dealing with this trade-off are categorized by @rossettiCommunityDiscoveryDynamic2018 and @dakicheTrackingCommunityEvolution2019 where authors reach a consensus over three main groups. In the following sections, we briefly introduce these strategies and describe the current state of the art in similar order.

#### Independent Community Detection and Matching

% * Works in two stages:
%   * CD methods are applied directly to each snapshot (identify stage)
%   * Then the communities are matched between the snapshots (match stage)
% * Advantages:
%   * Use of unmodified CD algorithms (built on top of exisiting work)
%   * Highly parallelizable
% * Disadvantage:
%   * Instability of community detection algorithms (may give **very** different results if network changes)
%   * Difficult to distinguish between instability of algorithm and evolution of the network

Also referred to as the two-stage approach. Works by splitting the DCD task into two stages. The first stage applies CD directly to every snapshot of the network. Followed by the second stage matching the detected communities between the subsequent snapshots.

The advantages of this approach include the fact that it allows for use of mostly unmodified CD algorithms for the first step and that it is highly parallelizable as both detection and matching steps can be applied to each snapshot independently. The main disadvantage is the instability of underlying CD algorithms which may disrupt the community matching process. Many CD methods may give drastically different results in response to slight changes in the network. During the matching, it becomes difficult to distinguish between this algorithm instability and the evolution of the network.



% @wangCommunityEvolutionSocial2008 (core nodes / leader nodes)
% 
% * Circumvents instability issue by studying most stable part of communities (community core nodes)
% * Observations:
%   * The social network scale inflates when it evolves
%   * Members change dramatically and only a small portion exists stably
%     * Therefore only a few can be relied on
% * Introduce algorithm CommTracker
%   * Relies heavily on core nodes
%   * Example: co-authorship community where core nodes represent famous professors
%   * Core Node Detection Algorithm
%     * Each node evaluates centrality of the nodes linked to it
%     * If a node’s weight is higher than it’s neighbors - then its centrality is increased and neighbors decreased
%       * The change value is set as difference is weight
%     * Nodes with non-negative centrality are core nodes
%   * Core-based Algorithm to track communities
%     * Define a set of rules based on presence of core nodes

In @wangCommunityEvolutionSocial2008 the authors circumvent this instability issue by looking at the most stable part of the communities, namely core/leader nodes. In their research, they observe that in various datasets most of the nodes change dramatically while only a small portion of the network persists stably. To exploit this feature, the algorithm CommTracker is introduced which first detects the said core nodes, and then defines rules to both extract communities as well as their evolutional events. The community members are assigned based on their connectivity relative to core nodes.



% @rossettiANGELEfficientEffective2020
% 
% * Detects overlapping communities 
% * Mainly improves computational complexity
% * Use label propagation to extract overlapping fine grained communities
%   * Very fast and has good quality
%   * Runs label propagation for each node on the graph without said node 
%   * Multiple communities are determined by looking at resulting communities of nodes neighbors
%   * These communities are aggregated in a global list
% * The resulting communities are merged into larger ones
%   * By checking if their overlap exceeds a threshold

@rossettiANGELEfficientEffective2020 proposes a way to detect overlapping communities in dynamic networks. A more robust two-phase community detection method (ANGEL) is proposed to ensure the stability of the found communities. The first phase extracts and aggregates local communities by applying Label Propagation on the Ego-Graph (graph excluding a single node) for each node in network. The found communities are biased due to their partial view of the network and are merged in the second step based on their overlap yielding more stable communities with the possibility of overlap. During the matching for each snapshot, a step forward and backward in time is considered where community splits and merges are detected by reusing the matching criteria of the second phase of the CD step.

#### Dependent Community Detection

% Dependent Community Detection / Temporal Trade-Off Communities Discovery
% 
% * Use snapshots to detect communtities
% * To detect communities in current snapshot, rely on communities from the previous
% * Advantage:
%   * Introduces temporal smoothness (fixes the instability problem mostly)
%   * Does not diverge from the usual CD definition (search at each timestep)
% * Disadvantage:
%   * Not parallelizable
%   * Impacts long term coherence of dynamic communtities
%   * Each steps experience substantial drift compared to what a static algorithm would find

Dependent Community Detection strategy works by detecting communities in each snapshot based on the communities found in the previous snapshots. This approach introduces temporal smoothness because the previous solution is reused making the matching process obsolete. Though as part of the described trade-off it can impact the long-term coherence of the dynamic communities. Mainly, because each step introduces a certain amount of error into the results (community drift) which may get amplified within further iterations (error accumulation). Another disadvantage is the fact that the strategy has limited possibilities of parallelization due to its dependent nature.

To lessen the complexity of continuous re-detection of the communities some algorithms process the changes incrementally by limiting the change to a local neighborhood. While this approach has many benefits, it is important to note that these algorithms face a problem where only applying local changes can cause communities to drift toward invalid ones in a global context.

% @heFastAlgorithmCommunity2015 (modified louvain)
% 
% * Modify louvain algorithm to use previous communities
% * They start by detecting communities within the initial graph
% * In subsequent snapshots:
%   * Say that nodes whose edges do not change, stay in the same community
%   * Nodes with changing edges, have to be recomputed
%   * Unchanged nodes maintain their community and are grouped into a single node
%   * Edge weights to changed nodes change proportionally to their connectivity to the community
%   * Louvain method algorithm is run on the newly constructed graph
% * Therefore introducing temporal smoothness and high efficiency

@heFastAlgorithmCommunity2015 introduces an efficient algorithm by modifying the Louvain method. Based on the observation that between consecutive timesteps only a fraction of connections changes and do not affect communities dramatically, they argue that if all community nodes remain unchanged, the community also remains unchanged. With this in mind, they make a distinction between two types of nodes, ones that change the connection in a snapshot transition and ones that do not. The former have to be recomputed, while the latter maintain their community label. The nodes that maintain their community are merged into community nodes with edges to other community nodes and changed nodes weighted proportionally to their real connectivity (amount of edges when ungrouped). This simplified graph is passed to the Louvain method algorithm for community detection. By reusing the community assignments temporal smoothness is maintained and due to the incremental nature of this algorithm, the overall complexity remains low.



% @guoDynamicCommunityDetection2016 (based on distance dynamics)
% 
% * Based on distance dynamics
% * Define distance dynamics in form of a so called Attractor algorithm
%   * Edge weights are initialized using Jaccard distance
%   * Interaction patterns are defined that describe how nodes behave based on their connection to other nodes
%   * Algorithm is run until the weights converge
%   * By removing edges with low distance, communities can be found as connected components
% * Increments can be treated as network disturbance
%   * They can be limited to a certain area by a disturbance factor

@guoDynamicCommunityDetection2016 envision the target network as an adaptive dynamical system, where each node interacts with its neighbors. The interaction will change the distances among nodes, while the distances will affect the interactions. The intuition is that nodes sharing the same community move together, and the nodes in different communities keep far away from each other. This is modeled by defining the so-called Attractor algorithm, which consists of three interaction patterns that describe how node connection strength is influenced by neighboring nodes. The edge weights are initialized using Jaccard distance and the propagation is run until convergence. The communities can be extracted by thresholding on edge weight/distance. Thereafter, all changes are treated as network disturbances. The disturbance can be limited to a certain area using a disturbance factor which defines a bound on the possible propagation.



% @yinMultiobjectiveEvolutionaryClustering2021
% 
% * Look at the problem from an Evolutionary Clustering Perspective
%   * Propose a generic algorithm as in Evolutionary Algorithms
%   * Goal: detect community structure at the current time under guidance of one obtained immediately in the past
%     * Fit observed data well
%     * And keep historical consistency
%   * Combine traditional evolutionary clustering with particle swarm algorithm
% * Solve major drawbacks:
%   * Absence of error correction - which may lead to result-drifting and error accumulation
%   * If initial community structure is not accurate, or the following - this may lead to the “result drift” and “error accumulation”
% * Algorithm:
%   * Use random walks to build a diverse initial population
%   * Search Phase:
%   * Uses custom operators MICO (Communtity Quality) and NBM+ (Community Consistency) to improve global convergence
%     * Cross-over operators - combining multiple individuals (parents)



More recently the @yinMultiobjectiveEvolutionaryClustering2021 has proposed an evolutionary algorithm by looking at the DCD from an Evolutionary Clustering Perspective. They detect community structure at the current time under the guidance of one obtained immediately in the past by simultaneously optimizing for community quality score (modularity) and community similarity between subsequent time steps (NMI). In the methodology, a way is proposed to encode a graph efficiently into a genetic sequence. Additionally, new mutation and crossover operators are proposed which maximize either of the two objectives. By using a local search algorithm, building a diverse initial population, and selecting for dominant candidates the communities maximizing both objectives are obtained.

% Feedback: Define NMI here (instead of in evaluation?) - Or move it into community detection?



#### Simultaneous community detection

% Simultaneous community detection / Cross-Time Community Discovery
% 
% * Doesnt consider independently the different steps of the network evolution
% * Advantage:
%   * Doenst suffer from instability or community drift
%   * More potential to deal with slow evolution and local anomalies
% * Disadvantage:
%   * Not based on usual principle of a unique partition with each timestep
%   * Cant handle real time community detection

The final strategy we consider sidesteps the matching issue by considering all snapshots of the dynamic network at once. This is done by flattening the network in the temporal dimension and coupling edges between the same nodes at different timesteps. These approaches usually don't suffer from instability or community drift. The disadvantages include that the standard principle of an unique partition for each time step can't be applied, as only the combined network is used, therefore limiting the number of possible algorithms. Handling real-time changes to the graph are also usually not considered.

% @muchaCommunityStructureTimeDependent2009
% 
% * Connected identical nodes between different timesteps
% * Then used used louvain method to detect communities

@muchaCommunityStructureTimeDependent2009 adopt a simple yet powerful solution to this problem by connecting identical nodes between different time steps within the unified network. On this network, they apply a modified Louvain method algorithm to extract the communities whose members can be split over different timesteps.



% @ghasemianDetectabilityThresholdsOptimal2016
% 
% * Stochastic block model based approach
% * Define a way to derive a limit to the detectability as function of strength of a community
%   * Some communities are not detectable - probabliity equal to chance
%   * Model is based on prediction of edges that are generated between the timesteps
% * Define a community detection algorithm using Belief propagation equations
%   * To lean marginal probablilities of node labels over time
%   * The use two edge types spatial edges and temporal edges
%   * Similar to spectral clustering

@ghasemianDetectabilityThresholdsOptimal2016 apply stochastic block model-based approach. They make a distinction between two edge types: (i) spatial edges (edges between neighbors) and (ii) temporal edges (edges between nodes in subsequent timesteps). Using this distinction, they define a Belief Propagation equation to learn marginal probabilities of node labels over time. Additionally in their research, they introduce a way to derive a limit to the detectability of communities. This is, because some communities may not be detectable as their probability nears that of random chance.



% TODO: I promised some extensions in the intro. Should I add them?
% 
% * @liuCommunityDetectionMultiPartite2016  - Heterogenous Networks
% * @wanyeTopologyGuidedSamplingFast2021 - Sampling

## Representation-based Approaches

% * Work similarly as link-based methods
%   * Based on the idea of homophily (connected nodes are similar)
% * Instead of directly modelling based on connection
%   * They learn an intermediate representation
%   * Benefits is that it solves non-linearity problem real-world networks have
%   * Representations are more robust against noise
% * The representations are beneficial because
%   * They can encode auxiliary objectives
%   * Multi-modal features such as content
%   * Network (meta) structure and Temporal dimension
%   * Are naturally easy to compute similarity on

The main difference between Representation-based approaches and link-based approaches is the fact that they usually don't directly model the network based on connections. Instead, they learn an intermediate representation of the graph or its components to which CD detection can be applied to. While also relying on the idea of *homophily*, most methods define additional objectives to improve community quality.

The main reasoning for this is the fact that real-world networks are non-linear, meaning that there may be no connections when they make sense and vice versa [@wangEvolutionaryAutoencoderDynamic2020]. By using deep neural networks to learn these embeddings one can address such non-linearity as they are in general very robust against noise. Other notable benefits to using representation-based approaches include the fact that they compress the data efficiently as real-world networks are very sparse. They can also represent multi-modal features, network (meta) structure, and temporal dimension by defining them all in a compatible similarity space or learning mappings to this space. Finally, representations are naturally easy to compute similarity on.

In this section, we describe representation-based approaches by covering both CD and DCD approaches. To provide a more cohesive overview of the methods, we group them by their innovations instead.



### Affiliation Graph Models

% @yangCommunityAffiliationGraphModel2012
% 
% AGM: Affiliation Graph Model: a generative model $B(V, C, M, \{p_c\})$ for graphs
% 
% * $V$ vertices, $C$ communities, $M$ community memberships,
% * ${p_c}$ model parameters (probability $p_c$ per community $c$ indicating a connection strength.)
% * Can model variety of community structures: (all have distinct characteristics in M)
%   * Non-overlapping, Overlapping and hierarchical (TODO: add figure?)
% * Usage: Generative Model: (generating edges)
%   * generates links for each pair of nodes in community $A$ with probability $p_a$ 
%   * The more communities they have in common, means higher probability for a connection
%   * If no overlap, then $P(u, v) = \epsilon$
% * Usage: Discriminative Model: Given a graph, find a model $F$ that may have generated it
%   * Assume a model was generated by AGM
%   * Parameters $M$, number of communities $C$, $p_c$
%   * Graph Fitting: using max likelihood estimation
%     * Given a model, it can be compared to data using log likelihood
%     * AGM is relaxed to have edge strength
%     * Using the relaxed model memberships now have strengths $F_uC$
%     * Probability of two nodes connecting is defined in terms of them connecting through a community
%     * Gradient descent is used to find the optimal parameters

While arguably not being representational by itself, Affiliation Graph Network (AGM) model introduced in @yangCommunityAffiliationGraphModel2012 is very influential within the deep learning/representation learning CD field.

The AGM models a network as a bipartite graph with communities as first-class citizens and is represented by the following equation $B(V, C, M, \{p_c\})$, where $V$ represents nodes, $C$ set of communities, $M$ node-community memberships and $\{p_c\}$ model parameters (a single probability $p_c$ per community). It can model non-overlapping, overlapping, and hierarchical communities by defining rules on membership sets in $M$. AGM can be used in both generative and discriminative settings.

The generative scenario works as follows: Given an AGM $F(V, C, M, {p_c})$, generate links between each pair of nodes exceeding a baseline probability $p$. This is done by considering that, according to AGM, each pair of nodes in community $C_i$ is connected with a probability $p_{c_i}$. Therefore, the probability of two nodes having a connection is proportional to the number of communities they share (which can be derived from the model).

The discriminative scenario is defined as: Given a graph $G$, find a model $F(V, C, M, {p_c})$ that may have generated it. By assuming that the graph was generated using an AGM, the parameters $M$, number of communities $|C|$ and $\{p_c\}$ have to be found. Process for finding such a model to the graph involves max likelihood fitting. AGM is relaxed to have membership strengths $F_{uC}$, which helps to define the probability of nodes $u$ and $v$ connecting through community $C$ as $P_{C}(u, v)$, and by themselves $P(u, v)$ (by marginalizing over communities). Using this, a probability $P(G|F)$ can be constructed quantifying how well the model fits the data. Finally, gradient ascent can be applied to optimize for the model's parameters.



### Graph Reinforcement

% Class of methods for Community Detection: Graph Augmentation
% 
% * These type of method use representation learning techniques to 
%   * Enhance the graph (augmentation) by adding valuable edges
%   * Remove noise by adding missing connections or removing noisy ones
% * TODO: non-linearity, sparsity of real-world graphs

The first class of methods we consider are Graph Reinforcement methods. These methods use representation-based learning techniques to enhance the graph by adding valuable edges or reducing the noise by removing noisy connections. This is usually done by training a model on a link-prediction task. A notable benefit of this approach is that other well-known CD methods can be used on the enhanced graph afterward.



% @kangCommunityReinforcementEffective2021
% 
% * Present a **Community Reinforcement** approach
% * Is CD algorithm agnostic
%   * Shown in experiments - therefore the graph itself benefits
% * Reinforces the Graph by
%   * Creating inter-community edges
%   * Deleting intra-community edges
%   * Determines the appropriate amount of reinforcement a graph needs
% * Which helps the dense inter-community and sparse intra-community property of the graph
%   * Can effectively turn difficult-to-detect community situation into that of easy-to-detect communities
% * Challenges:
%   * Needs to be unsupervised (doesn't need community annotations to work)
%   * Appropriate amount of reinforcement needs to be determined (otherwise noise is introduced)
%   * Needs to be fast, checking every possible edge is infeasible
% * Methodology:
%   * Edge Addition / Deletion
%     * Based on node similarity of connected pairs
%       * Similar nodes are likely to be in a community (intra edges)
%       * Dissimilar ones are less likely to be in a community (inter edges)
%     * Employ graph embedding to generate topological embeddings
%       * Adamic/Adar (local link-based)
%       * SimRank (gloabl link-based)
%       * Node2vec graph embedding based
%     * Predict similarities and bucket them
%       * Use similarity buckets to select intra and inter edges - and to tune the model
%       * Buckets are selected on how well they predict current edges
%   * Detect the right amount of addition
%     * Use a gradual reinforcement strategy by generating a series of graphs (adding top x inter and removing intra edges)
%     * Pick the best graph using a scoring function (modularity)
%     * Simply run CD over the graph and see
%   * Reducing computational overhead
%     * Using a greedy similarity computation
%     * Prefer nodes which are likely to be in same community of inter similarity detection

@kangCommunityReinforcementEffective2021 present a pre-processing method for strengthening the community structure of a graph by adding non-existing predicted intra-community edges and deleting existing predicted inter-community edges. Their strategy is to learn topological embedding using a graph representation learning algorithm (node2vec) based on the existing link prediction task. The similarity is computed between different node pairs and put into buckets. Then with the assumption of *homophily* the buckets with a higher value can be considered holding intra-community while buckets with lower inter-community connections. Right buckets are picked from both extremes to create or delete edges. The preemptive CD is done to greedily guide pair-wise similarity computation and avoid a high complexity. Once the reinforced graph is constructed, already existing CD algorithms can be applied.



% @jiaCommunityGANCommunityDetection2019
% 
% * Has some info in related work to extend on graph representation learning
% * Solves issue of detecting overlapping communities:
%   * K-means and Gaussian Mixture Model cant do that
% * Proposes CommunityGAN:
%   * Solves graph representation learning and community detection jointly
%   * Embeddings indicate the membership strength of vertices to communities
%   * Make use of Affiliation Graph Model AGM for community (detection) assignment
%   * Uses GAN structure to:
%     * Generate most likely vertex subset s to compose a specified kind of motif
%       * "Graph AGM" motif generation model
%     * Discriminate whether vertex subset s is a real motif or not
%     * Motif = in this case is an n-clique
%   * Study motif distribution among ground truth communities to analyse how they impact quality of detected communities
% * Methodology:
%   * Define a method to efficiently random walk such cliques / motifs
%   * Generator tries to learn $p_{true}(m|v_c)$ as preference distribution of motifs containing $v_c$ vs all the motifs
%     * To be able to generate most likely motifs (vertex subsets) similar to real motifs covering $v_c$
%   * Discriminator tries to learn probability of a vertex subset being a real motif
%     * Tries to discriminate between ground truth motifs and not
%   * AGM:
%     * Can define a measure to check whether two nodes are affiliated through a specific (or any) community
%     * Usually applied for edge generation
%     * In this case, extended to motif generation (edge is a 2-clique)
%       * The affiliation is defined now in form of a motif in community
%   * Amount of communities are chosen by hyperparameter tuning

@jiaCommunityGANCommunityDetection2019 solves the issue of detecting overlapping communities by proposing the CommunityGAN algorithm which jointly optimizes for node and community representations. First, they define a method for efficient motif (in their case clique) sampling from the graph (true/clique, and false/vertex subset). Then, they define a GAN based structure for learning representational vectors where the generator $G$ tries to learn $p_{true}(m|\mathbf{v}_c)$ as preference distribution of motifs to generate most likely vertex subsets most likely to be real motifs. Discriminator $D$ tries to learn the probability of a vertex subset being a real motif, therefore creating a minimax game of progressively optimizing embeddings to be able to encode rich information about network topology.

Both components ($G$ and $D$) are implemented as a modified relaxed AGM model with a more general definition to be able to handle the motif generation (rather than edge generation). The probability of a set of vertices being a motif is defined in terms of their probability being a motif through a community, therefore making them community-aware as they now represent the affiliation weight between a vertex and a community. The number of communities is chosen by training and testing part of data on link prediction task.



### Multi-objective optimization

% * Is another subject where representation based approaches excel
% * Usually multiple objectives are defined such as:
%   * Homophily
%   * Community Quality
%   * Temporal Consistency
% * in terms of given representations to be able to back-propagate the combined error

Another subject where representation-based approaches excel is multi-objective optimization. Usually, a combined objective is defined in terms of a community quality, temporal consistency, or homophily measure. These measures in turn use the proximity between the representation to be able to back-propagate the combined error and optimize the representation (function) directly.



% @rozemberczkiGEMSECGraphEmbedding2019
% 
% * Learns clustering (centers) simultaneously with computing embeddings
% * Objective functions includes:
%   * Term to embed around the origin
%   * Term to force nodes with similar sampled neighborhoods to be embedded close
%   * Term to force nodes to be close to the nearest cluster (weighted by clustering coefficient)
% * Weights are randomly initialized
% * Clustering coefficient is annealed (changes overtime)
% * Uses negative sampling to avoid huge cost of softmax
% * Adds regularizer "social network cost" to optimize for homophily
%   * weighs distance in latent space by neighborhood overlap
%   * Makes algorithm more robust to changes in hyperparameters
% * Evaluate cluster quality by modularity
% * Evaluate embeddings by genre prediction / recommendation

In @rozemberczkiGEMSECGraphEmbedding2019 authors propose a method that learns cluster centers along with node embeddings. They define an objective function as a combination of three terms: normalization term (ensures embeddings are centered at the origin), proximity term (forces nodes with similar neighborhoods to be embedded close), cluster quality term (forces nodes to be close to their nearest cluster). Additionally, a "social network cost" term is added as a regularizer to optimize for proximity between nodes within the same cluster. During training, the clustering coefficient is annealed to ensure convergence and negative sampling is employed to avoid large softmax costs.



% @yangGraphClusteringDynamic2017
% 
% * Goal: unsupervised clustering on networks with contents
%   * Propose a way to utilize deep embedding for graph clustering
% * Simultaneously solve node representation problem and find optimal clustering in a e2e manner
%   * Jointly learns embeddings X and soft clustering $q_i \in Q$
%   * $\mathcal{J}*{2}=K L(\mathcal{P} | Q)=\sum*{i} \sum_{k} p_{i k} \log \frac{p_{i k}}{q_{i k}}$: probablility of node $v_i$ belonging to kth cluster
%   * K is known a-priori
% * Employ Deep Denoise Autoencoder (DAE) - good for features with high-dimensional sparse noisy inputs
% * Use stable influence propagation technique (for computing embeddings)
%   * Use a transition matrix for a single step embedding propagation
%   * Because:
%     * Random walk requires more tuning
%     * Their transition matrix is very similar to a spectral method (symmetric Laplacian matrix)
%     * Influence propagation is like kipf and welling - doenst require matrix decomposition
%   * Embedding loss: $\mathcal{J}*{2}=K L(\mathcal{P} | Q)=\sum*{i} \sum_{k} p_{i k} \log \frac{p_{i k}}{q_{i k}}$
% * Introduce GRACE cluster module:
%   * Computes soft clustering Q from: $q_{i k}=\frac{\left(1+\left|\mathbf{x}*{i}-\mathbf{u}*{k}\right|^{2}\right)^{-1}}{\sum_{j}\left(1+\left|\mathbf{x}*{i}-\mathbf{u}*{j}\right|^{2}\right)^{-1}}$
%   * Learn clustering results by learning distribuition P where $p_{i k}=\frac{q_{i k}^{2} / f_{k}}{\sum_{j} q_{i j}^{2} / f_{j}}$
%     * and $f_{k}=\sum_{i} q_{i k}$ total number of nodes softly assigned to kth cluster
%   * Clustering Loss: $\mathcal{J}*{2}=K L(\mathcal{P} | Q)=\sum*{i} \sum_{k} p_{i k} \log \frac{p_{i k}}{q_{i k}}$
%   * Training is done in alternating steps:
%     * Macrostep: Compute: P and fix it
%     * S Microsteps: Update node embeddings S and cluster centers U
%       * Tries to make Q catch up with P

@yangGraphClusteringDynamic2017 propose a similar idea of combining embedding and clustering tasks and solving them in an end-to-end manner. In their work, they employ a Deep Denoise Autoencoder (DAE) to learn topological information of the network by optimizing for reconstruction loss. To learn cluster/community centers they define GRACE cluster module which first computes soft cluster assignment matrix $Q$ by utilizing the embeddings and cluster centers which contain probabilities $q_{ik}$ of node $i$ belonging to cluster $k$. The clustering loss is defined as (Kullback Leiber) KL-divergence between the soft clustering $Q$ and auxiliary target distribution $P$ which is computed by squaring and normalizing the soft assignments to reinforce more confident clustering results while preventing the formation of excessively large clusters. Both embeddings and clustering are optimized alternatively until convergence.



% @maCommunityawareDynamicNetwork2020 (use as baseline?)
% 
% * Define communities in terms of large and small scale communities
% * They propose a method for dynamic *community aware* network representation learning
%   * By creating a unified objective optimizing stability of communities, temporal stability and structure representation
%   * Uses both first-order as well as second order proximity for node representation learning
% * They define community representations as average of their members
%   * Adopt a stacked autoencoder to learn low-dimensional (generic) representations of nodes
% * They define loss in terms of:
%   * Reconstruction Error: How well the graph can be reconstructed from the representation
%   * Local structure preservation: Preservation of homophiliy - connected nodes are similar
%   * Community evolution preservation: Preservation of smoothness of communities in time at multiple granularity levels
% * The communities are initialized using classical methods:
%   * First large communities are detected using Genlouvin (fast and doesnt require priors)
%   * Then small scale communities are detected using k-means by defining a max community size w
%     * Which provides more fine tuned communities
% * Using the initial embeddings the temporal embeddings are optimized
%   * Done by optimizing all at once - therefore maintaining the stability
%   * And use of the mentioned combined objective
% * Though they present / evaluate their algorithm in terms of Dynamic Representation Algorithms
%   * Therefore the actual quality of communities remains to be known

@maCommunityawareDynamicNetwork2020 proposed a novel approach to constructing community-aware dynamic network embeddings by leveraging multi-objective optimization and extending it into a temporal dimension. They adopt a Graph Autoencoder structure which works by encoding the full graph into a lower-dimensional structure and decoding it again into a graph. Assuming a well-tuned autoencoder, this allows authors to encode the input network (and its nodes) into a more efficient representation vectors which characterize the network well.

The objective function they use is defined by three terms: the reconstruction error term minimizing the distance between the ground-truth and the autoencoder output, local structure/homophily preservation term minimizing first- and second-order proximity between connected nodes, and the community evolution preservation term maximizing temporal smoothness of communities at different granularity levels given their representation as an aggregation of their members.

The initial community assignment is generated using the Louvain method for high-level communities and using K-means for fine-grained communities given a max community size parameter $w$. After that, embeddings at each snapshot are optimized by employing a dependent community detection-like strategy.



% @wangEvolutionaryAutoencoderDynamic2020
% 
% * Approach is similar to to @maCommunityawareDynamicNetwork2020
% * Defines a unified objective where
%   * community characteristics
%   * previous clustering
%   * are incorporated as a regularization term
% * **They argue that real world networks are non-linear** in nature and **classical approaches can not capture this**
%   * Autoencoders can though
% * Methodology:
%   * Construct a similarity matrix using Dice Coefficient (handles varying degrees well)
%   * Apply stacked (deep) autoencoders to learn the low-dimensional representation
%   * Characterizes tradeoff between two costs:
%     * Snapshot cost (SC):
%       * Reconstruction loss
%       * Node embedding similarity (homophiliy) between connected nodes
%       * Node embedding similarity (homophiliy) between nodes in same community
%     * Temporal cost (TC)
%       * Temporal smoothness of node embeddings
%   * Adopt K-means to discover community structure

@wangEvolutionaryAutoencoderDynamic2020 employs a similar strategy for DCD detection by utilizing the Graph Autoencoder architecture. Authors add an additional community score term to the objective function, therefore also minimizing the distance between nodes in the same community. At last, K-means is run on the representational vectors to detect communities at different timesteps while reusing the outputs from the previous step.



### Multi-modal community detection

% TODO: Describe and maybe more refs
% 
% * Improves community quality
% * Comes in form of Content or Meta-topological -based data
% * Represented separately and aggregated into a single representation vector
% * Can calculate similarity over these
% * e2e optimization makes these additions viable

Another way to improve community quality is by incorporating multi-modal features. These can come in the form of node attributes, content-based or meta-topological data. These representations are incorporated into learned embedding vectors either by direct learning, incorporating them into the objective function, or use of pre-trained models.



% @faniUserCommunityDetection2020
% 
% * Propose a new method of identifying user communities through multi-modal feature learning:
%   * learn user embeddings based on their **temporal content similarity**
%     * Base on topics of interest
%     * Users are considered like-minded if they are interested in similar topics at similar times
%     * Learn embeddings using a context modelling approach
%   * learn user embeddings based on their **social network connections**
%     * Use GNN which works as a skip-gram like approach by generating context using random walks
%   * **interpolate** temporal content-based embeddings and social link-based embeddings
% * Then they use these multi-modal embeddings to detect dynamic communities\
%   * Communities are detected on a modified graph
%     * Weights are set given embedding similarity
%     * Communities are detected using Louvain methods
%   * Then test their approach on specific tasks such as
%   * News recommendation
%   * User for content prediction
% * Note: **This approach detects static communities**
%   * But the communities implicitly take time into account

In @faniUserCommunityDetection2020 the authors describe their method for identifying user communities through multi-modal feature learning. First user embeddings are learned based on their temporal content similarity by looking at topics of interest. Per-user, a heat map is constructed measuring the user's interest over time and topic axes. By considering users like-minded if their heat maps overlap enough they train low-dimensional content embeddings spanning this user similarity space. Next, they use random walk-based GNN methods to learn topological similarity embeddings for network nodes. Finally, they modify the graph by setting edge weights proportionally to node proximity in this combined embeddings space. After that, the Louvain method is applied to extract these time and content-aware communities.



% @wangVehicleTrajectoryClustering2020
% 
% * Transform task of trajectory clustering into one of Dynamic Community Detection
%   * discretion the trajectories by recording entity their current neigbors at each time interval
%   * Edge streaming network is created
% * Use representation learning to learn node their embeddings
%   * Use dyn walks to perform random walks in time dimenstion
%   * Use negative sampling to avoid the softmax cost
% * Then use K-means to find the communities
%   * Try K-means, K-medioids and GMM (Gaussian Mixture Models)
%   * Initalize the centers at the previous timestamp centers
% * Use quality measures to establish quality of results



% Important drawback:
% 
% * Require representation for each node



% **Resources:**
% 
% * https://paperswithcode.com/paper/a-comprehensive-survey-on-community-detection/review/

## Evaluation

% * Community definition varies a lot
% * Describe how detection and result tracking are evaluation
% * Detection and Tracking are evaluated separately

In the previous section, we have described the different variations of community structure definitions as well as the approaches used for detecting communities. In this section, we will cover how the found dynamic community structures can be evaluated in a more general setting to allow a comparison of different approaches. Dynamic community detection problems can be seen as a combination of two tasks, matching and tracking. Due to the large difference between the two tasks, the evaluation is usually conducted separately for each of the tasks.

In the following sections, we cover both of the evaluation methods for both tasks by classifying approaches used in the literature in three classes, namely, annotated, metric-based, and task-specific.



### Annotated

% * Ground Truth communities - compare against them
% * Use NMI or other set overlap method
% * Talk about NF1 @rossettiNovelApproachEvaluate2016
% * Usually no planted communities:
%   * Synthetic datasets
%   * Use annotated features

Evaluation of detected (dynamic) communities becomes much easier when the *ground-truth communities* are provided. The evaluation is then done by comparing the difference between the produced communities and the effective ones. To perform this comparison, the information theory-based metric Normalized Mutual Information (NMI) is used which converts community sets to bit-strings and quantifies the “amount of information” that can be obtained about one community by observing the other [@lancichinettiDetectingOverlappingHierarchical2009].

A possible drawback of this measure is that its complexity is quadratic in terms of identified communities. In [@rossettiNovelApproachEvaluate2016] alternative measure (NF1) with linear complexity is introduced which similarly to the F1 score uses the trade-off between precision and recall (of the average of harmonic means) of the matched communities. In the follow-up work [@rossettiANGELEfficientEffective2020] the authors describe a way to apply this measure within the context of DCD by calculating this score for all the snapshots and aggregating the results into one single measure.

% * Accuracy: @mrabahRethinkingGraphAutoEncoder2021
% * NMI: @mrabahRethinkingGraphAutoEncoder2021, @jiaCommunityGANCommunityDetection2019, @yangCommunityAffiliationGraphModel2012
% * ARI: @mrabahRethinkingGraphAutoEncoder2021, @luoDetectingCommunitiesHeterogeneous2021
% * F1: @jiaCommunityGANCommunityDetection2019, @yangCommunityAffiliationGraphModel2012
% * Omega Index: @yangCommunityAffiliationGraphModel2012
%   * is the accuracy on estimating the number of communities that each pair of nodes shares

Aside from NMI other measures are employed such as Jaccard Coefficient, Accuracy and Rand-Index measuring community overlap [@yangGraphClusteringDynamic2017; @mrabahRethinkingGraphAutoEncoder2021; @luoDetectingCommunitiesHeterogeneous2021], Overlapping-NMI [@yeDeepAutoencoderlikeNonnegative2018] and Omega-Index measuring is the accuracy on estimating the number of communities that each pair of nodes shares [@yangCommunityAffiliationGraphModel2012],

In the real world, there are usually no ground-truth communities. Therefore this approach is usually applied on synthetic datasets where the communities and their dynamicity is sampled from a distribution. An alternative approach some papers take is by defining ground-truth communities using the metadata and node attributes present within the datasets. Some datasets may include annotated communities, but this is not common within DCD datasets.



### Metric based

% * When Ground Truth Communities don't exist
% * Network-based measures

Another way to evaluate and compare different CD algorithms without knowing ground-truth communities is using a quality function. 

#### Network-based metrics

The first group of measures we consider operates directly on the network structure. They are most commonly used to evaluate link-based methods as their results are network partitioning sets. Modularity is the most widely used measure [@newmanFastAlgorithmDetecting2004; @suComprehensiveSurveyCommunity2021], since it measures the strength of division of a network into modules. Networks with high modularity have dense connections between the nodes within the modules and sparse connections between nodes in different modules. Other measures are used as well including:

* **Conductance**: the percentage of edges that cross the cluster border
* **Expansion**: the number of edges that cross the community border
* **Internal Density**: the ratio of edges within the cluster with respect to all possible edges
* **Cut Ratio and Normalized Cut**: the fraction of all possible edges leaving the cluster
* **Maximum/Average ODF** (out-degree fraction): the maximum/average fraction of nodes’ edges crossing the cluster border
* **Triangle Participation Ratio TPR**: measures fraction of triads within the community. A higher TPR indicates a denser community structure

% See: @suComprehensiveSurveyCommunity2021 as it has a well curated collection of evaluation metrics

#### Proximity-based measures

Proximity-based measures are often used to evaluate clustering tasks but are also often employed for representation-based CD methods since there is a large overlap in their methodology. Additionally, representation-based approaches have the benefit of being able to quantify both entities and communities as a d-dimensional vector enabling a more direct comparison of the two [@wangVehicleTrajectoryClustering2020]. The most common measures include:

% @wangVehicleTrajectoryClustering2020
% 
% * Use taxi dataset with license plates
% * Compare to other deep GNN - they only learn static representation
% * Metrics:
%   * **Silhouette Coefficient** (SC) - range [-1, 1]
%     * $S(i)=\frac{b(i)-a(i)}{\max \{a(i), b(i)\}}$
%     * $a$ avg distance between node and neighbors in cluster
%     * $b$ is min val of avg distances between node and other clusters
%   * **Davies-Bouldin Index** (DBI)
%     * $D B I=\frac{1}{N} \sum_{i=1}^{N} \max_{j \neq i}\left(\frac{\overline{S_{i}}+\overline{S_{j}}}{\left\|w_{i}-w_{j}\right\|_{2}}\right)$
%     * $\bar{S_i}$: avg distance of nodes in cluster $i$ to centroid of cluster $i$
%     * $w_i$ is the centroid of cluster $w_i$
%     * It is the ratio of the sum of the average distance to the distance between the centers of mass of the two clusters
%     * The closer the clustering result is with the inner cluster, and the farther the different clusters, the better the result
%   * **Calinski-Harabaz Index** (CHI): Ratio of the between-cluster variance and within-cluster variance
%     * $C H I=\frac{\operatorname{tr}\left(B_{k}\right)}{\operatorname{tr}\left(W_{k}\right)} \frac{m-k}{k-1}$
%     * $m$ number of nodes, $k$ number of clusters,
%     * $B_k$ is covariance matrix between the clusters
%     * $W_k$ is covariance matrix between the data in the cluster
%     * $tr$ is trace of the matrix

* **Silhouette Coefficient**: is defined as $S(i)=\frac{b(i)-a(i)}{\max \{a(i), b(i)\}}$ where $a(i)$ defines the mean distance from node $i$  to other nodes in the same cluster while $b(i)$ is mean distance to any node not in the same cluster. It measures cohesion of a cluster/community and indicates how well a node is matched to its own cluster.
* **Davies-Bouldin Index**: is the ratio of the sum of the average distance to the distance between the centers of mass of the two clusters. In other words, it is defined as a ratio of within-cluster, to the between cluster separation. The index is defined as an average over all the found clusters and is therefore also a good measure to deciding how many clusters should be used.
* **Calinski-Harabasz Index**: is the ratio of the between-cluster to the within-cluster variance. Therefore it measures both cohesion (how well its members fit the cluster) as well as compares it to other clusters (separation).
  
  

#### Stability-based measures

% * Stability-based measures evaluate temporal stability and consistency of the network
%   * The ones we consider were introduced in @maCommunityawareDynamicNetwork2020
%   * 
% * **Network Stabilization**: evaluates performance of DNE on stabilization of embedding
%   * dynamic network should have similar evolutionary patterns in both the learned low-dimensional representation and the
%     network representation over time
%   * evaluates the evolution ratio of the low-dimensional node representations to the network representations at $a$-th time stamp
%   * $p_{s}^{a}=\frac{\left(\left\|\mathrm{H}^{a+1}-\mathrm{H}^{a}\right\|_{2}^{2}\right) /\left\|\mathrm{H}^{a}\right\|_{2}^{2}}{\left(\left\|\mathrm{~A}^{a+1}-\mathrm{A}^{a}\right\|_{2}^{2}\right) /\left\|\mathrm{A}^{a}\right\|_{2}^{2}} .$
% * **Community Stabilization**: evaluates stability of communities in dynamic networks on the embedded low-dimensional representations
%   * Evaluates community evolution ratio to network representation evolution between subsequent time stamps
%   * Lower values point to more stable communities and are better
%   * $p_{c}^{a}=\sum_{k=1}^{q}\left(\frac{\left(\left\|\mathrm{H}_{c_{k}}^{a+1}-\mathrm{H}_{c_{k}}^{a}\right\|_{2}^{2}\right) /\left\|\mathrm{H}_{c_{k}}^{a}\right\|_{2}^{2}}{\left(\left\|\mathrm{~A}_{c_{k}}^{a+1}-\mathrm{A}_{c_{k}}^{a}\right\|_{2}^{2}\right) /\left\|\mathrm{A}_{c_{k}}^{a}\right\|_{2}^{2}}\right) / q$
% * Note: the evaluation is at graph level since their methods are spectral GAE based

To evaluate temporal stability and consistency of the node and community structures, measures based on temporal smoothness are proposed in @maCommunityawareDynamicNetwork2020. These measures compare the evolution rate of network structures against the evolution rate of the whole network given node embeddings between two consecutive snapshots. The intuition is that rapidly evolving structures relative to the global evolution rate are temporally unstable and thus of low quality. Metrics proposed include:

* **Network Stability**: Is defined as +@eq:networkstability and evaluates the evolution ratio of the low-dimensional node representations to the network representations between snapshots at $a$-th time stamp.
* **Community Stability**: Is defined as +@eq:communitystability and computes stability of communities in dynamic networks on the embedded low-dimensional representations. It is represented as evolution ratio of the low-dimensional community representations to the network representations between snapshots at $a$-th time stamp.

$$
p_{s}^{a}=\frac{\left(\left\|\mathrm{H}^{a+1}-\mathrm{H}^{a}\right\|_{2}^{2}\right) /\left\|\mathrm{H}^{a}\right\|_{2}^{2}}{\left(\left\|\mathrm{~A}^{a+1}-\mathrm{A}^{a}\right\|_{2}^{2}\right) /\left\|\mathrm{A}^{a}\right\|_{2}^{2}}
$$ {#eq:networkstability}

$$
p_{c}^{a}=\sum_{k=1}^{q}\left(\frac{\left(\left\|\mathrm{H}_{c_{k}}^{a+1}-\mathrm{H}_{c_{k}}^{a}\right\|_{2}^{2}\right) /\left\|\mathrm{H}_{c_{k}}^{a}\right\|_{2}^{2}}{\left(\left\|\mathrm{~A}_{c_{k}}^{a+1}-\mathrm{A}_{c_{k}}^{a}\right\|_{2}^{2}\right) /\left\|\mathrm{A}_{c_{k}}^{a}\right\|_{2}^{2}}\right) / q
$$ {#eq:communitystability}

Where $H^a_i$ and $H^a_{c_k}$ represent the embedding vector for a node $i$ and a community $k$ at $a$-th timestamp respectively. Similarly separate node and community based adjacency matrices are defined as $A^a$ and $A^a_{c_k}$. Therefore, in this case $\left\|\mathrm{H}_{c_{k}}^{a+1}-\mathrm{H}_{c_{k}}^{a}\right\|_{2}^{2}$ represents the normalized euclidean distance between the two clusters while $\left\|\mathrm{~A}_{c_{k}}^{a+1}-\mathrm{A}_{c_{k}}^{a}\right\|_{2}^{2}$ represents the normalized euclidean distance between the adjacency matrices of the two clusters. For network stability no such grouping is done and evaluation is performed on node-based (global) representation and adjacency matrices. 

### Task specific

% * @peelGroundTruthMetadata2017 Criticize methods that define their own community criteria
%   * No free lunch - solving task for your definition wont solve all CD problems
%   * When CD algorithm, it is indistinguishable from possibilities: irrelevant metadata, orthogonal data, network lacks structure.
%   * Evaluate on tasks and use-cases and not based on a single measure

In [@peelGroundTruthMetadata2017] the authors criticize annotation and metric-based CD evaluation approaches by proving that they introduce severe theoretical and practical problems. For one, they prove the no-free lunch theorem for CD, ie. they prove that algorithmic biases that improve performance on one class of networks must reduce performance on others. Therefore, there can be no algorithm that is optimal for all possible community detection tasks, as the quality of communities may differ by the optimized metrics. Additionally, they demonstrate that when a CD algorithm fails, the poor performance is indistinguishable from any of the three alternative possibilities: (i) the metadata is irrelevant to the network structure, (ii) the metadata and communities capture different aspects of network structure, (iii) the network itself lacks structure. Therefore, which community is optimal should depend on its subsequent use cases and not a single measure.

% * Problems:
%   * Absence of ground truth communities
%   * Modularity cant be used - based on explicit links between users (structural)
%     * Doesn't account for content at all
% * Solutions: Application level evaluation
%   * A user community detection method is considered to have better quality iff its output communities improve an underlying application

To address this issue, it is common to evaluate the algorithm on both earlier described approaches as auxiliary tasks. The general sentiment behind it is, that communities have better quality if they improve an underlying application. In the following sections, we describe a few commonly used auxiliary tasks in literature.



#### Link-prediction

% * Link-prediction is a common task to evaluate embedding quality within Graph Representation learning
%   * It usually given an existing graph links are split into a train and test set
%   * The model is trained on the test set, while the edges in train set are predicted and evaluated using classification metrics
%   * In context of dynamic community detection some changes can be applied 
% * **User Prediction** @faniUserCommunityDetection2020
%   * Goal: Predict which users posted a news article $a$ at time $t$
%   * Methodology:
%     * Find closest community to the article in terms of interest at time $t$ (cosine sim)
%     * Members of community are predicted users
%   * Same reasoning as news prediction
%   * Metrics (classification metrics)
%     * Precision, Recall, F-measure
% * **Link Prediction**: @maCommunityawareDynamicNetwork2020
%   * Prediction of existence of links between nodes in the next time stamps
%   * Based on representation in the current time stamp

A common way to evaluate the quality of extracted node embeddings within Graph Representation learning is using the link-prediction task. As link-prediction can be done in an unsupervised manner, it does not require additional labeling for evaluation. The edge set of the input network is split into a train set on which the model is trained, and a test set on which is used to compare the predicted links against. When using node representation learning, the predictions are defined by proximity between the nodes and a threshold. To quantify the quality of the results classification metrics are usually employed such as accuracy, precision, recall, and f-score.

In the context of CD and DCD, the link prediction is modified to measure the predicting capability of the community embeddings. @faniUserCommunityDetection2020 defines a user prediction task to predict which users posted a certain news article at a certain time. Their methodology is, given a news item at time $t$, find the closest community to the article in representational similarity space. All members of the given community are seen as predicted users over which the classification metrics are calculated. Similarly, @maCommunityawareDynamicNetwork2020 modify the task by predicting whether the edges will still exist within the next timestamp to also quantify the temporal prediction capability of the trained embeddings.

#### Recommendation Tasks

% * Generally identified by the fact that they do population based recommendation
% * **News recommendation** (in time) @faniUserCommunityDetection2020
%   * Curate dataset of news articles mentioned by users (user mention means user interest)
%   * Methodology:
%     * Detect communities and assign them a topic of interest at a time
%     * Topic is average of user interests
%     * All news articles are ranked based on their similarity with the overall topic (in time)
%     * Each member in community is recommended the ranked list
%   * Metrics: (stadard info retreval metrics)
%     * Precision at rank $k$ ($P_k$)
%       * $\mathrm{P}_{k}=\frac{1}{|\mathrm{U}|} \sum_{u \in \mathbb{U}} \frac{t p_{u}}{k}$
%       * $u$ is user
%     * Mean Reciprocal Rank (MRR)
%       * $\mathrm{MRR}=\frac{1}{|\mathbb{U}|} \sum_{u \in \mathbb{U}} \frac{1}{\operatorname{rank}_{u}}$
%       * First position correct result occurs in list
%     * Success at rank $k$ ($S_k$)
%       * Probability that at least one correct item is within a top-k list
%       * $\mathrm{S}_{k}=\frac{1}{|\mathbb{U}|} \sum_{u \in \mathcal{U}}\left(\operatorname{rank}_{u} \leq k\right)$
% * @rozemberczkiGEMSECGraphEmbedding2019
%   * Music recommendation
% * Based on link prediction or friend recommendation @huangInformationFusionOriented2022
%   * Precision
%   * Recall
%   * F-score
%   * normalized discounted cumulative gain (nDCG)
%   * mean reciprocal rank (MRR)

Another way the quality of node representations can be evaluated is by using recommendation tasks. Here the idea is, instead of predicting a single item like in link-prediction, to rank the items based on their recommendation confidence. Using the ranked list, standard information retrieval metrics such as precision at rank, mean reciprocal rank, and success at rank k can be computed. This approach is applied to CD [@rozemberczkiGEMSECGraphEmbedding2019; @huangInformationFusionOriented2022; @faniUserCommunityDetection2020] by ranking recommendations per community instead of on an individual basis. Communities with higher scores therefore would indicate high similarity between their members.



% @maCommunityawareDynamicNetwork2020
% 
% * Use both synthetic and real world datasets
% * Use not perse community detection baselines
% * Define auxilary helper tasks in context of *community aware* **Deep Network Embedding**:
%   * **Network Reconstruction**: Evaluates model on ability of reconstructing link structures of the network
%     * Average reconstruction precision is measured
%     * This is done for each timestamp
%     * For each node, the nearest embedding neighbors are used as predicted links

## Datasets

% * Curate the popular dataset used in literature % * Start by discussing Synthetic Datasets % * Their properties and
%   assumptions they are based on % * Then we discuss Real World Datasets % * Describe properties

In this section, we curate popular datasets used in literature for CD and DCD tasks. First, we discuss methods for
generating synthetic datasets containing ground-truth communities. We conclude this section with an overview of
real-world dynamic networks.

### Synthetic Datasets

Synthetic datasets are datasets generated by specific models using manually designed rules. These datasets are generated
with communities in mind and therefore most of the time contain ground-truth communities which can be used for the
evaluation of CD algorithms. In this section, we describe a selection of popular synthetic dataset generation
techniques.

% * Generated by specific models on manually designed rules % * @lancichinettiBenchmarkGraphsTesting2008 (LFR)
% * Most common benchmark % * Accounts for heterogenity of degree and community size % * simulates the degree of nodes
%   in a real-world network and the scale-free nature of the network % * Modularity based % * Static communities % * Each
%   node is given degree from power of law distribution % * Each node shares as fraction $\mu$ of links with its community %

* Sizes of communities are taken from power of law distribution % * Rewirings are applied to keep degrees the same and
  distribution between in and out links % * @lancichinettiBenchmarksTestingCommunity2009 % * Extension for overlapping
  communities

The most common benchmark dataset was introduced in @lancichinettiBenchmarkGraphsTesting2008. Their method for
generating a network creates communities and their nodes with size and degree drawn from a power-law distribution.
Thereafter, node edge rewiring is applied to enforce a mixture parameter $\mu$ which determines the ratio of in- to
between-community connections. In follow-up [@lancichinettiBenchmarksTestingCommunity2009] the authors extend their
method to work for overlapping communities.

% @greeneTrackingEvolutionCommunities2010 % % * Generation is based on @lancichinettiBenchmarkGraphsTesting2008 % *
% Extend to generate graph snapshots % * Changes are controlled by injecting user-specified community events

In @greeneTrackingEvolutionCommunities2010 the authors modify the @lancichinettiBenchmarkGraphsTesting2008 method to
generate dynamic community networks. They do this by defining a set of community events that can be used as seeds for
the temporal evolution of the network. The subsequent snapshots of the graph are defined by this set of events.

% @granellBenchmarkModelAssess2015 % % * Propose a model for generating simple dynamic networks % * based on stochastic
% block models % * Time evolution consists of periodic oscillation of the systems structure between configurations % *
% Method:
% 
% * Graph is divided in subgraph communities % * Their interconnecitons have a probabiliy % * Their intra-connecitons
%   have probabilitily % * Edges are drawn from binom distribution % * Implements growing and shrinking, merge and split
%   actions by modifying these probabilities for ceratian subsets

@granellBenchmarkModelAssess2015 propose a model for generating simple dynamic networks based on stochastic block
models (SBM). The time evolution consists of periodic oscillation of the system's structure between configurations. Once
the initial network is created, it is divided into subgraphs each having certain inter-and intra- connectivity
probability. The edges between the timestamps are drawn from binomial distributions which themselves are modified by
stochastic community-based events such as shrinking, growing, merging, and splitting.

% @ghalebiDynamicNetworkModel2019 % % * Develeop a generative dynamic network model from partial observations (diffusion
% data)
% 
% * Assuming each nodes belongs to a cluster where a model is created modelling the ingoing and outgoing edges % * Model
%   can be used to determine model parameters from real world datasets and generate similar ones

@ghalebiDynamicNetworkModel2019 similarly proposes an SBM model-based approach to generate dynamic community networks.
Additionally, they propose a methodology to learn model parameters from existing real-world networks.

### Real-world Datasets

% * Can be categorized into % * Citation/Co-authorship Networks % * Social Networks % * Biological % * Webpage % *
%   Purchasing/co-purchasing % * We describe their scale in terms of Node and Edge count % * Most of the datasets have meta
%   topological information in terms of Node types % * As edge types are not common we ignore this data % * Additionally we
%   list the amount of available attributes

Real-world network datasets are often categorized based on the source they model such as social networks, biological
networks, and webpage networks. In +@tbl:realworlddatasets we present an overview of the most commonly used datasets in
the literature. For each of the datasets, we describe their scale in terms of their node and edge counts. Additionally,
we specify the presence of meta-topological information in the form of node types, whether the network is dynamic,
whether it includes content-based (unstructured/textual) information, and the availability of node features.

| Dataset                                                                                                | Reference                                                                 | Nodes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Edges&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Node/Edge Types&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Properties &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
| ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------- | ----------------------------------------- | ----------------------------------------- | --------------------------------------------------------------------------------------- | ----------------------------------------------------------- |
| [Zachary karate club](http://konect.cc/networks/ucidata-zachary/)                                      | [@zacharyInformationFlowModel1976]                                        | 34                                        | 78                                        | 1/1                                                                                     | N/N/Y/Y                                                     |
| [Football](https://networkrepository.com/misc-football.php)                                            | [@girvanCommunityStructureSocial2002]                                     | 115                                       | 613                                       | 1/1                                                                                     | N/N/N/N                                                     |
| [Star Wars Social](https://www.kaggle.com/ruchi798/star-wars)                                          | [@StarWarsSocial]                                                         | 113                                       | 1599                                      | 1/2                                                                                     | Y/Y/N/N                                                     |
| [Enron](https://www.kaggle.com/wcukierski/enron-email-dataset/)                                        | [@vanburenEnronDatasetResearch2009]                                       | 605K                                      | 4.1M                                      | 2/3                                                                                     | Y/Y/Y/Y                                                     |
| [IMDB 5000](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset)                               | [@IMDB5000Movie]                                                          | 16K                                       | 52K                                       | 4/4                                                                                     | Y/N/N/N                                                     |
| [DBLP-HCN](https://data.mendeley.com/datasets/t4xmpbrr6v/1)                                            | [@tangArnetMinerExtractionMining2008; @yangDefiningEvaluatingNetwork2012] | 11K                                       | 16K                                       | 3/2                                                                                     | Y/N/N/Y                                                     |
| [DBLP-V1](https://www.aminer.org/citation)                                                             | [@tangArnetMinerExtractionMining2008; @yangDefiningEvaluatingNetwork2012] | 1.2M                                      | 2.4M                                      | 3/3                                                                                     | Y/N/N/Y                                                     |
| [DBLP-V3](https://www.aminer.org/citation)                                                             | [@tangArnetMinerExtractionMining2008; @yangDefiningEvaluatingNetwork2012] | 2.7M                                      | 8.2M                                      | 3/3                                                                                     | Y/N/N/Y                                                     |
| [Weibo](http://www.wise2012.cs.ucy.ac.cy/challenge.html)                                               | [@Home13thInternationala]                                                 | 8.3M                                      | 49M                                       | 2/3                                                                                     | Y/Y/N/N                                                     |
| [FB-wosn](http://socialnetworks.mpi-sws.org/datasets.html)                                             | [@viswanathEvolutionUserInteraction2009]                                  | 64K                                       | 1.3M                                      | 2/2                                                                                     | Y/Y/N/N                                                     |
| [LastFM](http://ocelma.net/MusicRecommendationDataset/lastfm-1K.html)                                  | [@celmaMusicRecommendationDiscovery2008]                                  | 272K                                      | 350K                                      | 3/3                                                                                     | Y/Y/Y/N                                                     |
| [Reddit](https://zenodo.org/record/3608135)                                                            | [@baumgartnerPushshiftRedditDataset2020]                                  | 61M                                       | 1.2B                                      | 4/1                                                                                     | Y/Y/Y/N                                                     |
| [Digg](https://www.isi.edu/~lerman/downloads/digg2009.html)                                            | [@hoggSocialDynamicsDigg2012]                                             | 142K                                      | 3.7M                                      | 3/2                                                                                     | Y/Y/N/N                                                     |
| [Wiki-RFA](https://snap.stanford.edu/data/wiki-RfA.html)                                               | [@westExploitingSocialNetwork2014]                                        | 10K                                       | 159K                                      | 1/1                                                                                     | Y/Y/Y/N                                                     |
| [Bitcoin](https://snap.stanford.edu/data/soc-sign-bitcoin-otc.html)                                    | [@kumarEdgeWeightPrediction2016]                                          | 5K                                        | 35K                                       | 1/1                                                                                     | Y/Y/N/N                                                     |
| [Rumor Detection](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0168344)           | [@kwonRumorDetectionVarying2017]                                          | 54M                                       | 1.9B                                      | 2/2                                                                                     | Y/Y/N/N                                                     |
| [sx-mathoverflow](https://snap.stanford.edu/data/sx-mathoverflow.html)                                 | [@SNAPNetworkDatasets]                                                    | 24K                                       | 506K                                      | 1/3                                                                                     | Y/Y/N/N                                                     |
| [sx-superuser](https://snap.stanford.edu/data/sx-superuser.html)                                       | [@SNAPNetworkDatasets]                                                    | 194K                                      | 1.4M                                      | 1/3                                                                                     | Y/Y/N/N                                                     |
| [Eu-core network](https://snap.stanford.edu/data/email-Eu-core.html)                                   | [@SNAPNetworkDatasets]                                                    | 1005                                      | 25K                                       | 1/1                                                                                     | N/Y/Y/Y                                                     |
| [com-Youtube](https://snap.stanford.edu/data/com-Youtube.html)                                         | [@SNAPNetworkDatasets]                                                    | 1.1M                                      | 298K                                      | 1/1                                                                                     | N/N/Y/Y                                                     |
| [116th House of Representatives](https://www.kaggle.com/aavigan/house-of-representatives-congress-116) | [@116thHouseRepresentatives]                                              | 6249                                      | 12K                                       | 3/4                                                                                     | N/N/Y/N                                                     |
| [social-distancing-student]()                                                                          | [@wangPublicSentimentGovernmental2020]                                    | 93K                                       | 3.7M                                      | 3/7                                                                                     | Y/N/N/N                                                     |

Table: Summary of real-world datasets. First two columns indicate the source and reference work for the given dataset.
Subsequent columns define the statistics about the node and edge counts. Node/Edge types defines the amount of different
types of nodes/edges indicating whether a network is heterogenouous. The "Properties" indicates whether network is a
temporal network, whether it is an interaction network, presence of unstructured text content, and whether there
are ground truth communities available for this network. {#tbl:realworlddatasets}

% TODO: Added my found datasets % % * TODO: Specify edge types % * TODO: Update node and edge counts using my numbers

# Research Questions

% The main goal of my thesis: to build a framework for community detection and representation in dynamic heterogeneous networks.
% 
% * This in order to analyse the communities within the set of datasets provided by Prof. Wang (**todo** cite work)
%   * Datasets are Dynamic, heterogeneous, Include content-based data
%   * Combination poses a problem
% * To the best of my knowledge: there are currently no algorithms that can do this
% * Direct comparison is therefore impossible
% * Introduce our research questions
%   * Elaborate a bit on their relevance
%   * In next section explain their implementation

The main goal of my thesis is to build a framework for community detection and representation in dynamic heterogeneous networks.

This is, to enable dynamic community analysis on the datasets proposed in @wangPublicSentimentGovernmental2020. The data described is collected from the Twitter social platform and is dynamic, heterogeneous, and rich in content-based (unstructured text) data. To the best of our knowledge, there are currently no dynamic community detection algorithms that can handle this data without relaxing its rich data representation (data loss).

As there are no alike algorithms, direct comparison is not possible. To both validate the merit of our methods as well as the quality of the results, we spread our research over four research questions.



[Information]{#rqq:rq1}

: *Does addition of meta-topological and/or content-based information improve quality of detected communities?*

% * Dataset rich in extra data: 
%   * meta-topological
%   * content based
% * Previous approach treat all nodes alike 
%   * ignoring most important structural features (node types)
%   * types such as hashtags can also be represented as nodes solving many issues which require topic modelling
% * Improvements in natural text analysis allow for representation of unstructured text
%   * Would this data improve quality of embeddings

While focusing on link-based features, CD algorithms treat all nodes alike. Therefore ignoring arguably most important structural features, namely node types. Additionally, supplementary node types can be constructed from categorical features enhancing network topology and solving issues requiring topic modeling.

Similarly, recent improvements in natural text processing allow for efficient representation of natural text which is proven to improve the quality of node embeddings. As the formation of communities is not purely a sociological process, the CD problem should benefit from the incorporation of such content-based features.



[Scale]{#rqq:rq2}

: *Does usage of graph representation function learning techniques improve scale of CD beyond current state-of-the-art?*

% * In last few years new graph representation approaches were introduced
%   * Instead of operating on the whole networks
%   * They sample network using random walks or convolutions
% * As previous methods for CD ignored them (used spectral methods)
%   * Causing scalability issues posed by spectral methods
%   * It is important to test these approaches as they may yield performance improvements
% * Previous approaches use spectral methods limiting them to one network per snapshot
% * Instead learning representation function
%   * would allow limit computational complexity
%   * allow for parameter sharing across timesteps
%   * Allow for streaming graphs

The previously mentioned representation-based DCD method use spectral graph representation methods which operate on the whole network at once. More recent graph representation approaches instead learn a graph representation function by sampling the network using random walks or convolutions.

This has a two-fold positive effect on the scalability of the algorithms. Computation can be done more efficiently as opposed to spectral methods which rely on adjacency matrices. By learning a representation function, embeddings are computed on-demand instead of being held in memory for the whole network, therefore, limiting the impact of big networks. Other benefits may include the fact that they would be a suitable choice for processing streaming edge network variants.



[Modelling]{#rqq:rq3}

: *Does making temporal features implicit in node representations provide better quality communities as opposed to making them explicit?*

% * Previous approaches either
%   * Learn the temporal component implicitly in node representations, causing embedding be temporally aware
%   * Separate the temporal aspect explicitly by defining 

Throughout the literature, various ways are used to incorporate temporal aspects into node embeddings. The implicit approach aims to make the embeddings temporally aware while the explicit approach creates a separate embedding for each snapshot. While methods using either approach have presented good results in the literature, it is important to analyze the potential trade-off and benefits of both.



[Results]{#rqq:rq4}

: *Do community-aware node embeddings perform well on both node as well as community evaluation based tasks?*

% * While optimizing for multiple objectives (community and individual based)
%   * It is important to evaluate algorithm against other community algorithms
%   * As well as dynamic representation methods
%   * To see whether community-aware embeddings provide benefits for standard node represenation tasks

While the main task of the algorithm is to find high-quality dynamic communities, the result also includes community-aware dynamic embeddings. Aside from testing the quality of the communities, it is important to compare how this community awareness influences the embeddings on dynamic node representation tasks such as link prediction and node classification.

# Approach

% * Split our task into multiple sections:
%   * Final result will be Framework for DCD in heterogenous graphs
%   * Set of results comparing this algorithm on current state of the art
%   * Ablation tests in-line with research questions providing evidence for our choices
% * First we talk about setup for experimentation and benchmarking
% * Then we talk about strategy for extracting representation learning and graph / data sampling
% * Then we will define components for our objective function
% * Finally we will discuss how community detection results are extracted 

To answer to our research questions, the methodology is split into multiple parts which address the algorithm architecture, construction of the objective function, DCD result extraction, selection of the baselines and setup of evaluation benchmarks. While split, it is important to note that these tasks have a large overlap and won't be considered in isolation.

The final result of my thesis will consist of the graph representation learning based framework for DCD within dynamic heterogeneous graphs, a set of results comparing the algorithm to the current state-of-the-art approaches on various related tasks, and a set of ablation tests providing empirical corroboration for important design choices.



## Representation learning

% * Core part of the framework as it is responsible for graph sampling and embedding inference
% * Subcomponents:
%   * Graph random walk algorithm (time aware) - random walk or convolution
%   * Representation learning function (shallow or deep)
%   * Feature fusion

The core part of the proposed framework is the representation learning algorithm as it is responsible for both graph sampling, as well as provides the architecture for the deep neural network (DNN) used to learn the node representation. The representation learning can be split into three main components. Each of the components respectively has sufficient prior work and has been used in literature. The challenge is in combining them into an efficient architecture for representation learning capable of maximizing set objective function.

### Graph Sampling

Graph sampling is a way to enforce learning of desired topological properties from the network. Various ways to sample graphs can be found in the literature. For both shallow as well as graph representation function learning approaches the trade-off lies between depth-first search (DFS) [@tangLINELargescaleInformation2015; @groverNode2vecScalableFeature2016] and breadth-first search (BFS) [@perozziDeepWalkOnlineLearning2014] based approaches. While literature has shown that DFS based approaches work better for optimizing homophily [@groverNode2vecScalableFeature2016], our goal is to explore the benefits of both as a hybrid approach within our multi-objective setting. Additionally, we plan on supporting both support heterogeneous graph sampling [@wuAuthor2VecFrameworkGenerating2020; @yingGraphConvolutionalNeural2018; @yangHeterogeneousNetworkRepresentation2020; @dongMetapath2vecScalableRepresentation2017; @wangHeterogeneousGraphAttention2019], and temporally aware sampling [@nguyenContinuousTimeDynamicNetwork2018; @wuSageDyNovelSampling2021; @dasguptaHyTEHyperplanebasedTemporally2018] by adopting extensions described in the literature.

% * TODO: BFS random walk is not equivalent to graph convolution?

### Neural Network Architecture

The architecture and training strategy of the underlying neural network are crucial for a well performing algorithm. Many methods introduced in the previous section already outline an effective architecture suitable for training on the introduced sampling method. Because both data and requirements for our task differ, the algorithms can not be used out of the box. The final architecture should support heterogeneous graph samples, temporal-aware samples, and node features.

The core of our network will be a representation function based algorithm [@hamiltonInductiveRepresentationLearning2018; @yingGraphConvolutionalNeural2018] adopted to deal with different node and edge types, therefore generating embeddings for a nodes sampled neighborhood in time. Related literature will be used as inspiration to improve network's performance in content-rich networks [@wuAuthor2VecFrameworkGenerating2020; @yingGraphConvolutionalNeural2018], using attention-based mechanisms [@abu-el-haijaWatchYourStep2018; @sankarDynamicGraphRepresentation2019; @wangHeterogeneousGraphAttention2019], and alternative training strategies such (variational or diffusion) auto-encoders, GAN [@liVariationalDiffusionAutoencoders2020; @kipfVariationalGraphAutoEncoders2016].

### Feature Fusion

Many of the real-world datasets are feature-rich. Some features are structured and can be passed to the neural network with minimal pre-processing. Some feature types require additional attention. Natural (unstructured) text features can be aggregated into a single representation vector using pre-trained embeddings [@devlinBERTPretrainingDeep2019; @penningtonGloveGlobalVectors2014], and (large) categorical features may be transformed into network nodes, thus moving the information into the topological domain [@chenCatGCNGraphConvolutional2021; @wuTopologicalMachineLearning2020].

## Objective Function

% Build an objective function
% 
% * Homophily (First order + Second order?)
% * Community Cohesion
%   * Sampling based (pairs or motifs)
%   * Similar to AGM
%   * or Modeling the clusters explicitly
% * Temporal Smoothness
% * Community Temporal Smoothness
%   * Sampling based (pairs or motifs)

A crucial part of representation-based DCD methods is the objective function. By utilizing node (and community) representation vectors one can optimize the network to maximize a differentiable multi-objective function using back-propagation. During the training process, the focus can be shifted between the different objectives. By focusing on defining necessary criteria for dynamic community detection which will give a specification for our multi-objective function.

#### Cohesion

The most common definition states, that communities are characterized by more dense inter-community connections compared to intra-community density. Representation methods extend this definition by noting that the density of the connections is can be represented by the topological similarity measure of two nodes. Clustering methods further extend this definition by defining similarity on multi-modal embeddings, therefore keeping the definition consistent for feature rich networks. A viable choice for community cohesion measure would be the Silhouette Coefficient (See +@evaluation)

#### Homophily

To ensure reliable computation of the cohesion measure, the representation vectors need to be accurate. A way to train these representations is by assuming homophily which states that the more two nodes occur in the same context, the more similar they are. This is translated to graph representation learning problems by using node neighborhood as context. Either using first-order proximity where nodes should occur in their counterpart's context or by utilizing second-order proximity where two nodes are similar if they share the same context (through common nodes). Hybrid approaches exist which optimize for both as they both model different semantics. Similarly, this idea can also be extended to feature aware embeddings, therefore, extending the definition of a community through transitivity when the above definition for cohesion is utilized. Homophily is usually measured by the distance of connected node their embeddings within a similarity space (euclidean, cosine, etc.). 

#### Temporal Smoothness

When talking about dynamic networks and communities, temporal smoothness should also be considered. Between subsequent timesteps, the dynamic networks often evolve, but not by a large amount. While individual nodes may change drastically within a single timestep, the communities are seen as more stable structures within the networks. Therefore the evolution of the communities should not exceed to global (network) evolution rate.

In most of the literature, this temporal smoothness is indirectly handled by result matching or reuse of results from previous timesteps. Within representation-based approaches, this property can be quantified and optimized for using cross-times measures. A similar approach is employed to keep the embedding space temporally stable while only individual nodes may change.

## Community Detection

% Pick a community detection/grouping algorithm
% 
% * May be by using KNN and fixed-size groups
% * May be using BIRCH or OPTICS
% * May be using augmentation based method
% * Depending on implementation communities may already be found

In the final step of the framework, dynamic communities need to be identified. This may be done by simultaneously training community embeddings along with the node embeddings [@maCommunityawareDynamicNetwork2020; @limBlackHoleRobustCommunity2016; @wangEvolutionaryAutoencoderDynamic2020], therefore having the advantage that objective function can directly influence the resulting communities. Other approaches instead operate on the resulting embedding space or the augmented graphs to extract the resulting communities using link-based methods such as the Louvain method or density-based clustering algorithms such as K-means, BIRCH [@zhangBIRCHEfficientData1996], or OPTICS [@ankerstOPTICSOrderingPoints1999] yielding the benefit of losing the community count assumption.

In our approach, we plan to focus on direct community optimization, while avoiding hard-coding the model to specific assumptions using spectral clustering-based techniques and soft assignment clustering [@liDivideandconquerBasedLargeScale2021; @maCommunityawareDynamicNetwork2020].





## Benchmarks and Baselines

% * Implement or set up necessary baseline algorithms:
%   * Static Representation:
%     * @rozemberczkiGEMSECGraphEmbedding2019
%     * @cavallariLearningCommunityEmbedding2017
%     * @jiaCommunityGANCommunityDetection2019
%   * Dynamic link-based:
%     * @rossettiANGELEfficientEffective2020
%   * Dynamic representation:
%     * @wangEvolutionaryAutoencoderDynamic2020
%     * @maCommunityawareDynamicNetwork2020
% * Implement evaluation benchmarks:
%   * These are meant to provide empircal evidence providing answer to the posed research questions
%     * Research questions are of an Explorative and Quantitative nature
%   * Generic:    
%     * Annotated community detection
%     * Quality metric community detection (Modularity)
%   * Task specific: (on public datasets / on own dataset)
%     * Link prediction
%     * Link recommendation

As the research questions posed in +@research-questions are all mostly of a quantitative nature, it is very important to set up appropriate benchmarks in order to provide a valid answer. As a direct comparison between methods is not always possible, we define auxiliary task benchmarks testing algorithms on desired properties as well as reuse benchmarks used in previous literature to provide a fair comparison.



### Benchmarks and Evaluation

% RQ 1 (@rqq:rq1)
% 
% * Qualitative comparison of temporal and static communities
%   * Link-based:
%     * Annotated: NMI, NF1
%     * Quality Metric: Modularity, Conductance
%   * Representation based:
%     * Clustering measures
%     * Stability measures
%   * Task based:
%     * Content Recommendation
%     * Friend Recommendation
% * Test improvement based on inclusion of content and metadata

To provide an answer for @rqq:rq1, the quality of the algorithm on both static and dynamic communities needs to be compared against the benchmarks for various configurations (considering the content and/or meta-topological data). As our baselines include both representation- as well as link-based approaches, the benchmarks should cover measures used in both groups. To evaluate the quality of the communities, annotation-based approaches (computing NMI and NF1) and quality metric-based evaluation approaches will be employed (See +@evaluation). Since our definition of community slightly differs from the literature as it encompasses network external information (content) we will also employ task-based evaluation such as recommendation tasks (follower recommendation, hashtag recommendation - depending on the dataset).

The @rqq:rq3  is of a more exploratory nature concerning the modeling of temporal information. It aims to determine whether having the ability to track the communities through time yields better results in practice, as opposed to having communities incorporate their temporal information implicitly in their definition. This evaluation is conducted as an ablation test and similarly focuses on the quality measures and task-based evaluation.

The @rqq:rq2 aims to compare the scalability of our approach to the current representation-based approaches. Therefore a rough complexity analysis, as well as performance benchmarking (computation time), shall be conducted.

Finally, @rqq:rq4 addresses the usability of our dynamic community detection results to other tasks concerning dynamic node representation learning. Here we, make use of defined auxiliary tasks (recommendation and link-prediction) to compare our method against other dynamic node representation learning algorithms.

For benchmarking the datasets of different scale and properties are chosen (+@tbl:benchmarkdatasets). Synthetic dataset generation method introduced in @greeneTrackingEvolutionCommunities2010 will be used to create additional networks with ground-truth communities.



| Dataset                                                                                                | Nodes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Edges&nbsp; | Temporal | Annotated &nbsp; |
| ------------------------------------------------------------------------------------------------------ | ----------------------------------------- | ----------- | -------- | ---------------- |
| [Zachary karate club](http://konect.cc/networks/ucidata-zachary/)                                      | 34                                        | 78          | N        | Y                |
| [Football](https://networkrepository.com/misc-football.php)                                            | 115                                       | 613         | N        | N                |
| [Star Wars Social](https://www.kaggle.com/ruchi798/star-wars)                                          | 113                                       | 1599        | Y        | N                |
| [Enron](https://www.kaggle.com/wcukierski/enron-email-dataset/)                                        | 605K                                      | 4.1M        | Y        | Y                |
| [IMDB 5000](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset)                               | 16K                                       | 52K         | Y        | N                |
| [DBLP-HCN](https://data.mendeley.com/datasets/t4xmpbrr6v/1)                                            | 11K                                       | 16K         | Y        | Y                |
| [DBLP-V1](https://www.aminer.org/citation)                                                             | 1.2M                                      | 2.4M        | Y        | Y                |
| [DBLP-V3](https://www.aminer.org/citation)                                                             | 2.7M                                      | 8.2M        | Y        | Y                |
| [sx-mathoverflow](https://snap.stanford.edu/data/sx-mathoverflow.html)                                 | 24K                                       | 506K        | Y        | N                |
| [sx-superuser](https://snap.stanford.edu/data/sx-superuser.html)                                       | 194K                                      | 1.4M        | Y        | N                |
| [Eu-core network](https://snap.stanford.edu/data/email-Eu-core.html)                                   | 1005                                      | 25K         | N        | Y                |
| [com-Youtube](https://snap.stanford.edu/data/com-Youtube.html)                                         | 1.1M                                      | 298K        | N        | Y                |
| [116th House of Representatives](https://www.kaggle.com/aavigan/house-of-representatives-congress-116) | 6249                                      | 12K         | N        | N                |
| [social-distancing-student]()                                                                          | 93K                                       | 3.7M        | Y        | N                |

Table: Overview of the datasets used for evaluation. All the datasets will be used for quality measure as well as auxiliary task based evaluation. Annotated column indicates whether a dataset is eligible for annotation-based evaluation, ie. it contains ground truth communities. {#tbl:benchmarkdatasets}



### Baselines

% * Static Representation:
%   * @rozemberczkiGEMSECGraphEmbedding2019
%   * @cavallariLearningCommunityEmbedding2017
%   * @jiaCommunityGANCommunityDetection2019
% * Dynamic link-based:
%   * @rossettiANGELEfficientEffective2020
% * Dynamic representation:
%   * @wangEvolutionaryAutoencoderDynamic2020
%   * @maCommunityawareDynamicNetwork2020

To give a fair representation of the state-of-the-art the following methods are selected as baselines. The selection is based on the category of communities they learn, diversification of techniques, and competitiveness with the ideas introduced as part of our framework.

#### Static Community Detection

For @rqq:rq1 we will evaluate our algorithm against baselines in @tbl:baselinescd. We use both static as well as dynamic algorithms as baselines to identify the benefit of dynamic community detection over static communities.

| Reference                                | Dynamic | Method               |
| ---------------------------------------- | ------- | -------------------- |
| @heFastAlgorithmCommunity2015            | N       | Link-based           |
| @blondelFastUnfoldingCommunities2008     | N       | Link-based           |
| @rozemberczkiGEMSECGraphEmbedding2019    | N       | Representation-based |
| @cavallariLearningCommunityEmbedding2017 | N       | Representation-based |
| @jiaCommunityGANCommunityDetection2019   | N       | Representation-based |
| @rossettiANGELEfficientEffective2020     | Y       | Link-based           |
| @wangDynamicCommunityDetection2017       | Y       | Link-based           |
| @greeneTrackingEvolutionCommunities2010  | Y       | Link-based           |
| @wangEvolutionaryAutoencoderDynamic2020  | Y       | Representation-based |
| @maCommunityawareDynamicNetwork2020      | Y       | Representation-based |

Table: List of community detection methods we will use as baselines. Dynamic column indicates whether the algorithm is capable of detecting dynamic communities, and method indicates whether it is a link-based or representation-based algorithm. {#tbl:baselinescd}

#### Dynamic Representation Learning

To answer @rqq:rq3 we will evaluate the algorithm against other dynamic representation learning algorithms to verify that learned node embeddings are still usable for node level predictions tasks as well as community level tasks. 

* @nguyenContinuousTimeDynamicNetwork2018
* @wuSageDyNovelSampling2021
* @parejaEvolveGCNEvolvingGraph2020
  
  

## Extensions

% If the timing permits:
% 
% * Explore event detection possibilities in the results

If the timing of the research project permits, while not the main focus of the research, additional extensions of the algorithms for future work may be explored. These extensions may encompass exploring evolutional event detection within the extracted dynamic communities.

# Planning

% * Split planning (20 weeks) into sprints (2 weeks)
% * Deadline: June 13th, 2022

In this section, the approximate planning for my thesis is discussed. The research phase of the thesis is broken down into two-week blocks. Below a timeline of these sprints can be found along with goals expected to achieve by the end of the block. The final deadline for the thesis is June 13th, 2022.

## Timeline

### January 24 - 2 weeks

% * Implement the baselines
% * Set up experimentation setup and benchmarks

Preparing the datasets, and setting up benchmarks for the baseline methods. Implementation of baseline if the implementation is not publicly available.

### February 5 - 2 weeks

% * Finishing up benchmark implementation
% * Conducting an evaluation of existing baselines on the set RQ benchmarks

Finishing up the benchmark implementation. Conducting the evaluation and collection of the results on the baselines for the RQ's.

### February 19 - 2 weeks

% * Setting up graph sampling pipeline a dataset
% * Experimentation with representation based approaches (performance comparison)

Setting up graph sampling pipeline and experimentation with various representation-based approaches. Collecting performance results for ablation tests.

### March 3 - 2 weeks

% * Experimenting and implementing incorporation of community-based objectives into the framework
% * Conducting experiments for answering RQ3

Experimenting and implementing incorporation of community-based objectives into the framework. Conducting experiments with temporal modeling for @rqq:rq3.

### March 19 - 2 weeks

% * Finishing up Experimenting and implementing incorporation of community-based objectives
% * Experimenting with community extraction and subsampling

Finishing up experimenting and implementing incorporation of community-based objectives into the framework. Experimenting with community extraction and sub-sampling.

### April 2 - 2 weeks

% * Evaluation and tuning of the algorithm parameters for RQ1, and RQ2

Evaluation and tuning of the algorithm parameters to collect results for @rqq:rq1 and @rqq:rq4

### April 10 - 2 weeks

% * Evaluation and optimization of the algorithm for scalability,
% * Testing of the algorithm against RQ3

Evaluation and optimization of the algorithm for scalability. Collecting data for @rqq:rq2.

### April 30 - 2 weeks

% * Describing the full methodology within the thesis

Writing thesis: describing the full methodology

### May 14 - 2 weeks

% * Describing the evaluation and research results within the thesis

Writing thesis: describing evaluation and summarizing the results

### May 28 - 2 weeks

% * Complete the thesis
% * Finishing touches
% * Ensure everything is ready for delivery
% * Including publishing of results

Completing the thesis and ensuring everything is ready for delivery.
