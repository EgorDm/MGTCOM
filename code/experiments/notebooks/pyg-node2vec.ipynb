{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from shared.schema import DatasetSchema, GraphSchema\n",
    "from shared.graph.loading import pd_from_entity_schema"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DATASET = DatasetSchema.load_schema('star-wars')\n",
    "schema = GraphSchema.from_dataset(DATASET)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "explicit_label = False\n",
    "explicit_timestamp = True\n",
    "unix_timestamp = True\n",
    "prefix_id = None\n",
    "include_properties = lambda cs: [c for c in cs if c.startswith('feat_') or c == 'name']\n",
    "\n",
    "nodes_dfs = {\n",
    "    label: pd_from_entity_schema(\n",
    "        entity_schema,\n",
    "        explicit_label=explicit_label,\n",
    "        explicit_timestamp=explicit_timestamp,\n",
    "        include_properties=include_properties,\n",
    "        unix_timestamp=unix_timestamp,\n",
    "        prefix_id=prefix_id,\n",
    "    ).set_index('id').drop(columns=['type']).sort_index()\n",
    "    for label, entity_schema in schema.nodes.items()\n",
    "}\n",
    "node_mappings_dfs = {\n",
    "    label: pd.Series(range(len(df)), index=df.index, name='nid')\n",
    "    for label, df in nodes_dfs.items()\n",
    "}\n",
    "\n",
    "edges_dfs = {\n",
    "    label: pd_from_entity_schema(\n",
    "        entity_schema,\n",
    "        explicit_label=explicit_label,\n",
    "        explicit_timestamp=explicit_timestamp,\n",
    "        include_properties=include_properties,\n",
    "        unix_timestamp=unix_timestamp,\n",
    "        prefix_id=prefix_id,\n",
    "    )\n",
    "        .reset_index()\n",
    "        .drop(columns=['type'])\n",
    "        .drop_duplicates(subset=['src', 'dst', 'timestamp'])\n",
    "        .join(node_mappings_dfs[entity_schema.source_type], on='src')\n",
    "        .drop(columns=['src'])\n",
    "        .rename(columns={'nid': 'src'})\n",
    "        .join(node_mappings_dfs[entity_schema.target_type], on='dst')\n",
    "        .drop(columns=['dst'])\n",
    "        .rename(columns={'nid': 'dst'})\n",
    "    for label, entity_schema in schema.edges.items()\n",
    "}\n",
    "\n",
    "cursor = 0\n",
    "for df in edges_dfs.values():\n",
    "    df.index += cursor\n",
    "    cursor += len(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import HeteroData, Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "for ntype, ndf in nodes_dfs.items():\n",
    "    columns = [c for c in ndf.columns if c.startswith('feat_')]\n",
    "    data[ntype].x = torch.tensor(ndf[columns].values.astype(np.float32))\n",
    "    if 'timestamp' in ndf.columns:\n",
    "        data[ntype].timestamp = torch.tensor(ndf['timestamp'].values.astype(np.int32))\n",
    "\n",
    "for etype, edf in edges_dfs.items():\n",
    "    columns = [c for c in edf.columns if c.startswith('feat_')]\n",
    "    edge_schema = schema.edges[etype]\n",
    "    edge_type = (edge_schema.source_type, edge_schema.get_type(), edge_schema.target_type)\n",
    "    data[edge_type].edge_attr = torch.tensor(edf[columns].values.astype(np.float32))\n",
    "    data[edge_type].edge_index = torch.tensor(edf[['src', 'dst']].T.values.astype(np.int64))\n",
    "    if 'timestamp' in edf.columns:\n",
    "        data[edge_type].timestamp = torch.tensor(edf['timestamp'].values.astype(np.int32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Character', 'INTERACTIONS', 'Character'),\n ('Character', 'MENTIONS', 'Character')]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.edge_index_dict.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[24,  0, 25,  ..., 96, 37, 91],\n        [45, 28, 77,  ..., 79, 74, 29]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index_dict[('Character', 'INTERACTIONS', 'Character')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 958])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index_dict[('Character', 'INTERACTIONS', 'Character')].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "metapath = [\n",
    "    ('Character', 'INTERACTIONS', 'Character'),\n",
    "    ('Character', 'MENTIONS', 'Character')\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from ml.data import LinkSplitter\n",
    "\n",
    "transform = LinkSplitter(\n",
    "    num_val=0.3,\n",
    "    num_test=0.0,\n",
    "    edge_types=metapath,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = transform(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001B[1mCharacter\u001B[0m={ x=[113, 32] },\n",
      "  \u001B[1m(Character, INTERACTIONS, Character)\u001B[0m={\n",
      "    edge_attr=[671, 0],\n",
      "    edge_index=[2, 671],\n",
      "    timestamp=[671]\n",
      "  },\n",
      "  \u001B[1m(Character, MENTIONS, Character)\u001B[0m={\n",
      "    edge_attr=[784, 0],\n",
      "    edge_index=[2, 784],\n",
      "    timestamp=[784]\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001B[1mCharacter\u001B[0m={ x=[113, 32] },\n",
      "  \u001B[1m(Character, INTERACTIONS, Character)\u001B[0m={\n",
      "    edge_attr=[958, 0],\n",
      "    edge_index=[2, 958],\n",
      "    timestamp=[958]\n",
      "  },\n",
      "  \u001B[1m(Character, MENTIONS, Character)\u001B[0m={\n",
      "    edge_attr=[1120, 0],\n",
      "    edge_index=[2, 1120],\n",
      "    timestamp=[1120]\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(val_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MetaPath2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "model = MetaPath2Vec(\n",
    "    train_data.edge_index_dict,\n",
    "    embedding_dim=32,\n",
    "    metapath=metapath,\n",
    "    walk_length=5,\n",
    "    context_size=3,\n",
    "    walks_per_node=5,\n",
    "    num_negative_samples=5,\n",
    "    sparse=True\n",
    ").to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "model_val = MetaPath2Vec(\n",
    "    val_data.edge_index_dict,\n",
    "    embedding_dim=32,\n",
    "    metapath=metapath,\n",
    "    walk_length=5,\n",
    "    context_size=3,\n",
    "    walks_per_node=5,\n",
    "    num_negative_samples=5,\n",
    "    sparse=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "loader = model.loader(batch_size=8, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "loader_val = model_val.loader(batch_size=8, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "1 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "2 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "3 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "4 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "5 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "6 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "7 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "8 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "9 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "10 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "11 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "12 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "13 torch.Size([160, 3]) torch.Size([800, 3])\n",
      "14 torch.Size([20, 3]) torch.Size([100, 3])\n"
     ]
    }
   ],
   "source": [
    "for idx, (pos_rw, neg_rw) in enumerate(loader):\n",
    "    print(idx, pos_rw.shape, neg_rw.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 14,  27, 113]) tensor([15, 78, 43])\n"
     ]
    }
   ],
   "source": [
    "print(pos_rw[5], neg_rw[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 00010/15, Loss: 4.6709\n",
      "Val Loss: 4.5841\n",
      "Epoch: 0\n",
      "Epoch: 1, Step: 00010/15, Loss: 4.3992\n",
      "Val Loss: 4.5623\n",
      "Epoch: 1\n",
      "Epoch: 2, Step: 00010/15, Loss: 4.4315\n",
      "Val Loss: 4.5363\n",
      "Epoch: 2\n",
      "Epoch: 3, Step: 00010/15, Loss: 4.3543\n",
      "Val Loss: 4.4628\n",
      "Epoch: 3\n",
      "Epoch: 4, Step: 00010/15, Loss: 4.4213\n",
      "Val Loss: 4.2370\n",
      "Epoch: 4\n",
      "Epoch: 5, Step: 00010/15, Loss: 4.3693\n",
      "Val Loss: 4.2387\n",
      "Epoch: 5\n",
      "Epoch: 6, Step: 00010/15, Loss: 4.2699\n",
      "Val Loss: 4.1733\n",
      "Epoch: 6\n",
      "Epoch: 7, Step: 00010/15, Loss: 4.1207\n",
      "Val Loss: 4.2241\n",
      "Epoch: 7\n",
      "Epoch: 8, Step: 00010/15, Loss: 4.0273\n",
      "Val Loss: 4.0768\n",
      "Epoch: 8\n",
      "Epoch: 9, Step: 00010/15, Loss: 4.0696\n",
      "Val Loss: 4.0095\n",
      "Epoch: 9\n",
      "Epoch: 10, Step: 00010/15, Loss: 3.9247\n",
      "Val Loss: 3.9732\n",
      "Epoch: 10\n",
      "Epoch: 11, Step: 00010/15, Loss: 3.9155\n",
      "Val Loss: 3.9895\n",
      "Epoch: 11\n",
      "Epoch: 12, Step: 00010/15, Loss: 3.9311\n",
      "Val Loss: 3.9387\n",
      "Epoch: 12\n",
      "Epoch: 13, Step: 00010/15, Loss: 3.7940\n",
      "Val Loss: 3.8636\n",
      "Epoch: 13\n",
      "Epoch: 14, Step: 00010/15, Loss: 3.7046\n",
      "Val Loss: 3.8229\n",
      "Epoch: 14\n",
      "Epoch: 15, Step: 00010/15, Loss: 3.7402\n",
      "Val Loss: 3.7290\n",
      "Epoch: 15\n",
      "Epoch: 16, Step: 00010/15, Loss: 3.7225\n",
      "Val Loss: 3.6530\n",
      "Epoch: 16\n",
      "Epoch: 17, Step: 00010/15, Loss: 3.6496\n",
      "Val Loss: 3.6830\n",
      "Epoch: 17\n",
      "Epoch: 18, Step: 00010/15, Loss: 3.4772\n",
      "Val Loss: 3.6558\n",
      "Epoch: 18\n",
      "Epoch: 19, Step: 00010/15, Loss: 3.5503\n",
      "Val Loss: 3.6077\n",
      "Epoch: 19\n",
      "Epoch: 20, Step: 00010/15, Loss: 3.6088\n",
      "Val Loss: 3.6390\n",
      "Epoch: 20\n",
      "Epoch: 21, Step: 00010/15, Loss: 3.3974\n",
      "Val Loss: 3.6095\n",
      "Epoch: 21\n",
      "Epoch: 22, Step: 00010/15, Loss: 3.4261\n",
      "Val Loss: 3.5259\n",
      "Epoch: 22\n",
      "Epoch: 23, Step: 00010/15, Loss: 3.3511\n",
      "Val Loss: 3.3755\n",
      "Epoch: 23\n",
      "Epoch: 24, Step: 00010/15, Loss: 3.3144\n",
      "Val Loss: 3.3940\n",
      "Epoch: 24\n",
      "Epoch: 25, Step: 00010/15, Loss: 3.3469\n",
      "Val Loss: 3.3212\n",
      "Epoch: 25\n",
      "Epoch: 26, Step: 00010/15, Loss: 3.2311\n",
      "Val Loss: 3.3519\n",
      "Epoch: 26\n",
      "Epoch: 27, Step: 00010/15, Loss: 3.2117\n",
      "Val Loss: 3.3176\n",
      "Epoch: 27\n",
      "Epoch: 28, Step: 00010/15, Loss: 3.1449\n",
      "Val Loss: 3.2658\n",
      "Epoch: 28\n",
      "Epoch: 29, Step: 00010/15, Loss: 2.9712\n",
      "Val Loss: 3.1739\n",
      "Epoch: 29\n",
      "Epoch: 30, Step: 00010/15, Loss: 3.1040\n",
      "Val Loss: 3.2106\n",
      "Epoch: 30\n",
      "Epoch: 31, Step: 00010/15, Loss: 3.1198\n",
      "Val Loss: 3.1952\n",
      "Epoch: 31\n",
      "Epoch: 32, Step: 00010/15, Loss: 3.0340\n",
      "Val Loss: 3.0867\n",
      "Epoch: 32\n",
      "Epoch: 33, Step: 00010/15, Loss: 3.0386\n",
      "Val Loss: 3.1168\n",
      "Epoch: 33\n",
      "Epoch: 34, Step: 00010/15, Loss: 2.9314\n",
      "Val Loss: 3.0572\n",
      "Epoch: 34\n",
      "Epoch: 35, Step: 00010/15, Loss: 2.8820\n",
      "Val Loss: 3.0159\n",
      "Epoch: 35\n",
      "Epoch: 36, Step: 00010/15, Loss: 2.9238\n",
      "Val Loss: 3.0349\n",
      "Epoch: 36\n",
      "Epoch: 37, Step: 00010/15, Loss: 2.8857\n",
      "Val Loss: 2.9618\n",
      "Epoch: 37\n",
      "Epoch: 38, Step: 00010/15, Loss: 2.8345\n",
      "Val Loss: 2.9496\n",
      "Epoch: 38\n",
      "Epoch: 39, Step: 00010/15, Loss: 2.7679\n",
      "Val Loss: 2.8087\n",
      "Epoch: 39\n",
      "Epoch: 40, Step: 00010/15, Loss: 2.7188\n",
      "Val Loss: 2.9264\n",
      "Epoch: 40\n",
      "Epoch: 41, Step: 00010/15, Loss: 2.8024\n",
      "Val Loss: 2.8851\n",
      "Epoch: 41\n",
      "Epoch: 42, Step: 00010/15, Loss: 2.6698\n",
      "Val Loss: 2.8997\n",
      "Epoch: 42\n",
      "Epoch: 43, Step: 00010/15, Loss: 2.6235\n",
      "Val Loss: 2.8536\n",
      "Epoch: 43\n",
      "Epoch: 44, Step: 00010/15, Loss: 2.6777\n",
      "Val Loss: 2.7674\n",
      "Epoch: 44\n",
      "Epoch: 45, Step: 00010/15, Loss: 2.6230\n",
      "Val Loss: 2.7916\n",
      "Epoch: 45\n",
      "Epoch: 46, Step: 00010/15, Loss: 2.5622\n",
      "Val Loss: 2.7113\n",
      "Epoch: 46\n",
      "Epoch: 47, Step: 00010/15, Loss: 2.5942\n",
      "Val Loss: 2.7442\n",
      "Epoch: 47\n",
      "Epoch: 48, Step: 00010/15, Loss: 2.4977\n",
      "Val Loss: 2.6404\n",
      "Epoch: 48\n",
      "Epoch: 49, Step: 00010/15, Loss: 2.5154\n",
      "Val Loss: 2.5971\n",
      "Epoch: 49\n",
      "Epoch: 50, Step: 00010/15, Loss: 2.5804\n",
      "Val Loss: 2.6284\n",
      "Epoch: 50\n",
      "Epoch: 51, Step: 00010/15, Loss: 2.6171\n",
      "Val Loss: 2.6786\n",
      "Epoch: 51\n",
      "Epoch: 52, Step: 00010/15, Loss: 2.5147\n",
      "Val Loss: 2.5473\n",
      "Epoch: 52\n",
      "Epoch: 53, Step: 00010/15, Loss: 2.4328\n",
      "Val Loss: 2.4911\n",
      "Epoch: 53\n",
      "Epoch: 54, Step: 00010/15, Loss: 2.4024\n",
      "Val Loss: 2.5521\n",
      "Epoch: 54\n",
      "Epoch: 55, Step: 00010/15, Loss: 2.4570\n",
      "Val Loss: 2.4570\n",
      "Epoch: 55\n",
      "Epoch: 56, Step: 00010/15, Loss: 2.4147\n",
      "Val Loss: 2.5270\n",
      "Epoch: 56\n",
      "Epoch: 57, Step: 00010/15, Loss: 2.3110\n",
      "Val Loss: 2.4782\n",
      "Epoch: 57\n",
      "Epoch: 58, Step: 00010/15, Loss: 2.3334\n",
      "Val Loss: 2.5018\n",
      "Epoch: 58\n",
      "Epoch: 59, Step: 00010/15, Loss: 2.3375\n",
      "Val Loss: 2.4047\n",
      "Epoch: 59\n",
      "Epoch: 60, Step: 00010/15, Loss: 2.2819\n",
      "Val Loss: 2.4867\n",
      "Epoch: 60\n",
      "Epoch: 61, Step: 00010/15, Loss: 2.2809\n",
      "Val Loss: 2.4370\n",
      "Epoch: 61\n",
      "Epoch: 62, Step: 00010/15, Loss: 2.2612\n",
      "Val Loss: 2.4478\n",
      "Epoch: 62\n",
      "Epoch: 63, Step: 00010/15, Loss: 2.1835\n",
      "Val Loss: 2.3412\n",
      "Epoch: 63\n",
      "Epoch: 64, Step: 00010/15, Loss: 2.1513\n",
      "Val Loss: 2.3819\n",
      "Epoch: 64\n",
      "Epoch: 65, Step: 00010/15, Loss: 2.2441\n",
      "Val Loss: 2.3718\n",
      "Epoch: 65\n",
      "Epoch: 66, Step: 00010/15, Loss: 2.2095\n",
      "Val Loss: 2.3051\n",
      "Epoch: 66\n",
      "Epoch: 67, Step: 00010/15, Loss: 2.1871\n",
      "Val Loss: 2.3216\n",
      "Epoch: 67\n",
      "Epoch: 68, Step: 00010/15, Loss: 2.2283\n",
      "Val Loss: 2.3096\n",
      "Epoch: 68\n",
      "Epoch: 69, Step: 00010/15, Loss: 2.1576\n",
      "Val Loss: 2.3383\n",
      "Epoch: 69\n",
      "Epoch: 70, Step: 00010/15, Loss: 2.1018\n",
      "Val Loss: 2.2322\n",
      "Epoch: 70\n",
      "Epoch: 71, Step: 00010/15, Loss: 2.0620\n",
      "Val Loss: 2.2622\n",
      "Epoch: 71\n",
      "Epoch: 72, Step: 00010/15, Loss: 2.1118\n",
      "Val Loss: 2.2483\n",
      "Epoch: 72\n",
      "Epoch: 73, Step: 00010/15, Loss: 2.1054\n",
      "Val Loss: 2.1901\n",
      "Epoch: 73\n",
      "Epoch: 74, Step: 00010/15, Loss: 2.0902\n",
      "Val Loss: 2.1411\n",
      "Epoch: 74\n",
      "Epoch: 75, Step: 00010/15, Loss: 2.0375\n",
      "Val Loss: 2.1801\n",
      "Epoch: 75\n",
      "Epoch: 76, Step: 00010/15, Loss: 2.0030\n",
      "Val Loss: 2.1979\n",
      "Epoch: 76\n",
      "Epoch: 77, Step: 00010/15, Loss: 2.0217\n",
      "Val Loss: 2.1414\n",
      "Epoch: 77\n",
      "Epoch: 78, Step: 00010/15, Loss: 1.9941\n",
      "Val Loss: 2.1297\n",
      "Epoch: 78\n",
      "Epoch: 79, Step: 00010/15, Loss: 1.9811\n",
      "Val Loss: 2.1310\n",
      "Epoch: 79\n",
      "Epoch: 80, Step: 00010/15, Loss: 1.9673\n",
      "Val Loss: 2.1101\n",
      "Epoch: 80\n",
      "Epoch: 81, Step: 00010/15, Loss: 1.9712\n",
      "Val Loss: 2.1070\n",
      "Epoch: 81\n",
      "Epoch: 82, Step: 00010/15, Loss: 1.9694\n",
      "Val Loss: 2.0770\n",
      "Epoch: 82\n",
      "Epoch: 83, Step: 00010/15, Loss: 1.9488\n",
      "Val Loss: 2.0683\n",
      "Epoch: 83\n",
      "Epoch: 84, Step: 00010/15, Loss: 1.9273\n",
      "Val Loss: 2.0161\n",
      "Epoch: 84\n",
      "Epoch: 85, Step: 00010/15, Loss: 1.8999\n",
      "Val Loss: 2.0317\n",
      "Epoch: 85\n",
      "Epoch: 86, Step: 00010/15, Loss: 1.9603\n",
      "Val Loss: 2.0178\n",
      "Epoch: 86\n",
      "Epoch: 87, Step: 00010/15, Loss: 1.8778\n",
      "Val Loss: 2.0014\n",
      "Epoch: 87\n",
      "Epoch: 88, Step: 00010/15, Loss: 1.9190\n",
      "Val Loss: 2.0410\n",
      "Epoch: 88\n",
      "Epoch: 89, Step: 00010/15, Loss: 1.8159\n",
      "Val Loss: 1.9306\n",
      "Epoch: 89\n",
      "Epoch: 90, Step: 00010/15, Loss: 1.8374\n",
      "Val Loss: 1.9865\n",
      "Epoch: 90\n",
      "Epoch: 91, Step: 00010/15, Loss: 1.8273\n",
      "Val Loss: 1.9932\n",
      "Epoch: 91\n",
      "Epoch: 92, Step: 00010/15, Loss: 1.7772\n",
      "Val Loss: 1.9452\n",
      "Epoch: 92\n",
      "Epoch: 93, Step: 00010/15, Loss: 1.8868\n",
      "Val Loss: 1.9174\n",
      "Epoch: 93\n",
      "Epoch: 94, Step: 00010/15, Loss: 1.7923\n",
      "Val Loss: 1.9542\n",
      "Epoch: 94\n",
      "Epoch: 95, Step: 00010/15, Loss: 1.7387\n",
      "Val Loss: 1.9133\n",
      "Epoch: 95\n",
      "Epoch: 96, Step: 00010/15, Loss: 1.7934\n",
      "Val Loss: 1.8675\n",
      "Epoch: 96\n",
      "Epoch: 97, Step: 00010/15, Loss: 1.7313\n",
      "Val Loss: 1.9181\n",
      "Epoch: 97\n",
      "Epoch: 98, Step: 00010/15, Loss: 1.8388\n",
      "Val Loss: 1.9199\n",
      "Epoch: 98\n",
      "Epoch: 99, Step: 00010/15, Loss: 1.7686\n",
      "Val Loss: 1.8233\n",
      "Epoch: 99\n",
      "Epoch: 100, Step: 00010/15, Loss: 1.7940\n",
      "Val Loss: 1.8160\n",
      "Epoch: 100\n",
      "Epoch: 101, Step: 00010/15, Loss: 1.7837\n",
      "Val Loss: 1.8242\n",
      "Epoch: 101\n",
      "Epoch: 102, Step: 00010/15, Loss: 1.7447\n",
      "Val Loss: 1.8128\n",
      "Epoch: 102\n",
      "Epoch: 103, Step: 00010/15, Loss: 1.7140\n",
      "Val Loss: 1.8793\n",
      "Epoch: 103\n",
      "Epoch: 104, Step: 00010/15, Loss: 1.6898\n",
      "Val Loss: 1.8099\n",
      "Epoch: 104\n",
      "Epoch: 105, Step: 00010/15, Loss: 1.7142\n",
      "Val Loss: 1.8163\n",
      "Epoch: 105\n",
      "Epoch: 106, Step: 00010/15, Loss: 1.7211\n",
      "Val Loss: 1.7582\n",
      "Epoch: 106\n",
      "Epoch: 107, Step: 00010/15, Loss: 1.6856\n",
      "Val Loss: 1.8169\n",
      "Epoch: 107\n",
      "Epoch: 108, Step: 00010/15, Loss: 1.7051\n",
      "Val Loss: 1.7881\n",
      "Epoch: 108\n",
      "Epoch: 109, Step: 00010/15, Loss: 1.6986\n",
      "Val Loss: 1.7749\n",
      "Epoch: 109\n",
      "Epoch: 110, Step: 00010/15, Loss: 1.6133\n",
      "Val Loss: 1.7234\n",
      "Epoch: 110\n",
      "Epoch: 111, Step: 00010/15, Loss: 1.6828\n",
      "Val Loss: 1.7023\n",
      "Epoch: 111\n",
      "Epoch: 112, Step: 00010/15, Loss: 1.6195\n",
      "Val Loss: 1.7350\n",
      "Epoch: 112\n",
      "Epoch: 113, Step: 00010/15, Loss: 1.6640\n",
      "Val Loss: 1.7700\n",
      "Epoch: 113\n",
      "Epoch: 114, Step: 00010/15, Loss: 1.6079\n",
      "Val Loss: 1.7243\n",
      "Epoch: 114\n",
      "Epoch: 115, Step: 00010/15, Loss: 1.6519\n",
      "Val Loss: 1.6903\n",
      "Epoch: 115\n",
      "Epoch: 116, Step: 00010/15, Loss: 1.5765\n",
      "Val Loss: 1.7209\n",
      "Epoch: 116\n",
      "Epoch: 117, Step: 00010/15, Loss: 1.6077\n",
      "Val Loss: 1.7011\n",
      "Epoch: 117\n",
      "Epoch: 118, Step: 00010/15, Loss: 1.6132\n",
      "Val Loss: 1.6598\n",
      "Epoch: 118\n",
      "Epoch: 119, Step: 00010/15, Loss: 1.5677\n",
      "Val Loss: 1.6907\n",
      "Epoch: 119\n",
      "Epoch: 120, Step: 00010/15, Loss: 1.5726\n",
      "Val Loss: 1.6787\n",
      "Epoch: 120\n",
      "Epoch: 121, Step: 00010/15, Loss: 1.5727\n",
      "Val Loss: 1.6875\n",
      "Epoch: 121\n",
      "Epoch: 122, Step: 00010/15, Loss: 1.5247\n",
      "Val Loss: 1.6388\n",
      "Epoch: 122\n",
      "Epoch: 123, Step: 00010/15, Loss: 1.5851\n",
      "Val Loss: 1.6340\n",
      "Epoch: 123\n",
      "Epoch: 124, Step: 00010/15, Loss: 1.5692\n",
      "Val Loss: 1.6419\n",
      "Epoch: 124\n",
      "Epoch: 125, Step: 00010/15, Loss: 1.5152\n",
      "Val Loss: 1.6360\n",
      "Epoch: 125\n",
      "Epoch: 126, Step: 00010/15, Loss: 1.5324\n",
      "Val Loss: 1.6400\n",
      "Epoch: 126\n",
      "Epoch: 127, Step: 00010/15, Loss: 1.5310\n",
      "Val Loss: 1.5687\n",
      "Epoch: 127\n",
      "Epoch: 128, Step: 00010/15, Loss: 1.4943\n",
      "Val Loss: 1.6511\n",
      "Epoch: 128\n",
      "Epoch: 129, Step: 00010/15, Loss: 1.4860\n",
      "Val Loss: 1.6278\n",
      "Epoch: 129\n",
      "Epoch: 130, Step: 00010/15, Loss: 1.5056\n",
      "Val Loss: 1.5358\n",
      "Epoch: 130\n",
      "Epoch: 131, Step: 00010/15, Loss: 1.4599\n",
      "Val Loss: 1.5866\n",
      "Epoch: 131\n",
      "Epoch: 132, Step: 00010/15, Loss: 1.4639\n",
      "Val Loss: 1.6000\n",
      "Epoch: 132\n",
      "Epoch: 133, Step: 00010/15, Loss: 1.4753\n",
      "Val Loss: 1.5812\n",
      "Epoch: 133\n",
      "Epoch: 134, Step: 00010/15, Loss: 1.4754\n",
      "Val Loss: 1.5488\n",
      "Epoch: 134\n",
      "Epoch: 135, Step: 00010/15, Loss: 1.4802\n",
      "Val Loss: 1.5752\n",
      "Epoch: 135\n",
      "Epoch: 136, Step: 00010/15, Loss: 1.4488\n",
      "Val Loss: 1.5866\n",
      "Epoch: 136\n",
      "Epoch: 137, Step: 00010/15, Loss: 1.4250\n",
      "Val Loss: 1.4947\n",
      "Epoch: 137\n",
      "Epoch: 138, Step: 00010/15, Loss: 1.4587\n",
      "Val Loss: 1.5469\n",
      "Epoch: 138\n",
      "Epoch: 139, Step: 00010/15, Loss: 1.4723\n",
      "Val Loss: 1.5546\n",
      "Epoch: 139\n",
      "Epoch: 140, Step: 00010/15, Loss: 1.4360\n",
      "Val Loss: 1.4932\n",
      "Epoch: 140\n",
      "Epoch: 141, Step: 00010/15, Loss: 1.4648\n",
      "Val Loss: 1.5019\n",
      "Epoch: 141\n",
      "Epoch: 142, Step: 00010/15, Loss: 1.4184\n",
      "Val Loss: 1.5154\n",
      "Epoch: 142\n",
      "Epoch: 143, Step: 00010/15, Loss: 1.3682\n",
      "Val Loss: 1.5151\n",
      "Epoch: 143\n",
      "Epoch: 144, Step: 00010/15, Loss: 1.3992\n",
      "Val Loss: 1.5084\n",
      "Epoch: 144\n",
      "Epoch: 145, Step: 00010/15, Loss: 1.4162\n",
      "Val Loss: 1.4940\n",
      "Epoch: 145\n",
      "Epoch: 146, Step: 00010/15, Loss: 1.3759\n",
      "Val Loss: 1.4468\n",
      "Epoch: 146\n",
      "Epoch: 147, Step: 00010/15, Loss: 1.3756\n",
      "Val Loss: 1.5269\n",
      "Epoch: 147\n",
      "Epoch: 148, Step: 00010/15, Loss: 1.3847\n",
      "Val Loss: 1.5031\n",
      "Epoch: 148\n",
      "Epoch: 149, Step: 00010/15, Loss: 1.4049\n",
      "Val Loss: 1.4450\n",
      "Epoch: 149\n",
      "Epoch: 150, Step: 00010/15, Loss: 1.3542\n",
      "Val Loss: 1.4823\n",
      "Epoch: 150\n",
      "Epoch: 151, Step: 00010/15, Loss: 1.3926\n",
      "Val Loss: 1.4696\n",
      "Epoch: 151\n",
      "Epoch: 152, Step: 00010/15, Loss: 1.3864\n",
      "Val Loss: 1.4404\n",
      "Epoch: 152\n",
      "Epoch: 153, Step: 00010/15, Loss: 1.3328\n",
      "Val Loss: 1.4262\n",
      "Epoch: 153\n",
      "Epoch: 154, Step: 00010/15, Loss: 1.3872\n",
      "Val Loss: 1.4604\n",
      "Epoch: 154\n",
      "Epoch: 155, Step: 00010/15, Loss: 1.3556\n",
      "Val Loss: 1.4162\n",
      "Epoch: 155\n",
      "Epoch: 156, Step: 00010/15, Loss: 1.3992\n",
      "Val Loss: 1.4569\n",
      "Epoch: 156\n",
      "Epoch: 157, Step: 00010/15, Loss: 1.3203\n",
      "Val Loss: 1.4762\n",
      "Epoch: 157\n",
      "Epoch: 158, Step: 00010/15, Loss: 1.3251\n",
      "Val Loss: 1.4376\n",
      "Epoch: 158\n",
      "Epoch: 159, Step: 00010/15, Loss: 1.3284\n",
      "Val Loss: 1.4115\n",
      "Epoch: 159\n",
      "Epoch: 160, Step: 00010/15, Loss: 1.3543\n",
      "Val Loss: 1.4294\n",
      "Epoch: 160\n",
      "Epoch: 161, Step: 00010/15, Loss: 1.3460\n",
      "Val Loss: 1.3914\n",
      "Epoch: 161\n",
      "Epoch: 162, Step: 00010/15, Loss: 1.3152\n",
      "Val Loss: 1.4103\n",
      "Epoch: 162\n",
      "Epoch: 163, Step: 00010/15, Loss: 1.3263\n",
      "Val Loss: 1.3965\n",
      "Epoch: 163\n",
      "Epoch: 164, Step: 00010/15, Loss: 1.2966\n",
      "Val Loss: 1.4060\n",
      "Epoch: 164\n",
      "Epoch: 165, Step: 00010/15, Loss: 1.3106\n",
      "Val Loss: 1.3598\n",
      "Epoch: 165\n",
      "Epoch: 166, Step: 00010/15, Loss: 1.3165\n",
      "Val Loss: 1.3666\n",
      "Epoch: 166\n",
      "Epoch: 167, Step: 00010/15, Loss: 1.2722\n",
      "Val Loss: 1.4152\n",
      "Epoch: 167\n",
      "Epoch: 168, Step: 00010/15, Loss: 1.3229\n",
      "Val Loss: 1.3736\n",
      "Epoch: 168\n",
      "Epoch: 169, Step: 00010/15, Loss: 1.2812\n",
      "Val Loss: 1.3543\n",
      "Epoch: 169\n",
      "Epoch: 170, Step: 00010/15, Loss: 1.2885\n",
      "Val Loss: 1.3634\n",
      "Epoch: 170\n",
      "Epoch: 171, Step: 00010/15, Loss: 1.2893\n",
      "Val Loss: 1.3605\n",
      "Epoch: 171\n",
      "Epoch: 172, Step: 00010/15, Loss: 1.2887\n",
      "Val Loss: 1.3268\n",
      "Epoch: 172\n",
      "Epoch: 173, Step: 00010/15, Loss: 1.2625\n",
      "Val Loss: 1.3626\n",
      "Epoch: 173\n",
      "Epoch: 174, Step: 00010/15, Loss: 1.2798\n",
      "Val Loss: 1.3637\n",
      "Epoch: 174\n",
      "Epoch: 175, Step: 00010/15, Loss: 1.2702\n",
      "Val Loss: 1.3178\n",
      "Epoch: 175\n",
      "Epoch: 176, Step: 00010/15, Loss: 1.2738\n",
      "Val Loss: 1.3563\n",
      "Epoch: 176\n",
      "Epoch: 177, Step: 00010/15, Loss: 1.2425\n",
      "Val Loss: 1.2976\n",
      "Epoch: 177\n",
      "Epoch: 178, Step: 00010/15, Loss: 1.2583\n",
      "Val Loss: 1.3068\n",
      "Epoch: 178\n",
      "Epoch: 179, Step: 00010/15, Loss: 1.2491\n",
      "Val Loss: 1.3467\n",
      "Epoch: 179\n",
      "Epoch: 180, Step: 00010/15, Loss: 1.2406\n",
      "Val Loss: 1.3443\n",
      "Epoch: 180\n",
      "Epoch: 181, Step: 00010/15, Loss: 1.2576\n",
      "Val Loss: 1.3016\n",
      "Epoch: 181\n",
      "Epoch: 182, Step: 00010/15, Loss: 1.2450\n",
      "Val Loss: 1.3161\n",
      "Epoch: 182\n",
      "Epoch: 183, Step: 00010/15, Loss: 1.2154\n",
      "Val Loss: 1.3455\n",
      "Epoch: 183\n",
      "Epoch: 184, Step: 00010/15, Loss: 1.2359\n",
      "Val Loss: 1.3078\n",
      "Epoch: 184\n",
      "Epoch: 185, Step: 00010/15, Loss: 1.2337\n",
      "Val Loss: 1.3121\n",
      "Epoch: 185\n",
      "Epoch: 186, Step: 00010/15, Loss: 1.2547\n",
      "Val Loss: 1.2732\n",
      "Epoch: 186\n",
      "Epoch: 187, Step: 00010/15, Loss: 1.2095\n",
      "Val Loss: 1.3585\n",
      "Epoch: 187\n",
      "Epoch: 188, Step: 00010/15, Loss: 1.2379\n",
      "Val Loss: 1.2955\n",
      "Epoch: 188\n",
      "Epoch: 189, Step: 00010/15, Loss: 1.1921\n",
      "Val Loss: 1.2979\n",
      "Epoch: 189\n",
      "Epoch: 190, Step: 00010/15, Loss: 1.2321\n",
      "Val Loss: 1.2686\n",
      "Epoch: 190\n",
      "Epoch: 191, Step: 00010/15, Loss: 1.2218\n",
      "Val Loss: 1.2819\n",
      "Epoch: 191\n",
      "Epoch: 192, Step: 00010/15, Loss: 1.2266\n",
      "Val Loss: 1.2979\n",
      "Epoch: 192\n",
      "Epoch: 193, Step: 00010/15, Loss: 1.2295\n",
      "Val Loss: 1.2740\n",
      "Epoch: 193\n",
      "Epoch: 194, Step: 00010/15, Loss: 1.2022\n",
      "Val Loss: 1.2536\n",
      "Epoch: 194\n",
      "Epoch: 195, Step: 00010/15, Loss: 1.2109\n",
      "Val Loss: 1.2473\n",
      "Epoch: 195\n",
      "Epoch: 196, Step: 00010/15, Loss: 1.1968\n",
      "Val Loss: 1.2921\n",
      "Epoch: 196\n",
      "Epoch: 197, Step: 00010/15, Loss: 1.1666\n",
      "Val Loss: 1.2711\n",
      "Epoch: 197\n",
      "Epoch: 198, Step: 00010/15, Loss: 1.1839\n",
      "Val Loss: 1.2783\n",
      "Epoch: 198\n",
      "Epoch: 199, Step: 00010/15, Loss: 1.1885\n",
      "Val Loss: 1.2515\n",
      "Epoch: 199\n",
      "Epoch: 200, Step: 00010/15, Loss: 1.1760\n",
      "Val Loss: 1.2162\n",
      "Epoch: 200\n",
      "Epoch: 201, Step: 00010/15, Loss: 1.1649\n",
      "Val Loss: 1.2640\n",
      "Epoch: 201\n",
      "Epoch: 202, Step: 00010/15, Loss: 1.1516\n",
      "Val Loss: 1.2695\n",
      "Epoch: 202\n",
      "Epoch: 203, Step: 00010/15, Loss: 1.1641\n",
      "Val Loss: 1.2371\n",
      "Epoch: 203\n",
      "Epoch: 204, Step: 00010/15, Loss: 1.1572\n",
      "Val Loss: 1.2509\n",
      "Epoch: 204\n",
      "Epoch: 205, Step: 00010/15, Loss: 1.1601\n",
      "Val Loss: 1.2233\n",
      "Epoch: 205\n",
      "Epoch: 206, Step: 00010/15, Loss: 1.1784\n",
      "Val Loss: 1.2404\n",
      "Epoch: 206\n",
      "Epoch: 207, Step: 00010/15, Loss: 1.1582\n",
      "Val Loss: 1.2045\n",
      "Epoch: 207\n",
      "Epoch: 208, Step: 00010/15, Loss: 1.1788\n",
      "Val Loss: 1.1989\n",
      "Epoch: 208\n",
      "Epoch: 209, Step: 00010/15, Loss: 1.1520\n",
      "Val Loss: 1.2229\n",
      "Epoch: 209\n",
      "Epoch: 210, Step: 00010/15, Loss: 1.1481\n",
      "Val Loss: 1.2067\n",
      "Epoch: 210\n",
      "Epoch: 211, Step: 00010/15, Loss: 1.1432\n",
      "Val Loss: 1.2293\n",
      "Epoch: 211\n",
      "Epoch: 212, Step: 00010/15, Loss: 1.1457\n",
      "Val Loss: 1.2289\n",
      "Epoch: 212\n",
      "Epoch: 213, Step: 00010/15, Loss: 1.1429\n",
      "Val Loss: 1.2313\n",
      "Epoch: 213\n",
      "Epoch: 214, Step: 00010/15, Loss: 1.1549\n",
      "Val Loss: 1.2139\n",
      "Epoch: 214\n",
      "Epoch: 215, Step: 00010/15, Loss: 1.1195\n",
      "Val Loss: 1.2184\n",
      "Epoch: 215\n",
      "Epoch: 216, Step: 00010/15, Loss: 1.1257\n",
      "Val Loss: 1.1943\n",
      "Epoch: 216\n",
      "Epoch: 217, Step: 00010/15, Loss: 1.1223\n",
      "Val Loss: 1.2055\n",
      "Epoch: 217\n",
      "Epoch: 218, Step: 00010/15, Loss: 1.1153\n",
      "Val Loss: 1.2118\n",
      "Epoch: 218\n",
      "Epoch: 219, Step: 00010/15, Loss: 1.1502\n",
      "Val Loss: 1.2074\n",
      "Epoch: 219\n",
      "Epoch: 220, Step: 00010/15, Loss: 1.1370\n",
      "Val Loss: 1.2056\n",
      "Epoch: 220\n",
      "Epoch: 221, Step: 00010/15, Loss: 1.1426\n",
      "Val Loss: 1.2185\n",
      "Epoch: 221\n",
      "Epoch: 222, Step: 00010/15, Loss: 1.1332\n",
      "Val Loss: 1.1818\n",
      "Epoch: 222\n",
      "Epoch: 223, Step: 00010/15, Loss: 1.1235\n",
      "Val Loss: 1.2106\n",
      "Epoch: 223\n",
      "Epoch: 224, Step: 00010/15, Loss: 1.1079\n",
      "Val Loss: 1.2060\n",
      "Epoch: 224\n",
      "Epoch: 225, Step: 00010/15, Loss: 1.1262\n",
      "Val Loss: 1.1961\n",
      "Epoch: 225\n",
      "Epoch: 226, Step: 00010/15, Loss: 1.1165\n",
      "Val Loss: 1.1698\n",
      "Epoch: 226\n",
      "Epoch: 227, Step: 00010/15, Loss: 1.0951\n",
      "Val Loss: 1.1840\n",
      "Epoch: 227\n",
      "Epoch: 228, Step: 00010/15, Loss: 1.0961\n",
      "Val Loss: 1.1959\n",
      "Epoch: 228\n",
      "Epoch: 229, Step: 00010/15, Loss: 1.1347\n",
      "Val Loss: 1.1783\n",
      "Epoch: 229\n",
      "Epoch: 230, Step: 00010/15, Loss: 1.0799\n",
      "Val Loss: 1.1579\n",
      "Epoch: 230\n",
      "Epoch: 231, Step: 00010/15, Loss: 1.1150\n",
      "Val Loss: 1.2064\n",
      "Epoch: 231\n",
      "Epoch: 232, Step: 00010/15, Loss: 1.1185\n",
      "Val Loss: 1.1794\n",
      "Epoch: 232\n",
      "Epoch: 233, Step: 00010/15, Loss: 1.1147\n",
      "Val Loss: 1.1806\n",
      "Epoch: 233\n",
      "Epoch: 234, Step: 00010/15, Loss: 1.1044\n",
      "Val Loss: 1.1682\n",
      "Epoch: 234\n",
      "Epoch: 235, Step: 00010/15, Loss: 1.0959\n",
      "Val Loss: 1.2108\n",
      "Epoch: 235\n",
      "Epoch: 236, Step: 00010/15, Loss: 1.1027\n",
      "Val Loss: 1.1679\n",
      "Epoch: 236\n",
      "Epoch: 237, Step: 00010/15, Loss: 1.0818\n",
      "Val Loss: 1.1736\n",
      "Epoch: 237\n",
      "Epoch: 238, Step: 00010/15, Loss: 1.1069\n",
      "Val Loss: 1.1699\n",
      "Epoch: 238\n",
      "Epoch: 239, Step: 00010/15, Loss: 1.0934\n",
      "Val Loss: 1.1710\n",
      "Epoch: 239\n",
      "Epoch: 240, Step: 00010/15, Loss: 1.0843\n",
      "Val Loss: 1.1686\n",
      "Epoch: 240\n",
      "Epoch: 241, Step: 00010/15, Loss: 1.0679\n",
      "Val Loss: 1.1366\n",
      "Epoch: 241\n",
      "Epoch: 242, Step: 00010/15, Loss: 1.0984\n",
      "Val Loss: 1.1731\n",
      "Epoch: 242\n",
      "Epoch: 243, Step: 00010/15, Loss: 1.1120\n",
      "Val Loss: 1.1591\n",
      "Epoch: 243\n",
      "Epoch: 244, Step: 00010/15, Loss: 1.0885\n",
      "Val Loss: 1.1499\n",
      "Epoch: 244\n",
      "Epoch: 245, Step: 00010/15, Loss: 1.0744\n",
      "Val Loss: 1.1609\n",
      "Epoch: 245\n",
      "Epoch: 246, Step: 00010/15, Loss: 1.0733\n",
      "Val Loss: 1.1208\n",
      "Epoch: 246\n",
      "Epoch: 247, Step: 00010/15, Loss: 1.0867\n",
      "Val Loss: 1.1252\n",
      "Epoch: 247\n",
      "Epoch: 248, Step: 00010/15, Loss: 1.0911\n",
      "Val Loss: 1.1643\n",
      "Epoch: 248\n",
      "Epoch: 249, Step: 00010/15, Loss: 1.0882\n",
      "Val Loss: 1.1303\n",
      "Epoch: 249\n",
      "Epoch: 250, Step: 00010/15, Loss: 1.0545\n",
      "Val Loss: 1.1399\n",
      "Epoch: 250\n",
      "Epoch: 251, Step: 00010/15, Loss: 1.0809\n",
      "Val Loss: 1.1480\n",
      "Epoch: 251\n",
      "Epoch: 252, Step: 00010/15, Loss: 1.0569\n",
      "Val Loss: 1.1559\n",
      "Epoch: 252\n",
      "Epoch: 253, Step: 00010/15, Loss: 1.0796\n",
      "Val Loss: 1.1478\n",
      "Epoch: 253\n",
      "Epoch: 254, Step: 00010/15, Loss: 1.0584\n",
      "Val Loss: 1.1538\n",
      "Epoch: 254\n",
      "Epoch: 255, Step: 00010/15, Loss: 1.0521\n",
      "Val Loss: 1.1317\n",
      "Epoch: 255\n",
      "Epoch: 256, Step: 00010/15, Loss: 1.0695\n",
      "Val Loss: 1.1549\n",
      "Epoch: 256\n",
      "Epoch: 257, Step: 00010/15, Loss: 1.0649\n",
      "Val Loss: 1.1375\n",
      "Epoch: 257\n",
      "Epoch: 258, Step: 00010/15, Loss: 1.0535\n",
      "Val Loss: 1.1356\n",
      "Epoch: 258\n",
      "Epoch: 259, Step: 00010/15, Loss: 1.0572\n",
      "Val Loss: 1.1344\n",
      "Epoch: 259\n",
      "Epoch: 260, Step: 00010/15, Loss: 1.0562\n",
      "Val Loss: 1.1124\n",
      "Epoch: 260\n",
      "Epoch: 261, Step: 00010/15, Loss: 1.0480\n",
      "Val Loss: 1.1298\n",
      "Epoch: 261\n",
      "Epoch: 262, Step: 00010/15, Loss: 1.0617\n",
      "Val Loss: 1.1187\n",
      "Epoch: 262\n",
      "Epoch: 263, Step: 00010/15, Loss: 1.0314\n",
      "Val Loss: 1.1110\n",
      "Epoch: 263\n",
      "Epoch: 264, Step: 00010/15, Loss: 1.0391\n",
      "Val Loss: 1.1171\n",
      "Epoch: 264\n",
      "Epoch: 265, Step: 00010/15, Loss: 1.0443\n",
      "Val Loss: 1.0943\n",
      "Epoch: 265\n",
      "Epoch: 266, Step: 00010/15, Loss: 1.0478\n",
      "Val Loss: 1.1143\n",
      "Epoch: 266\n",
      "Epoch: 267, Step: 00010/15, Loss: 1.0596\n",
      "Val Loss: 1.1307\n",
      "Epoch: 267\n",
      "Epoch: 268, Step: 00010/15, Loss: 1.0353\n",
      "Val Loss: 1.1199\n",
      "Epoch: 268\n",
      "Epoch: 269, Step: 00010/15, Loss: 1.0368\n",
      "Val Loss: 1.1252\n",
      "Epoch: 269\n",
      "Epoch: 270, Step: 00010/15, Loss: 1.0499\n",
      "Val Loss: 1.1100\n",
      "Epoch: 270\n",
      "Epoch: 271, Step: 00010/15, Loss: 1.0416\n",
      "Val Loss: 1.1064\n",
      "Epoch: 271\n",
      "Epoch: 272, Step: 00010/15, Loss: 1.0242\n",
      "Val Loss: 1.1137\n",
      "Epoch: 272\n",
      "Epoch: 273, Step: 00010/15, Loss: 1.0383\n",
      "Val Loss: 1.1379\n",
      "Epoch: 273\n",
      "Epoch: 274, Step: 00010/15, Loss: 1.0385\n",
      "Val Loss: 1.1090\n",
      "Epoch: 274\n",
      "Epoch: 275, Step: 00010/15, Loss: 1.0197\n",
      "Val Loss: 1.1355\n",
      "Epoch: 275\n",
      "Epoch: 276, Step: 00010/15, Loss: 1.0367\n",
      "Val Loss: 1.1160\n",
      "Epoch: 276\n",
      "Epoch: 277, Step: 00010/15, Loss: 1.0173\n",
      "Val Loss: 1.0974\n",
      "Epoch: 277\n",
      "Epoch: 278, Step: 00010/15, Loss: 1.0144\n",
      "Val Loss: 1.1130\n",
      "Epoch: 278\n",
      "Epoch: 279, Step: 00010/15, Loss: 1.0123\n",
      "Val Loss: 1.1041\n",
      "Epoch: 279\n",
      "Epoch: 280, Step: 00010/15, Loss: 1.0150\n",
      "Val Loss: 1.1061\n",
      "Epoch: 280\n",
      "Epoch: 281, Step: 00010/15, Loss: 1.0265\n",
      "Val Loss: 1.1111\n",
      "Epoch: 281\n",
      "Epoch: 282, Step: 00010/15, Loss: 1.0268\n",
      "Val Loss: 1.0717\n",
      "Epoch: 282\n",
      "Epoch: 283, Step: 00010/15, Loss: 1.0174\n",
      "Val Loss: 1.1097\n",
      "Epoch: 283\n",
      "Epoch: 284, Step: 00010/15, Loss: 1.0143\n",
      "Val Loss: 1.1094\n",
      "Epoch: 284\n",
      "Epoch: 285, Step: 00010/15, Loss: 1.0216\n",
      "Val Loss: 1.0960\n",
      "Epoch: 285\n",
      "Epoch: 286, Step: 00010/15, Loss: 1.0023\n",
      "Val Loss: 1.1011\n",
      "Epoch: 286\n",
      "Epoch: 287, Step: 00010/15, Loss: 1.0225\n",
      "Val Loss: 1.0846\n",
      "Epoch: 287\n",
      "Epoch: 288, Step: 00010/15, Loss: 1.0396\n",
      "Val Loss: 1.0939\n",
      "Epoch: 288\n",
      "Epoch: 289, Step: 00010/15, Loss: 1.0087\n",
      "Val Loss: 1.1014\n",
      "Epoch: 289\n",
      "Epoch: 290, Step: 00010/15, Loss: 0.9969\n",
      "Val Loss: 1.0987\n",
      "Epoch: 290\n",
      "Epoch: 291, Step: 00010/15, Loss: 1.0253\n",
      "Val Loss: 1.1042\n",
      "Epoch: 291\n",
      "Epoch: 292, Step: 00010/15, Loss: 1.0065\n",
      "Val Loss: 1.0856\n",
      "Epoch: 292\n",
      "Epoch: 293, Step: 00010/15, Loss: 1.0123\n",
      "Val Loss: 1.0904\n",
      "Epoch: 293\n",
      "Epoch: 294, Step: 00010/15, Loss: 1.0076\n",
      "Val Loss: 1.0654\n",
      "Epoch: 294\n",
      "Epoch: 295, Step: 00010/15, Loss: 0.9924\n",
      "Val Loss: 1.0960\n",
      "Epoch: 295\n",
      "Epoch: 296, Step: 00010/15, Loss: 1.0022\n",
      "Val Loss: 1.0962\n",
      "Epoch: 296\n",
      "Epoch: 297, Step: 00010/15, Loss: 1.0157\n",
      "Val Loss: 1.0753\n",
      "Epoch: 297\n",
      "Epoch: 298, Step: 00010/15, Loss: 1.0164\n",
      "Val Loss: 1.0860\n",
      "Epoch: 298\n",
      "Epoch: 299, Step: 00010/15, Loss: 1.0030\n",
      "Val Loss: 1.0946\n",
      "Epoch: 299\n",
      "Epoch: 300, Step: 00010/15, Loss: 1.0063\n",
      "Val Loss: 1.1029\n",
      "Epoch: 300\n",
      "Epoch: 301, Step: 00010/15, Loss: 1.0160\n",
      "Val Loss: 1.0752\n",
      "Epoch: 301\n",
      "Epoch: 302, Step: 00010/15, Loss: 1.0075\n",
      "Val Loss: 1.0781\n",
      "Epoch: 302\n",
      "Epoch: 303, Step: 00010/15, Loss: 0.9896\n",
      "Val Loss: 1.0893\n",
      "Epoch: 303\n",
      "Epoch: 304, Step: 00010/15, Loss: 1.0039\n",
      "Val Loss: 1.0887\n",
      "Epoch: 304\n",
      "Epoch: 305, Step: 00010/15, Loss: 1.0164\n",
      "Val Loss: 1.0703\n",
      "Epoch: 305\n",
      "Epoch: 306, Step: 00010/15, Loss: 0.9945\n",
      "Val Loss: 1.0834\n",
      "Epoch: 306\n",
      "Epoch: 307, Step: 00010/15, Loss: 1.0056\n",
      "Val Loss: 1.0645\n",
      "Epoch: 307\n",
      "Epoch: 308, Step: 00010/15, Loss: 1.0122\n",
      "Val Loss: 1.0958\n",
      "Epoch: 308\n",
      "Epoch: 309, Step: 00010/15, Loss: 0.9991\n",
      "Val Loss: 1.0821\n",
      "Epoch: 309\n",
      "Epoch: 310, Step: 00010/15, Loss: 0.9919\n",
      "Val Loss: 1.0883\n",
      "Epoch: 310\n",
      "Epoch: 311, Step: 00010/15, Loss: 0.9908\n",
      "Val Loss: 1.0869\n",
      "Epoch: 311\n",
      "Epoch: 312, Step: 00010/15, Loss: 1.0052\n",
      "Val Loss: 1.0790\n",
      "Epoch: 312\n",
      "Epoch: 313, Step: 00010/15, Loss: 0.9987\n",
      "Val Loss: 1.0703\n",
      "Epoch: 313\n",
      "Epoch: 314, Step: 00010/15, Loss: 1.0027\n",
      "Val Loss: 1.0896\n",
      "Epoch: 314\n",
      "Epoch: 315, Step: 00010/15, Loss: 0.9863\n",
      "Val Loss: 1.0726\n",
      "Epoch: 315\n",
      "Epoch: 316, Step: 00010/15, Loss: 1.0091\n",
      "Val Loss: 1.0775\n",
      "Epoch: 316\n",
      "Epoch: 317, Step: 00010/15, Loss: 0.9929\n",
      "Val Loss: 1.0763\n",
      "Epoch: 317\n",
      "Epoch: 318, Step: 00010/15, Loss: 1.0089\n",
      "Val Loss: 1.0893\n",
      "Epoch: 318\n",
      "Epoch: 319, Step: 00010/15, Loss: 0.9883\n",
      "Val Loss: 1.0752\n",
      "Epoch: 319\n",
      "Epoch: 320, Step: 00010/15, Loss: 0.9853\n",
      "Val Loss: 1.0683\n",
      "Epoch: 320\n",
      "Epoch: 321, Step: 00010/15, Loss: 0.9990\n",
      "Val Loss: 1.0676\n",
      "Epoch: 321\n",
      "Epoch: 322, Step: 00010/15, Loss: 0.9929\n",
      "Val Loss: 1.0638\n",
      "Epoch: 322\n",
      "Epoch: 323, Step: 00010/15, Loss: 0.9787\n",
      "Val Loss: 1.0678\n",
      "Epoch: 323\n",
      "Epoch: 324, Step: 00010/15, Loss: 0.9923\n",
      "Val Loss: 1.0765\n",
      "Epoch: 324\n",
      "Epoch: 325, Step: 00010/15, Loss: 1.0073\n",
      "Val Loss: 1.0668\n",
      "Epoch: 325\n",
      "Epoch: 326, Step: 00010/15, Loss: 0.9954\n",
      "Val Loss: 1.0731\n",
      "Epoch: 326\n",
      "Epoch: 327, Step: 00010/15, Loss: 0.9886\n",
      "Val Loss: 1.0587\n",
      "Epoch: 327\n",
      "Epoch: 328, Step: 00010/15, Loss: 1.0001\n",
      "Val Loss: 1.0750\n",
      "Epoch: 328\n",
      "Epoch: 329, Step: 00010/15, Loss: 0.9893\n",
      "Val Loss: 1.0476\n",
      "Epoch: 329\n",
      "Epoch: 330, Step: 00010/15, Loss: 0.9791\n",
      "Val Loss: 1.0825\n",
      "Epoch: 330\n",
      "Epoch: 331, Step: 00010/15, Loss: 0.9871\n",
      "Val Loss: 1.0582\n",
      "Epoch: 331\n",
      "Epoch: 332, Step: 00010/15, Loss: 0.9905\n",
      "Val Loss: 1.0768\n",
      "Epoch: 332\n",
      "Epoch: 333, Step: 00010/15, Loss: 0.9908\n",
      "Val Loss: 1.0692\n",
      "Epoch: 333\n",
      "Epoch: 334, Step: 00010/15, Loss: 0.9883\n",
      "Val Loss: 1.0575\n",
      "Epoch: 334\n",
      "Epoch: 335, Step: 00010/15, Loss: 0.9861\n",
      "Val Loss: 1.0640\n",
      "Epoch: 335\n",
      "Epoch: 336, Step: 00010/15, Loss: 0.9913\n",
      "Val Loss: 1.0529\n",
      "Epoch: 336\n",
      "Epoch: 337, Step: 00010/15, Loss: 0.9730\n",
      "Val Loss: 1.0606\n",
      "Epoch: 337\n",
      "Epoch: 338, Step: 00010/15, Loss: 0.9877\n",
      "Val Loss: 1.0950\n",
      "Epoch: 338\n",
      "Epoch: 339, Step: 00010/15, Loss: 0.9733\n",
      "Val Loss: 1.0614\n",
      "Epoch: 339\n",
      "Epoch: 340, Step: 00010/15, Loss: 0.9879\n",
      "Val Loss: 1.0594\n",
      "Epoch: 340\n",
      "Epoch: 341, Step: 00010/15, Loss: 0.9920\n",
      "Val Loss: 1.0636\n",
      "Epoch: 341\n",
      "Epoch: 342, Step: 00010/15, Loss: 0.9833\n",
      "Val Loss: 1.0656\n",
      "Epoch: 342\n",
      "Epoch: 343, Step: 00010/15, Loss: 0.9881\n",
      "Val Loss: 1.0666\n",
      "Epoch: 343\n",
      "Epoch: 344, Step: 00010/15, Loss: 0.9877\n",
      "Val Loss: 1.0598\n",
      "Epoch: 344\n",
      "Epoch: 345, Step: 00010/15, Loss: 0.9723\n",
      "Val Loss: 1.0471\n",
      "Epoch: 345\n",
      "Epoch: 346, Step: 00010/15, Loss: 0.9760\n",
      "Val Loss: 1.0564\n",
      "Epoch: 346\n",
      "Epoch: 347, Step: 00010/15, Loss: 0.9945\n",
      "Val Loss: 1.0610\n",
      "Epoch: 347\n",
      "Epoch: 348, Step: 00010/15, Loss: 0.9839\n",
      "Val Loss: 1.0559\n",
      "Epoch: 348\n",
      "Epoch: 349, Step: 00010/15, Loss: 0.9812\n",
      "Val Loss: 1.0475\n",
      "Epoch: 349\n",
      "Epoch: 350, Step: 00010/15, Loss: 0.9893\n",
      "Val Loss: 1.0617\n",
      "Epoch: 350\n",
      "Epoch: 351, Step: 00010/15, Loss: 0.9788\n",
      "Val Loss: 1.0758\n",
      "Epoch: 351\n",
      "Epoch: 352, Step: 00010/15, Loss: 0.9789\n",
      "Val Loss: 1.0471\n",
      "Epoch: 352\n",
      "Epoch: 353, Step: 00010/15, Loss: 0.9800\n",
      "Val Loss: 1.0460\n",
      "Epoch: 353\n",
      "Epoch: 354, Step: 00010/15, Loss: 0.9791\n",
      "Val Loss: 1.0694\n",
      "Epoch: 354\n",
      "Epoch: 355, Step: 00010/15, Loss: 0.9744\n",
      "Val Loss: 1.0734\n",
      "Epoch: 355\n",
      "Epoch: 356, Step: 00010/15, Loss: 0.9801\n",
      "Val Loss: 1.0491\n",
      "Epoch: 356\n",
      "Epoch: 357, Step: 00010/15, Loss: 0.9872\n",
      "Val Loss: 1.0430\n",
      "Epoch: 357\n",
      "Epoch: 358, Step: 00010/15, Loss: 0.9761\n",
      "Val Loss: 1.0521\n",
      "Epoch: 358\n",
      "Epoch: 359, Step: 00010/15, Loss: 0.9638\n",
      "Val Loss: 1.0447\n",
      "Epoch: 359\n",
      "Epoch: 360, Step: 00010/15, Loss: 1.0016\n",
      "Val Loss: 1.0571\n",
      "Epoch: 360\n",
      "Epoch: 361, Step: 00010/15, Loss: 0.9779\n",
      "Val Loss: 1.0445\n",
      "Epoch: 361\n",
      "Epoch: 362, Step: 00010/15, Loss: 0.9604\n",
      "Val Loss: 1.0754\n",
      "Epoch: 362\n",
      "Epoch: 363, Step: 00010/15, Loss: 0.9809\n",
      "Val Loss: 1.0611\n",
      "Epoch: 363\n",
      "Epoch: 364, Step: 00010/15, Loss: 0.9728\n",
      "Val Loss: 1.0410\n",
      "Epoch: 364\n",
      "Epoch: 365, Step: 00010/15, Loss: 0.9863\n",
      "Val Loss: 1.0582\n",
      "Epoch: 365\n",
      "Epoch: 366, Step: 00010/15, Loss: 0.9817\n",
      "Val Loss: 1.0490\n",
      "Epoch: 366\n",
      "Epoch: 367, Step: 00010/15, Loss: 0.9689\n",
      "Val Loss: 1.0600\n",
      "Epoch: 367\n",
      "Epoch: 368, Step: 00010/15, Loss: 0.9711\n",
      "Val Loss: 1.0490\n",
      "Epoch: 368\n",
      "Epoch: 369, Step: 00010/15, Loss: 0.9813\n",
      "Val Loss: 1.0449\n",
      "Epoch: 369\n",
      "Epoch: 370, Step: 00010/15, Loss: 0.9728\n",
      "Val Loss: 1.0291\n",
      "Epoch: 370\n",
      "Epoch: 371, Step: 00010/15, Loss: 0.9584\n",
      "Val Loss: 1.0542\n",
      "Epoch: 371\n",
      "Epoch: 372, Step: 00010/15, Loss: 0.9758\n",
      "Val Loss: 1.0548\n",
      "Epoch: 372\n",
      "Epoch: 373, Step: 00010/15, Loss: 0.9697\n",
      "Val Loss: 1.0605\n",
      "Epoch: 373\n",
      "Epoch: 374, Step: 00010/15, Loss: 0.9902\n",
      "Val Loss: 1.0502\n",
      "Epoch: 374\n",
      "Epoch: 375, Step: 00010/15, Loss: 0.9746\n",
      "Val Loss: 1.0458\n",
      "Epoch: 375\n",
      "Epoch: 376, Step: 00010/15, Loss: 0.9652\n",
      "Val Loss: 1.0594\n",
      "Epoch: 376\n",
      "Epoch: 377, Step: 00010/15, Loss: 0.9806\n",
      "Val Loss: 1.0396\n",
      "Epoch: 377\n",
      "Epoch: 378, Step: 00010/15, Loss: 0.9625\n",
      "Val Loss: 1.0421\n",
      "Epoch: 378\n",
      "Epoch: 379, Step: 00010/15, Loss: 0.9694\n",
      "Val Loss: 1.0298\n",
      "Epoch: 379\n",
      "Epoch: 380, Step: 00010/15, Loss: 0.9705\n",
      "Val Loss: 1.0416\n",
      "Epoch: 380\n",
      "Epoch: 381, Step: 00010/15, Loss: 0.9638\n",
      "Val Loss: 1.0444\n",
      "Epoch: 381\n",
      "Epoch: 382, Step: 00010/15, Loss: 0.9699\n",
      "Val Loss: 1.0491\n",
      "Epoch: 382\n",
      "Epoch: 383, Step: 00010/15, Loss: 0.9598\n",
      "Val Loss: 1.0468\n",
      "Epoch: 383\n",
      "Epoch: 384, Step: 00010/15, Loss: 0.9804\n",
      "Val Loss: 1.0439\n",
      "Epoch: 384\n",
      "Epoch: 385, Step: 00010/15, Loss: 0.9737\n",
      "Val Loss: 1.0267\n",
      "Epoch: 385\n",
      "Epoch: 386, Step: 00010/15, Loss: 0.9559\n",
      "Val Loss: 1.0668\n",
      "Epoch: 386\n",
      "Epoch: 387, Step: 00010/15, Loss: 0.9727\n",
      "Val Loss: 1.0548\n",
      "Epoch: 387\n",
      "Epoch: 388, Step: 00010/15, Loss: 0.9599\n",
      "Val Loss: 1.0309\n",
      "Epoch: 388\n",
      "Epoch: 389, Step: 00010/15, Loss: 0.9708\n",
      "Val Loss: 1.0503\n",
      "Epoch: 389\n",
      "Epoch: 390, Step: 00010/15, Loss: 0.9722\n",
      "Val Loss: 1.0477\n",
      "Epoch: 390\n",
      "Epoch: 391, Step: 00010/15, Loss: 0.9577\n",
      "Val Loss: 1.0427\n",
      "Epoch: 391\n",
      "Epoch: 392, Step: 00010/15, Loss: 0.9713\n",
      "Val Loss: 1.0258\n",
      "Epoch: 392\n",
      "Epoch: 393, Step: 00010/15, Loss: 0.9736\n",
      "Val Loss: 1.0682\n",
      "Epoch: 393\n",
      "Epoch: 394, Step: 00010/15, Loss: 0.9612\n",
      "Val Loss: 1.0444\n",
      "Epoch: 394\n",
      "Epoch: 395, Step: 00010/15, Loss: 0.9657\n",
      "Val Loss: 1.0421\n",
      "Epoch: 395\n",
      "Epoch: 396, Step: 00010/15, Loss: 0.9794\n",
      "Val Loss: 1.0472\n",
      "Epoch: 396\n",
      "Epoch: 397, Step: 00010/15, Loss: 0.9603\n",
      "Val Loss: 1.0355\n",
      "Epoch: 397\n",
      "Epoch: 398, Step: 00010/15, Loss: 0.9567\n",
      "Val Loss: 1.0449\n",
      "Epoch: 398\n",
      "Epoch: 399, Step: 00010/15, Loss: 0.9631\n",
      "Val Loss: 1.0555\n",
      "Epoch: 399\n"
     ]
    }
   ],
   "source": [
    "def train(epoch, log_steps=10, eval_steps=1000):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, (pos_rw, neg_rw) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % log_steps == 0:\n",
    "            print((f'Epoch: {epoch}, Step: {i + 1:05d}/{len(loader)}, '\n",
    "                   f'Loss: {total_loss / log_steps:.4f}'))\n",
    "            total_loss = 0\n",
    "\n",
    "    val_loss = test()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    losses = []\n",
    "    for i, (pos_rw, neg_rw) in enumerate(loader_val):\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        losses.append(loss.item())\n",
    "    print(f'Val Loss: {np.mean(losses):.4f}')\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "for epoch in range(400):\n",
    "    train(epoch)\n",
    "    # acc = test()\n",
    "    print(f'Epoch: {epoch}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}