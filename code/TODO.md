- [x] Look into https://graphia.app/
- [x] GraphML export script for Neo4j
- [x] Add smart schema overwrite (preserving existing properties)
- [x] Add preprocessing for other datasets
- [x] Figure out how to use timestamps in gephi
- [ ] Add docker configs and run scripts for baselines
  - [ ] SageDy
  - [ ] EvolveGCN
  - [ ] CTDNE
- [x] Add dataset export to edge list
- [x] Find ground truth communities if applicable on current datasets
  - [x] Karate Club
  - [x] DBLP
  - [x] com-* datasets
- [x] Add baseline config format (and add configs)
- [x] Add dynamic community file format holding inter time community id matches
- [x] Add baseline benchmarking based on config
- [x] Add W&B support for running baselines
- [x] Implement evaluation metrics:
  - Annotated:
  - [x] NMI
  - [x] NF1
  - [x] Jaccard Coefficient
  - [x] Rand-Index
  - [x] Overlapping-NMI
  - [x] Omega-Index
  - Metric based:
  - [x] Modularity
  - [x] Conductance
  - [x] Expansion
  - [x] Internal Density
  - [x] Cut Ratio/Normalized Cut
  - [x] Maximum/Average ODF
  - [x] Triangle Participation Ratio (probably wont - high computation complexity)
- [x] Add evaluation metrics to the baselines
- [x] Add a way to visualize CD results
- [x] Add a way to generate synthetic benchmarks
- [x] Double check alignment of comlist to snapshot files during evaluation
- [x] Change GENSIM baseline to the tensorflow one // gensim performs poorly
- [ ] Add https://github.com/isaranto/community-tracking synthetic benchmark
- [ ] Add task specific evalution
- [ ] Add cluster based evaluation metrics
- [x] Run Static Community Detection on benchmark datasets
- [x] Run Dynamic Community Detection on benchmark datasets
- [ ] Run Representation Learning on benchmark datasets
- [ ] Implement remaining DCD methods
- [x] Check [Dynamo](https://github.com/nogrady/dynamo) is seems like a recent DCD
- [x] Run thesis through GeCTOR 
- [x] Collect dataset statistics
  - [x] Node/Edge counts (global and per type)
  - [x] Amount of connected components (and their side distribution)
  - [x] Temporal Distribution (Histogram of nodes and edges)
  - [x] Degree Distribution
  - [x] Modularity
  - [x] List most central nodes (Degree centrality, Betweenness centrality, Closeness centrality)
  - [x] Per cluster statistics (Node/Edge counts, Temporal Distribution, Degree Distribution, Modularity)
  - [x] Centrality etc per node type
  - [x] Clustering statistics per edge type
- [ ] Implement a cluster entropy metric
- [ ] Experiment and define good candidate objective measures for:
  - [ ] Homophily (unsupervised node similarity <-> connectedness)
  - [ ] Community cohesion (node - community relation)
  - [ ] Community quality (community - community [- to node] relation)
  - [ ] Later on (apply this to temporal communities)
- [ ] Figure a way out to efficiently calculate/approximate these measures
  - [ ] Through sampling (if possible)
  - [ ] Through incremental calculation
- [ ] If applicable, figure a training regimen to optimize them
  - [ ] Do we need to do two step (node - communty) optimization? If yes can we balance the two? (meta learning?)
- [ ] Fix Dynamo (I think loading goes wrong somewhere)
