{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from shared.constants import DatasetPath"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "DATASET = DatasetPath('DBLP-V1')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(f'{DATASET}')\n",
    "         .config('spark.sql.legacy.timeParserPolicy', 'LEGACY')\n",
    "         .config(\"spark.executor.memory\", \"8g\")\n",
    "         .config(\"spark.driver.memory\", \"8g\")\n",
    "         .config(\"spark.memory.offHeap.enabled\", True)\n",
    "         .config(\"spark.memory.offHeap.size\", \"16g\")\n",
    "         .getOrCreate())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(value=['629814', '#*Automated Deduction in Geometry: 5th International Workshop, ADG 2004, Gainesville, FL, USA, September 16-18, 2004, Revised Papers (Lecture Notes in Computer ... / Lecture Notes in Artificial Intelligence)', '#@Hoon Hong,Dongming Wang', '#t2006', '#c', '#index0']),\n Row(value=['#*A+ Certification Core Hardware (Text & Lab Manual)', '#@Charles J. Brooks', '#t2003', '#c', '#index1']),\n Row(value=['#*Performance engineering in industry: current practices and adoption challenges', '#@Ahmed E. Hassan,Parminder Flora', '#t2007', '#cProceedings of the 6th international workshop on Software and performance', '#index2', '#!This panel session discusses performance engineering practices in industry. Presentations in the session will explore the use of lightweight techniques and approaches in order to permit the cost effective and rapid adoption of performance modeling research by large industrial software systems.']),\n Row(value=['#*Dude, You Can Do It! How to Build a Sweeet PC', '#@Darrel Creacy,Carlito Vicencio', '#t2005', '#c', '#index3', \"#!Whether you're frustrated with current PC offerings (and their inflated prices) or are simply looking for a cool project to take on, building a computer from the ground up using off-the-shelf parts can offer significant advantages. In these pages, computer dudes Darrel Wayne Creacy and Carlito Vicencio outline those advantages and then show you how to build the computer of your dreams. The pair begins by explaining what components make up a PC and what you need to think about when selecting those components, before helping you determine your needs and suggesting various configurations to fit those uses. Breaking the process down into its simplest terms, the authors provide component lists for a number of different PC setups: for students, home users, multimedia/home-theater enthusiasts, high-end graphic/video/audio producers, and more. Using plain language and plenty of visual and instructional aids--photos, illustrations, diagrams, step-by-step directions, and more--the authors ensure that even someone (like you!) who knows nothing about technology can build the perfect PC! On a more personal note, the authors are donating a percentage of their income from this book to the Breast Cancer Research Foundationï¾\\x96to thank all the women in their lives who have supported them and battled the disease. For more information about BCRF, please visit http://www.bcrfcure.org/.\"]),\n Row(value=['#*What Every Programmer Needs to Know about Security (Advances in Information Security)', '#@Neil Daswani,Anita Kesavan', '#t2006', '#c', '#index4']),\n Row(value=['#*Interpreting Kullback-Leibler divergence with the Neyman-Pearson lemma', '#@Shinto Eguchi,John Copas', '#t2006', '#cJournal of Multivariate Analysis', '#index5', '#%436405', '#!Kullback-Leibler divergence and the Neyman-Pearson lemma are two fundamental concepts in statistics. Both are about likelihood ratios: Kullback-Leibler divergence is the expected log-likelihood ratio, and the Neyman-Pearson lemma is about error rates of likelihood ratio tests. Exploring this connection gives another statistical interpretation of the Kullback-Leibler divergence in terms of the loss of power of the likelihood ratio test when the wrong distribution is used for one of the hypotheses. In this interpretation, the standard non-negativity property of the Kullback-Leibler divergence is essentially a restatement of the optimal property of likelihood ratios established by the Neyman-Pearson lemma. The asymmetry of Kullback-Leibler divergence is overviewed in information geometry.']),\n Row(value=['#*Digital Media: Transformations in Human Communication', '#@Lee Humphreys,Paul Messaris', '#t2006', '#c', '#index6']),\n Row(value=['#*TOPP---the OpenMS proteomics pipeline', '#@Oliver Kohlbacher,Knut Reinert,Clemens Gröpl,Eva Lange,Nico Pfeifer,Ole Schulz-Trieglaff,Marc Sturm', '#t2007', '#cBioinformatics', '#index7', \"#!Motivation: Experimental techniques in proteomics have seen rapid development over the last few years. Volume and complexity of the data have both been growing at a similar rate. Accordingly, data management and analysis are one of the major challenges in proteomics. Flexible algorithms are required to handle changing experimental setups and to assist in developing and validating new methods. In order to facilitate these studies, it would be desirable to have a flexible 'toolbox' of versatile and user-friendly applications allowing for rapid construction of computational workflows in proteomics. Results: We describe a set of tools for proteomics data analysis---TOPP, The OpenMS Proteomics Pipeline. TOPP provides a set of computational tools which can be easily combined into analysis pipelines even by non-experts and can be used in proteomics workflows. These applications range from useful utilities (file format conversion, peak picking) over wrapper applications for known applications (e.g. Mascot) to completely new algorithmic techniques for data reduction and data analysis. We anticipate that TOPP will greatly facilitate rapid prototyping of proteomics data evaluation pipelines. As such, we describe the basic concepts and the current abilities of TOPP and illustrate these concepts in the context of two example applications: the identification of peptides from a raw dataset through database search and the complex analysis of a standard addition experiment for the absolute quantitation of biomarkers. The latter example demonstrates TOPP's ability to construct flexible analysis pipelines in support of complex experimental setups. Availability: The TOPP components are available as open-source software under the lesser GNU public license (LGPL). Source code is available from the project website at www.OpenMS.de Contact: oliver.kohlbacher@uni-tuebingen.de\"]),\n Row(value=['#*Type Graphics and MacIntosh', '#@John Blaint', '#t1987', '#c', '#index8']),\n Row(value=['#*Adaptive Hypermedia and Adaptive Web-Based Systems: 4th International Conference, AH 2006, Dublin, Ireland, June 21-23, 2006, Proceedings (Lecture Notes in Computer Science)', '#@Vincent Wade,Helen Ashman,Barry Smyth', '#t2006', '#c', '#index9'])]"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    spark.read.text(DATASET.raw_str('outputacm.txt'), wholetext=False, lineSep='\\n\\n')\n",
    "        .withColumn('value', F.split(F.col('value'), '\\n'))\n",
    ")\n",
    "\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(title='Automated Deduction in Geometry: 5th International Workshop, ADG 2004, Gainesville, FL, USA, September 16-18, 2004, Revised Papers (Lecture Notes in Computer ... / Lecture Notes in Artificial Intelligence)', authors=['Hoon Hong', 'Dongming Wang'], year=2006, venue='', index=0, references=[], abstract=None),\n Row(title='A+ Certification Core Hardware (Text & Lab Manual)', authors=['Charles J. Brooks'], year=2003, venue='', index=1, references=[], abstract=None),\n Row(title='Performance engineering in industry: current practices and adoption challenges', authors=['Ahmed E. Hassan', 'Parminder Flora'], year=2007, venue='Proceedings of the 6th international workshop on Software and performance', index=2, references=[], abstract='This panel session discusses performance engineering practices in industry. Presentations in the session will explore the use of lightweight techniques and approaches in order to permit the cost effective and rapid adoption of performance modeling research by large industrial software systems.'),\n Row(title='Dude, You Can Do It! How to Build a Sweeet PC', authors=['Darrel Creacy', 'Carlito Vicencio'], year=2005, venue='', index=3, references=[], abstract=\"Whether you're frustrated with current PC offerings (and their inflated prices) or are simply looking for a cool project to take on, building a computer from the ground up using off-the-shelf parts can offer significant advantages. In these pages, computer dudes Darrel Wayne Creacy and Carlito Vicencio outline those advantages and then show you how to build the computer of your dreams. The pair begins by explaining what components make up a PC and what you need to think about when selecting those components, before helping you determine your needs and suggesting various configurations to fit those uses. Breaking the process down into its simplest terms, the authors provide component lists for a number of different PC setups: for students, home users, multimedia/home-theater enthusiasts, high-end graphic/video/audio producers, and more. Using plain language and plenty of visual and instructional aids--photos, illustrations, diagrams, step-by-step directions, and more--the authors ensure that even someone (like you!) who knows nothing about technology can build the perfect PC! On a more personal note, the authors are donating a percentage of their income from this book to the Breast Cancer Research Foundationï¾\\x96to thank all the women in their lives who have supported them and battled the disease. For more information about BCRF, please visit http://www.bcrfcure.org/.\"),\n Row(title='What Every Programmer Needs to Know about Security (Advances in Information Security)', authors=['Neil Daswani', 'Anita Kesavan'], year=2006, venue='', index=4, references=[], abstract=None)]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField('title', T.StringType(), True),\n",
    "    T.StructField('authors', T.ArrayType(T.StringType(), False), False),\n",
    "    T.StructField('year', T.IntegerType(), True),\n",
    "    T.StructField('venue', T.StringType(), True),\n",
    "    T.StructField('index', T.IntegerType(), True),\n",
    "    T.StructField('references', T.ArrayType(T.StringType(), False), False),\n",
    "    T.StructField('abstract', T.StringType(), True),\n",
    "])\n",
    "\n",
    "@F.udf(returnType=schema)\n",
    "def parse_citation(lines):\n",
    "    result = {\n",
    "        'title': None,\n",
    "        'authors': [],\n",
    "        'year': None,\n",
    "        'venue': None,\n",
    "        'index': None,\n",
    "        'references': [],\n",
    "        'abstract': None,\n",
    "    }\n",
    "    for line in lines:\n",
    "        if line.startswith('#*'):\n",
    "            result['title'] = line[2:].strip()\n",
    "        elif line.startswith('#@'):\n",
    "            result['authors'].extend(line[2:].strip().split(','))\n",
    "        elif line.startswith('#t'):\n",
    "            result['year'] = int(line[2:].strip())\n",
    "        elif line.startswith('#c'):\n",
    "            result['venue'] = line[2:].strip()\n",
    "        elif line.startswith('#index'):\n",
    "            result['index'] = int(line[6:].strip())\n",
    "        elif line.startswith('#%'):\n",
    "            result['references'].extend(line[2:].strip().split(','))\n",
    "        elif line.startswith('#!'):\n",
    "            result['abstract'] = line[2:].strip()\n",
    "    return result\n",
    "\n",
    "df_papers = df.select(\n",
    "    parse_citation(F.col('value')).alias('parsed_citation')\n",
    ").select('parsed_citation.*').cache()\n",
    "df_papers.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595775\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(name=' B. West', id=' B. West'),\n Row(name=' Consumer Electronics Society Staff', id=' Consumer Electronics Society Staff'),\n Row(name=' D. Crookes', id=' D. Crookes'),\n Row(name=' H', id=' H'),\n Row(name=' H. Hsu', id=' H. Hsu')]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_authors = (\n",
    "    df_papers.select(\n",
    "        F.explode(F.col('authors')).alias('name'),\n",
    "    ).withColumn('id', F.col('name'))\n",
    "     .filter(F.col('id').isNotNull())\n",
    "        .dropDuplicates(['id'])\n",
    ")\n",
    "print(df_nodes_authors.count())\n",
    "df_nodes_authors.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12609\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(id='(1987)', name='(1987)'),\n Row(id='(1992&ndash;1993)', name='(1992&ndash;1993)'),\n Row(id='(1993&ndash;1994)', name='(1993&ndash;1994)'),\n Row(id='(1994 Supplement)', name='(1994 Supplement)'),\n Row(id='(Fall 1991)', name='(Fall 1991)')]"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_venues = (\n",
    "    df_papers.select(\n",
    "        F.col('venue').alias('id'),\n",
    "        F.col('venue').alias('name'),\n",
    "    ).filter(\"id != ''\")\n",
    "     .filter(F.col('id').isNotNull())\n",
    "        .dropDuplicates(['id'])\n",
    ")\n",
    "print(df_nodes_venues.count())\n",
    "df_nodes_venues.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|  -1|    9|\n",
      "|1900|    1|\n",
      "|1941|    1|\n",
      "|1947|    1|\n",
      "|1949|    1|\n",
      "|1950|    2|\n",
      "|1951|   21|\n",
      "|1952|   35|\n",
      "|1953|   63|\n",
      "|1954|    6|\n",
      "|1955|   14|\n",
      "|1956|   53|\n",
      "|1957|    9|\n",
      "|1958|   45|\n",
      "|1959|   89|\n",
      "|1960|  174|\n",
      "|1961|  343|\n",
      "|1962|  412|\n",
      "|1963|  310|\n",
      "|1964|  359|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_papers.groupby('year').count().orderBy('year').show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(id=31, title='Beginning Ruby on Rails (Wrox Beginning Guides)', authors=['Steve Holzner'], venue='', year=2006, abstract=None, timestamp=datetime.datetime(2006, 1, 1, 0, 0)),\n Row(id=34, title='Hyperstat: Macintosh Hypermedia for Analyzing Data and Learning Statistics', authors=['David M. Lane'], venue='', year=1993, abstract=None, timestamp=datetime.datetime(1993, 1, 1, 0, 0)),\n Row(id=53, title='Class-specific feature polynomial classifier for pattern classification and its application to handwritten numeral recognition', authors=['Cheng-Lin Liu', 'Hiroshi Sako'], venue='Pattern Recognition', year=2006, abstract='The polynomial classifier (PC) that takes the binomial terms of reduced subspace features as inputs has shown superior performance to multilayer neural networks in pattern classification. In this paper, we propose a class-specific feature polynomial classifier (CFPC) that extracts class-specific features from class-specific subspaces, unlike the ordinary PC that uses a class-independent subspace. The CFPC can be viewed as a hybrid of ordinary PC and projection distance method. The class-specific features better separate one class from the others, and the incorporation of class-specific projection distance further improves the separability. The connecting weights of CFPC are efficiently learned class-by-class to minimize the mean square error on training samples. To justify the promise of CFPC, we have conducted experiments of handwritten digit recognition and numeral string recognition on the NIST Special Database 19 (SD19). The digit recognition task was also benchmarked on two standard databases USPS and MNIST. The results show that the performance of CFPC is superior to that of ordinary PC, and is competitive with support vector classifiers (SVCs).', timestamp=datetime.datetime(2006, 1, 1, 0, 0)),\n Row(id=65, title='GO Series: Microsoft Excel 2003 Volume 2 (Go With Microsoft Office)', authors=['Shelley Gaskin', 'John Preston', 'Sally Preston'], venue='', year=2004, abstract=None, timestamp=datetime.datetime(2004, 1, 1, 0, 0)),\n Row(id=78, title='The Prentice Hall Planner for Student Success', authors=[''], venue='', year=2004, abstract=None, timestamp=datetime.datetime(2004, 1, 1, 0, 0))]"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_papers = (\n",
    "    df_papers.filter('year > 1900').select(\n",
    "        F.col('index').alias('id'),\n",
    "        'title',\n",
    "        'authors',\n",
    "        'venue',\n",
    "        'year',\n",
    "        'abstract',\n",
    "        F.to_timestamp(F.col('year').cast('string'), 'yyyy').alias('timestamp')\n",
    "    ) .filter(F.col('id').isNotNull())\n",
    "        .dropDuplicates(['id'])\n",
    ")\n",
    "print(df_nodes_papers.count())\n",
    "df_nodes_papers.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "df_node_ids = (\n",
    "    df_nodes_authors.select('id')\n",
    "        .union(df_nodes_venues.select('id'))\n",
    "        .union(df_nodes_papers.select('id'))\n",
    "        .distinct()\n",
    ")\n",
    "\n",
    "def filter_node_ids(df):\n",
    "    return df.join(\n",
    "        df_node_ids,\n",
    "        F.col('src') == F.col('id'),\n",
    "        'inner'\n",
    "    ).drop(\n",
    "        'id'\n",
    "    ).join(\n",
    "        df_node_ids,\n",
    "        F.col('dst') == F.col('id'),\n",
    "        'inner'\n",
    "    ).drop('id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1337700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(src='Arthur Greef', dst=28),\n Row(src='Lars Dragheim Olsen', dst=28),\n Row(src='Michael Fruergaard Pontoppidan', dst=28),\n Row(src='Palle Agermark', dst=28),\n Row(src='Hans J. Skovgaard', dst=28)]"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_edges_authored = filter_node_ids(\n",
    "    df_papers.select(\n",
    "        F.explode(F.col('authors')).alias('src'),\n",
    "        F.col('index').alias('dst'),\n",
    "    ).distinct()\n",
    ")\n",
    "print(df_edges_authored.count())\n",
    "df_edges_authored.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(src=164367, dst='(March 1987)'),\n Row(src=517467, dst='(May 1991)'),\n Row(src=528823, dst='(May 1991)'),\n Row(src=515847, dst='(May 1991)'),\n Row(src=93847, dst='3C ON-LINE')]"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_edges_published_in = filter_node_ids(\n",
    "    df_papers.select(\n",
    "        F.col('index').alias('src'),\n",
    "        F.col('venue').alias('dst'),\n",
    "    ).filter(\"dst != ''\").distinct()\n",
    ")\n",
    "print(df_edges_published_in.count())\n",
    "df_edges_published_in.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632751\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(src=55537, dst=26),\n Row(src=391844, dst=76),\n Row(src=408689, dst=101),\n Row(src=56911, dst=101),\n Row(src=62796, dst=126)]"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_edges_cited = filter_node_ids(\n",
    "    df_papers.select(\n",
    "        F.col('index').alias('src'),\n",
    "        F.explode(F.col('references')).alias('dst'),\n",
    "    )\n",
    "        .withColumn('dst', F.col('dst').cast('int'))\n",
    "        .distinct()\n",
    "\n",
    ")\n",
    "print(df_edges_cited.count())\n",
    "df_edges_cited.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_nodes_authors.write.parquet(DATASET.processed_str('nodes_Author'), mode='overwrite')\n",
    "df_nodes_venues.write.parquet(DATASET.processed_str('nodes_Venue'), mode='overwrite')\n",
    "df_nodes_papers.write.parquet(DATASET.processed_str('nodes_Paper'), mode='overwrite')\n",
    "\n",
    "df_edges_authored.write.parquet(DATASET.processed_str('edges_AUTHORED'), mode='overwrite')\n",
    "df_edges_published_in.write.parquet(DATASET.processed_str('edges_PUBLISHED_IN'), mode='overwrite')\n",
    "df_edges_cited.write.parquet(DATASET.processed_str('edges_CITED'), mode='overwrite')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-01-19 20:31:32,712][/dd_volume/Development/Python/Thesis/code/datasets/datasets/build_schema.py][DEBUG] Merging old schema for DBLP-V1\n"
     ]
    },
    {
     "data": {
      "text/plain": "DatasetSchema(name='DBLP-V1', prefix='DBLP_V1', database='DBLP-V1', description='None', nodes=[NodeSchema(path='data/processed/DBLP-V1/nodes_Author', properties=[Property(name='name', type='string', ignore=False, label=True, timestamp=False), Property(name='id', type='string', ignore=False, label=False, timestamp=False)], label='Author'), NodeSchema(path='data/processed/DBLP-V1/nodes_Venue', properties=[Property(name='id', type='string', ignore=False, label=False, timestamp=False), Property(name='name', type='string', ignore=False, label=True, timestamp=False)], label='Venue'), NodeSchema(path='data/processed/DBLP-V1/nodes_Paper', properties=[Property(name='id', type='int', ignore=False, label=False, timestamp=False), Property(name='title', type='string', ignore=False, label=True, timestamp=False), Property(name='authors', type='string[]', ignore=False, label=False, timestamp=False), Property(name='venue', type='string', ignore=False, label=False, timestamp=False), Property(name='year', type='int', ignore=False, label=False, timestamp=False), Property(name='abstract', type='string', ignore=False, label=False, timestamp=False), Property(name='timestamp', type='datetime', ignore=False, label=False, timestamp=False)], label='Paper')], edges=[EdgeSchema(path='data/processed/DBLP-V1/edges_AUTHORED', properties=[Property(name='src', type='string', ignore=False, label=False, timestamp=False), Property(name='dst', type='int', ignore=False, label=False, timestamp=False)], type='AUTHORED', source='Author', target='Paper', directed=True), EdgeSchema(path='data/processed/DBLP-V1/edges_PUBLISHED_IN', properties=[Property(name='src', type='int', ignore=False, label=False, timestamp=False), Property(name='dst', type='string', ignore=False, label=False, timestamp=False)], type='PUBLISHED_IN', source='Paper', target='Venue', directed=True), EdgeSchema(path='data/processed/DBLP-V1/edges_CITED', properties=[Property(name='src', type='int', ignore=False, label=False, timestamp=False), Property(name='dst', type='int', ignore=False, label=False, timestamp=False)], type='CITED', source='Paper', target='Paper', directed=True)])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.build_schema import build_schema\n",
    "\n",
    "build_schema(\n",
    "    spark,\n",
    "    name=str(DATASET),\n",
    "    nodes=[\n",
    "        ('Author', DATASET.processed_str('nodes_Author')),\n",
    "        ('Venue', DATASET.processed_str('nodes_Venue')),\n",
    "        ('Paper', DATASET.processed_str('nodes_Paper')),\n",
    "    ],\n",
    "    edges=[\n",
    "        ('Authored', 'Author', 'Paper', DATASET.processed_str('edges_AUTHORED')),\n",
    "        ('PublishedIn', 'Paper', 'Venue', DATASET.processed_str('edges_PUBLISHED_IN')),\n",
    "        ('Cited', 'Paper', 'Paper', DATASET.processed_str('edges_CITED')),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}