{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import SparkSession"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from shared.schema import DatasetSchema\n",
    "\n",
    "DATASET = DatasetSchema.load_schema('DBLP-HCN')\n",
    "DATASET.save_schema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/23 01:31:24 WARN Utils: Your hostname, megatron resolves to a loopback address: 127.0.1.1; using 192.168.1.89 instead (on interface enp7s0)\n",
      "22/01/23 01:31:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/23 01:31:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/01/23 01:31:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/01/23 01:31:26 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/01/23 01:31:26 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/01/23 01:31:26 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/01/23 01:31:26 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(f'{DATASET}')\n",
    "         .config('spark.sql.legacy.timeParserPolicy', 'LEGACY')\n",
    "         .config(\"spark.executor.memory\", \"8g\")\n",
    "         .config(\"spark.driver.memory\", \"8g\")\n",
    "         .config(\"spark.memory.offHeap.enabled\", True)\n",
    "         .config(\"spark.memory.offHeap.size\", \"16g\")\n",
    "         .getOrCreate())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(id='104', title='SWORD: workload-aware data placement and replica selection for cloud data management systems.', venue='vldb', year=2014, authors=['K. Ashwin Kumar', 'Abdul Quamar'], abstract='Cloud computing is increasingly being seen as a way to reduce infrastructure costs and add elasticity, and is being used by a wide range of organizations. Cloud data management systems today need to serve a range of different workloads, from analytical read-heavy workloads to transactional (OLTP) workloads. For both the service providers and the users, it is critical to minimize the consumption of resources like CPU, memory, communication bandwidth, and energy, without compromising on service-level agreements if any. In this article, we develop a workload-aware data placement and replication approach, called SWORD, for minimizing resource consumption in such an environment. Specifically, we monitor and model the expected workload as a hypergraph and develop partitioning techniques that minimize the average query span, i.e., the average number of machines involved in the execution of a query or a transaction. We empirically justify the use of query span as the metric to optimize, for both analytical and transactional workloads, and develop a series of replication and data placement algorithms by drawing connections to several well-studied graph theoretic concepts. We introduce a suite of novel techniques to achieve high scalability by reducing the overhead of partitioning and query routing. To deal with workload changes, we propose an incremental repartitioning technique that modifies data placement in small steps without resorting to complete repartitioning. We propose the use of fine-grained quorums defined at the level of groups of data items to control the cost of distributed updates, improve throughput, and adapt to different workloads. We empirically illustrate the benefits of our approach through a comprehensive experimental evaluation for two classes of workloads. For analytical read-only workloads, we show that our techniques result in significant reduction in total resource consumption. For OLTP workloads, we show that our approach improves transaction latencies and overall throughput by minimizing the number of distributed transactions.'),\n Row(id='106', title='A survey of large-scale analytical query processing in MapReduce.', venue='vldb', year=2014, authors=['Christos Doulkeridis', 'Kjetil N&oslash'], abstract='Enterprises today acquire vast volumes of data from different sources and leverage this information by means of data analysis to support effective decision-making and provide new functionality and services. The key requirement of data analytics is scalability, simply due to the immense volume of data that need to be extracted, processed, and analyzed in a timely fashion. Arguably the most popular framework for contemporary large-scale data analytics is MapReduce, mainly due to its salient features that include scalability, fault-tolerance, ease of programming, and flexibility. However, despite its merits, MapReduce has evident performance limitations in miscellaneous analytical tasks, and this has given rise to a significant body of research that aim at improving its efficiency, while maintaining its desirable properties. This survey aims to review the state of the art in improving the performance of parallel query processing using MapReduce. A set of the most significant weaknesses and limitations of MapReduce is discussed at a high level, along with solving techniques. A taxonomy is presented for categorizing existing research on MapReduce improvements according to the specific problem they target. Based on the proposed taxonomy, a classification of existing research is provided focusing on the optimization objective. Concluding, we outline interesting directions for future parallel data processing systems.'),\n Row(id='107', title='Taking the Big Picture: representative skylines based on significance and diversity.', venue='vldb', year=2014, authors=['Matteo Magnani', 'Ira Assent'], abstract='The skyline is a popular operator to extract records from a database when a record scoring function is not available. However, the result of a skyline query can be very large. The problem addressed in this paper is the automatic selection of a small number \\\\((k)\\\\) of representative skyline records. Existing approaches have only focused on partial aspects of this problem. Some try to identify sets of diverse records giving an overall approximation of the skyline. These techniques, however, are sensitive to the scaling of attributes or to the insertion of non-skyline records into the database. Others exploit some knowledge of the record scoring function to identify the most significant record, but not sets of records representative of the whole skyline. In this paper, we introduce a novel approach taking both the significance of all the records and their diversity into account, adapting to available knowledge of the scoring function, but also working under complete ignorance. We show the intractability of the problem and present approximate algorithms. We experimentally show that our approach is efficient, scalable and that it improves existing works in terms of the significance and diversity of the results.'),\n Row(id='108', title='Calibrating trajectory data for spatio-temporal similarity analysis.', venue='vldb', year=2015, authors=['Han Su', 'Kai Zheng'], abstract='Due to the prevalence of GPS-enabled devices and wireless communications technologies, spatial trajectories that describe the movement history of moving objects are being generated and accumulated at an unprecedented pace. Trajectory data in a database are intrinsically heterogeneous, as they represent discrete approximations of original continuous paths derived using different sampling strategies and different sampling rates. Such heterogeneity can have a negative impact on the effectiveness of trajectory similarity measures, which are the basis of many crucial trajectory processing tasks. In this paper, we pioneer a systematic approach to trajectory calibration that is a process to transform a heterogeneous trajectory dataset to one with (almost) unified sampling strategies. Specifically, we propose an anchor-based calibration system that aligns trajectories to a set of anchor points, which are fixed locations independent of trajectory data. After examining four different types of anchor points for the purpose of building a stable reference system, we propose a spatial-only geometry-based calibration approach that considers the spatial relationship between anchor points and trajectories. Then a more advanced spatial-only model-based calibration method is presented, which exploits the power of machine learning techniques to train inference models from historical trajectory data to improve calibration effectiveness. Afterward, since trajectory has temporal information, we extend these two spatial-only trajectory calibration algorithms to incorporate the temporal information, which can infer a proper time stamp to each anchor point of a calibrated trajectory. At last, we provide a solution to reduce cost, i.e., the number of trajectories that is necessary to be re-calibrated, of the updating of the reference system. Finally, we conduct extensive experiments using real trajectory datasets to demonstrate the effectiveness and efficiency of the proposed calibration system.'),\n Row(id='111', title='Windowed pq-grams for approximate joins of data-centric XML.', venue='vldb', year=2012, authors=['Nikolaus Augsten', 'Michael H. B&ouml'], abstract='In data integration applications, a join matches elements that are common to two data sources. Since elements are represented slightly different in each source, an approximate join must be used to do the matching. For XML data, most existing approximate join strategies are based on some ordered tree matching technique, such as the tree edit distance. In data-centric XML, however, the sibling order is irrelevant, and two elements should match even if their subelement order varies. Thus, approximate joins for data-centric XML must leverage unordered tree matching techniques. This is computationally hard since the algorithms cannot rely on a predefined sibling order. In this paper, we give a solution for approximate joins based on unordered tree matching. The core of our solution are windowed pq-grams which are small subtrees of a specific shape. We develop an efficient technique to generate windowed pq-grams in a three-step process: sort the tree, extend the sorted tree with dummy nodes, and decompose the extended tree into windowed pq-grams. The windowed pq-grams distance between two trees is the number of pq-grams that are in one tree decomposition only. We show that our distance is a pseudo-metric and empirically demonstrate that it effectively approximates the unordered tree edit distance. The approximate join using windowed pq-grams can be efficiently implemented as an equality join on strings, which avoids the costly computation of the distance between every pair of input trees. Experiments with synthetic and real world data confirm the analytic results and show the effectiveness and efficiency of our technique.'),\n Row(id='112', title='Sorting networks on FPGAs.', venue='vldb', year=2012, authors=['Ren&eacute', 'M&uuml'], abstract='Computer architectures are quickly changing toward heterogeneous many-core systems. Such a trend opens up interesting opportunities but also raises immense challenges since the efficient use of heterogeneous many-core systems is not a trivial problem. Software-configurable microprocessors and FPGAs add further diversity but also increase complexity. In this paper, we explore the use of sorting networks on field-programmable gate arrays (FPGAs). FPGAs are very versatile in terms of how they can be used and can also be added as additional processing units in standard CPU sockets. Our results indicate that efficient usage of FPGAs involves non-trivial aspects such as having the right computation model (a sorting network in this case); a careful implementation that balances all the design constraints in an FPGA; and the proper integration strategy to link the FPGA to the rest of the system. Once these issues are properly addressed, our experiments show that FPGAs exhibit performance figures competitive with those of modern general-purpose CPUs while offering significant advantages in terms of power consumption and parallel stream evaluation.'),\n Row(id='115', title='Query reverse engineering.', venue='vldb', year=2014, authors=['Quoc Trung Tran', 'Chee Yong Chan'], abstract=\"In this paper, we introduce a new problem termed query reverse engineering (QRE). Given a database \\\\(D\\\\) and a result table \\\\(T\\\\)—the output of some known or unknown query \\\\(Q\\\\) on \\\\(D\\\\)—the goal of QRE is to reverse-engineer a query \\\\(Q'\\\\) such that the output of query \\\\(Q'\\\\) on database \\\\(D\\\\) (denoted by \\\\(Q'(D)\\\\)) is equal to \\\\(T\\\\) (i.e., \\\\(Q(D)\\\\)). The QRE problem has useful applications in database usability, data analysis, and data security. In this work, we propose a data-driven approach, TALOS for Tree-based classifier with At Least One Semantics, that is based on a novel dynamic data classification formulation and extend the approach to efficiently support the three key dimensions of the QRE problem: whether the input query is known/unknown, supporting different query fragments, and supporting multiple database versions.\"),\n Row(id='116', title='Outsourcing shortest distance computing with privacy protection.', venue='vldb', year=2013, authors=['Jun Gao', 'Jeffrey Xu Yu'], abstract='With the advent of cloud computing, it becomes desirable to outsource graphs into cloud servers to efficiently perform complex operations without compromising their sensitive information. In this paper, we take the shortest distance computation as a case to investigate the technique issues in outsourcing graph operations. We first propose a parameter-free, edge-based 2-HOP delegation security model (shorten as 2-HOP delegation model), which can greatly reduce the chances of the structural pattern attack and the graph reconstruction attack. We then transform the original graph into a link graph \\\\(G_l\\\\) kept locally and a set of outsourced graphs \\\\(\\\\mathcal G _o\\\\). Our objectives include (i) ensuring each outsourced graph meeting the requirement of 2-HOP delegation model, (ii) making shortest distance queries be answered using \\\\(G_l\\\\) and \\\\(\\\\mathcal G _o\\\\), (iii) minimizing the space cost of \\\\(G_l\\\\). We devise a greedy method to produce \\\\(G_l\\\\) and \\\\(\\\\mathcal G _o\\\\), which can exactly answer shortest distance queries. We also develop an efficient transformation method to support approximate shortest distance answering under a given average additive error bound. The experimental results illustrate the effectiveness and efficiency of our method.'),\n Row(id='118', title='High efficiency and quality: large graphs matching.', venue='vldb', year=2013, authors=['Yuanyuan Zhu', 'Lu Qin'], abstract='Graph matching plays an essential role in many real applications. In this paper, we study how to match two large graphs by maximizing the number of matched edges, which is known as maximum common subgraph matching and is NP-hard. To find exact matching, it cannot a graph with more than 30 nodes. To find an approximate matching, the quality can be very poor. We propose a novel two-step approach that can efficiently match two large graphs over thousands of nodes with high matching quality. In the first step, we propose an anchor-selection/expansion approach to compute a good initial matching. In the second step, we propose a new approach to refine the initial matching. We give the optimality of our refinement and discuss how to randomly refine the matching with different combinations. We further show how to extend our solution to handle labeled graphs. We conducted extensive testing using real and synthetic datasets and report our findings in this paper.'),\n Row(id='120', title='Efficient processing of $$k$$ k -hop reachability queries.', venue='vldb', year=2014, authors=['James Cheng', 'Zechao Shang'], abstract='We study the problem of answering k-hop reachability queries in a directed graph, i.e., whether there exists a directed path of length \\\\(k\\\\), from a source query vertex to a target query vertex in the input graph. The problem of \\\\(k\\\\)-hop reachability is a general problem of the classic reachability (where \\\\(k=\\\\infty \\\\)). Existing indexes for processing classic reachability queries, as well as for processing shortest path distance queries, are not applicable or not efficient for processing \\\\(k\\\\)-hop reachability queries. We propose an efficient index for processing \\\\(k\\\\)-hop reachability queries. Our experimental results on a wide range of real datasets show that our method is efficient and scalable in terms of both index construction and query processing.')]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField('id', T.StringType(), True),\n",
    "    T.StructField('title', T.StringType(), True),\n",
    "    T.StructField('venue', T.StringType(), True),\n",
    "    T.StructField('year', T.IntegerType(), True),\n",
    "    T.StructField('authors', T.StringType(), True),\n",
    "    T.StructField('abstract', T.StringType(), True),\n",
    "])\n",
    "\n",
    "df = (\n",
    "    spark.read.csv(DATASET.raw_str('author.txt'), schema=schema, sep='\\t\\t\\t')\n",
    "        .withColumn('authors', F.transform(F.split(F.col('authors'), ';'), F.trim))\n",
    ")\n",
    "\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5162\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(name='Abdul Quamar', id='Abdul Quamar'),\n Row(name='Karen Stepanyan', id='Karen Stepanyan'),\n Row(name='Wang-Pin Hsiung', id='Wang-Pin Hsiung'),\n Row(name='Hailong Sun', id='Hailong Sun'),\n Row(name='James Bailey', id='James Bailey')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_authors = (\n",
    "    df.select(\n",
    "        F.explode(F.col('authors')).alias('name'),\n",
    "    ).withColumn('id', F.col('name')).distinct()\n",
    ")\n",
    "print(df_nodes_authors.count())\n",
    "df_nodes_authors.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(id='vldb', name='vldb'),\n Row(id='www', name='www'),\n Row(id='icde', name='icde'),\n Row(id='nips', name='nips'),\n Row(id='icdm', name='icdm')]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_venues = (\n",
    "    df.select(\n",
    "        F.col('venue').alias('id'),\n",
    "        F.col('venue').alias('name'),\n",
    "    ).filter(\"id != ''\").distinct()\n",
    ")\n",
    "print(df_nodes_venues.count())\n",
    "df_nodes_venues.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|2012|  844|\n",
      "|2013| 1370|\n",
      "|2014| 1336|\n",
      "|2015| 1562|\n",
      "|2016|  404|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('year').count().orderBy('year').show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(id='840', title='Layered processing of skyline-window-join (SWJ) queries using iteration-fabric.', authors=['Mithila Nagendra', 'K. Sel&ccedil'], venue='icde', year=2013, abstract='The problem of finding interesting tuples in a data set, more commonly known as the skyline problem, has been extensively studied in scenarios where the data is static. More recently, skyline research has moved towards data streaming environments, where tuples arrive/expire in a continuous manner. Several algorithms have been developed to track skyline changes over sliding windows; however, existing methods focus on skyline analysis in which all required skyline attributes belong to a single incoming data stream. This constraint renders current algorithms unsuitable for applications that require a real-time “join” operation to be carried out between multiple incoming data streams, arriving from different sources, before the skyline query can be answered. Based on this motivation, in this paper, we address the problem of computing skyline-window-join (SWJ) queries over pairs of data streams, considering sliding windows that take into account only the most recent tuples. In particular, we propose a Layered Skyline-window-Join (LSJ) operator that (a) partitions the overall process into processing layers and (b) maintains skyline-join results in an incremental manner by continuously monitoring the changes in all layers of the process. We combine the advantages of existing skyline methods (including those that efficiently maintain skyline results over a single stream, and those that compute the skyline of pairs of static data sets) to develop a novel iteration-fabric skyline-window-join processing structure. Using the iteration-fabric, LSJ eliminates redundant work across consecutive windows by leveraging shared data across all iteration layers of the windowed skyline-join processing. To the best of our knowledge, this is the first paper that addresses join-based skyline queries over sliding windows. Extensive experimental evaluations over real and simulated data show that LSJ provides large gains over naive extensions of existing schemes which are not designed to eliminate redundant work across multiple processing layers.', timestamp=datetime.datetime(2013, 1, 1, 0, 0)),\n Row(id='948', title='Shallow Information Extraction for the knowledge Web.', authors=['Denilson Barbosa', 'Haixun Wang'], venue='icde', year=2013, abstract='A new breed of Information Extraction tools has become popular and shown to be very effective in building massive-scale knowledge bases that fuel applications such as question answering and semantic search. These approaches rely on Web-scale probabilistic models populated through shallow language processing of the text, pre-existing knowledge, and structured data already on the Web. This tutorial provides an introduction to these techniques, starting from the foundations of information extraction, and covering some of its key applications.', timestamp=datetime.datetime(2013, 1, 1, 0, 0)),\n Row(id='1359', title='Performance of Serializable Snapshot Isolation on Multicore Servers.', authors=['Hyungsoo Jung', 'Hyuck Han'], venue='dasfaa', year=2013, abstract='Snapshot isolation (SI) is a widely studied concurrency control approach, with great impact in practice within platforms such as Oracle or SQL Server. Berenson et al. showed though that SI does not guarantee serializable execution; in certain situations, data consistency can be violated through concurrency between correct applications. Recently, variants of SI have been proposed, that keep the key properties such as (often) allowing concurrency between reads and updates, and that also guarantee that every execution will be serializable. We have had the opportunity to use three implementations of two different algorithms of this type, all based on the InnoDB open source infrastructure. We measure the performance attained by these implementations, on high-end hardware with a substantial number of cores. We explore the impact of the differences in algorithm, and also of the low-level implementation decisions.', timestamp=datetime.datetime(2013, 1, 1, 0, 0)),\n Row(id='1756', title='Probabilistic Programming in Anglican.', authors=['David Tolpin', 'Jan-Willem van de Meent'], venue='pkdd', year=2015, abstract='Anglican is a probabilistic programming system designed to interoperate with Clojure and other JVM languages. We describe the implementation of Anglican and illustrate how its design facilitates both explorative and industrial use of probabilistic programming.', timestamp=datetime.datetime(2015, 1, 1, 0, 0)),\n Row(id='1797', title='CardioWheel: ECG Biometrics on the Steering Wheel.', authors=['Andr&eacute', 'Louren&ccedil'], venue='pkdd', year=2015, abstract='Monitoring physiological signals while driving is a recent trend in the automotive industry. We present CardioWheel, a state-of-the-art machine learning solution for driver biometrics based on electrocardiographic signals (ECG). The presented system pervasively acquires heart signals from the users hands through sensors embedded in the steering wheel, to recognize the driver’s identity. It combines unsupervised and supervised machine learning algorithms, and is being tested in real-world scenarios, illustrating one of the potential uses of this technology.', timestamp=datetime.datetime(2015, 1, 1, 0, 0))]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_papers = (\n",
    "    df.filter('year > 1900').select(\n",
    "        F.col('id').alias('id'),\n",
    "        'title',\n",
    "        'authors',\n",
    "        'venue',\n",
    "        'year',\n",
    "        'abstract',\n",
    "        F.to_timestamp(F.col('year').cast('string'), 'yyyy').alias('timestamp')\n",
    "    ).distinct()\n",
    ")\n",
    "print(df_nodes_papers.count())\n",
    "df_nodes_papers.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df_node_ids = (\n",
    "    df_nodes_authors.select('id')\n",
    "        .union(df_nodes_venues.select('id'))\n",
    "        .union(df_nodes_papers.select('id'))\n",
    "        .distinct()\n",
    ")\n",
    "\n",
    "\n",
    "def filter_node_ids(df):\n",
    "    return df.join(\n",
    "        df_node_ids,\n",
    "        F.col('src') == F.col('id'),\n",
    "        'inner'\n",
    "    ).drop(\n",
    "        'id'\n",
    "    ).join(\n",
    "        df_node_ids,\n",
    "        F.col('dst') == F.col('id'),\n",
    "        'inner'\n",
    "    ).drop('id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(src='Ashish Sabharwal', dst='16892'),\n Row(src='Ashish Sabharwal', dst='5431'),\n Row(src='Carsten Lutz', dst='17169'),\n Row(src='Carsten Lutz', dst='16733'),\n Row(src='Carsten Lutz', dst='16751')]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_edges_authored = filter_node_ids(\n",
    "    df.select(\n",
    "        F.explode(F.col('authors')).alias('src'),\n",
    "        F.col('id').alias('dst'),\n",
    "    ).distinct()\n",
    ")\n",
    "print(df_edges_authored.count())\n",
    "df_edges_authored.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5511\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(src='11332', dst='icml'),\n Row(src='4032', dst='aaai'),\n Row(src='9586', dst='nips'),\n Row(src='9993', dst='nips'),\n Row(src='11078', dst='sdm')]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_edges_published_in = filter_node_ids(\n",
    "    df.select(\n",
    "        F.col('id').alias('src'),\n",
    "        F.col('venue').alias('dst'),\n",
    "    ).filter(\"dst != ''\").distinct()\n",
    ")\n",
    "print(df_edges_published_in.count())\n",
    "df_edges_published_in.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_nodes_authors.write.parquet(DATASET.processed_str('nodes_Author'), mode='overwrite')\n",
    "df_nodes_venues.write.parquet(DATASET.processed_str('nodes_Venue'), mode='overwrite')\n",
    "df_nodes_papers.write.parquet(DATASET.processed_str('nodes_Paper'), mode='overwrite')\n",
    "\n",
    "df_edges_authored.write.parquet(DATASET.processed_str('edges_AUTHORED'), mode='overwrite')\n",
    "df_edges_published_in.write.parquet(DATASET.processed_str('edges_PUBLISHED_IN'), mode='overwrite')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "GraphSchema(_path=PosixPath('/dd_volume/Development/Python/Thesis/code/datasets/data/processed/DBLP-HCN'), nodes={'Author': NodeSchema(_type='Author', _schema=..., label='name', properties={'name': GraphProperty(_name='name', dtype=DType(atomic=<DTypeAtomic.STRING: 'string'>, array=False)), 'id': GraphProperty(_name='id', dtype=DType(atomic=<DTypeAtomic.STRING: 'string'>, array=False))}, dynamic=None), 'Venue': NodeSchema(_type='Venue', _schema=..., label='name', properties={'id': GraphProperty(_name='id', dtype=DType(atomic=<DTypeAtomic.STRING: 'string'>, array=False)), 'name': GraphProperty(_name='name', dtype=DType(atomic=<DTypeAtomic.STRING: 'string'>, array=False))}, dynamic=None), 'Paper': NodeSchema(_type='Paper', _schema=..., label='title', properties={'id': GraphProperty(_name='id', dtype=DType(atomic=<DTypeAtomic.STRING: 'string'>, array=False)), 'title': GraphProperty(_name='title', dtype=DType(atomic=<DTypeAtomic.STRING: 'string'>, array=False)), 'authors': GraphProperty(_name='authors', dtype=DType(atomic=<DTypeAtomic.STRING: 'string'>, array=True)), 'venue': GraphProperty(_name='venue', dtype=DType(atomic=<DTypeAtomic.STRING: 'string'>, array=False)), 'year': GraphProperty(_name='year', dtype=DType(atomic=<DTypeAtomic.INT: 'int'>, array=False)), 'abstract': GraphProperty(_name='abstract', dtype=DType(atomic=<DTypeAtomic.STRING: 'string'>, array=False)), 'timestamp': GraphProperty(_name='timestamp', dtype=DType(atomic=<DTypeAtomic.DATETIME: 'datetime'>, array=False))}, dynamic=DynamicConfig(timestamp='timestamp', interaction=False))}, edges={'AUTHORED': EdgeSchema(_type='AUTHORED', _schema=..., label=None, properties={'src': GraphProperty(_name='src', dtype=DType(atomic=<DTypeAtomic.STRING: 'string'>, array=False)), 'dst': GraphProperty(_name='dst', dtype=DType(atomic=<DTypeAtomic.STRING: 'string'>, array=False))}, dynamic=None, source_type='Author', target_type='Paper', directed=True), 'PUBLISHED_IN': EdgeSchema(_type='PUBLISHED_IN', _schema=..., label=None, properties={'src': GraphProperty(_name='src', dtype=DType(atomic=<DTypeAtomic.STRING: 'string'>, array=False)), 'dst': GraphProperty(_name='dst', dtype=DType(atomic=<DTypeAtomic.STRING: 'string'>, array=False))}, dynamic=None, source_type='Paper', target_type='Venue', directed=True)})"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shared.schema.graph import GraphSchema, NodeSchema, EdgeSchema\n",
    "\n",
    "(\n",
    "    GraphSchema()\n",
    "        .add_node_schema('Author', NodeSchema.from_spark(df_nodes_authors.schema, label='name'))\n",
    "        .add_node_schema('Venue', NodeSchema.from_spark(df_nodes_venues.schema, label='name'))\n",
    "        .add_node_schema('Paper', NodeSchema.from_spark(df_nodes_papers.schema, label='title', timestamp='timestamp', interaction=False))\n",
    "        .add_edge_schema('AUTHORED', EdgeSchema.from_spark(df_edges_authored.schema, source_type='Author', target_type='Paper', directed=True))\n",
    "        .add_edge_schema('PUBLISHED_IN', EdgeSchema.from_spark(df_edges_published_in.schema, source_type='Paper', target_type='Venue', directed=True))\n",
    "        .save_schema(DATASET.processed())\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ground truth communities\n",
    "Using same methodology as in:\n",
    "\n",
    "J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth,” in Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics, New York, NY, USA, Aug. 2012, pp. 1–8. doi: 10.1145/2350190.2350193.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(venue='icde', count=299), Row(venue='pakdd', count=210), Row(venue='aaai', count=1041), Row(venue='ecir', count=198), Row(venue='dasfaa', count=189), Row(venue='icdm', count=474), Row(venue='cvpr', count=898), Row(venue='vldb', count=94), Row(venue='icml', count=462), Row(venue='nips', count=689), Row(venue='pkdd', count=240), Row(venue='sdm', count=192), Row(venue='ijcai', count=461), Row(venue='www', count=64)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(df_nodes_papers.groupby('venue').count().head(100))\n",
    "print(df_nodes_papers.filter(F.col('venue').isNull()).head(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(id='vldb', cid='vldb'),\n Row(id='www', cid='www'),\n Row(id='icde', cid='icde'),\n Row(id='nips', cid='nips'),\n Row(id='icdm', cid='icdm')]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_venues_comm = (\n",
    "    df_nodes_venues\n",
    "        .select(\n",
    "        F.col('id').alias('id'),\n",
    "        F.col('id').alias('cid'),\n",
    "    )\n",
    "        .distinct()\n",
    ")\n",
    "df_nodes_venues_comm.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(id='498', cid='icde'),\n Row(id='880', cid='icde'),\n Row(id='1060', cid='icde'),\n Row(id='1431', cid='dasfaa'),\n Row(id='1579', cid='pkdd')]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_papers_comm = (\n",
    "    df_nodes_papers\n",
    "        .select(\n",
    "        F.col('id').alias('id'),\n",
    "        F.col('venue').alias('cid'),\n",
    "    )\n",
    "        .distinct()\n",
    ")\n",
    "df_nodes_papers_comm.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(id='Volkan Cevher', cid='nips'),\n Row(id='Viliam Lis&yacute', cid='nips'),\n Row(id='Jacob R. Gardner', cid='nips'),\n Row(id='Vuk Ercegovac', cid='icde'),\n Row(id='Fredrik Lindsten', cid='nips')]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_authors_comm = (\n",
    "    df_nodes_authors\n",
    "        .join(df_edges_authored, F.col('id') == F.col('src'), 'left')\n",
    "        .drop('src')\n",
    "        .withColumnRenamed('dst', 'paper_id')\n",
    "        .join(df_edges_published_in, F.col('paper_id') == F.col('src'), 'left')\n",
    "        .drop('src')\n",
    "        .withColumnRenamed('dst', 'cid')\n",
    "        .select(\n",
    "        F.col('id').alias('id'),\n",
    "        F.col('cid').alias('cid'),\n",
    "    )\n",
    "        .distinct()\n",
    ")\n",
    "df_nodes_authors_comm.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(id='Volkan Cevher', cid='nips'),\n Row(id='Viliam Lis&yacute', cid='nips'),\n Row(id='Jacob R. Gardner', cid='nips'),\n Row(id='Vuk Ercegovac', cid='icde'),\n Row(id='Fredrik Lindsten', cid='nips')]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_comm = (\n",
    "    df_nodes_authors_comm\n",
    "        .union(df_nodes_papers_comm)\n",
    "        .union(df_nodes_venues_comm)\n",
    "        .distinct()\n",
    ")\n",
    "df_nodes_comm.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_nodes_comm.coalesce(1).write.csv(\n",
    "    DATASET.processed_str('ground_truth.comlist.tmp'),\n",
    "    sep='\\t',\n",
    "    mode='overwrite',\n",
    "    header=None,\n",
    "    quoteAll=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.move(\n",
    "    str(next(DATASET.processed('ground_truth.comlist.tmp').glob('*.csv'))),\n",
    "    DATASET.processed_str('ground_truth.ncomlist')\n",
    ")\n",
    "\n",
    "shutil.rmtree(DATASET.processed_str('ground_truth.comlist.tmp'), ignore_errors=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}