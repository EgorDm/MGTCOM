{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from shared.constants import DatasetPath"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "DATASET = DatasetPath('DBLP-HCN')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/21 23:58:07 WARN Utils: Your hostname, megatron resolves to a loopback address: 127.0.1.1; using 192.168.1.89 instead (on interface enp7s0)\n",
      "22/01/21 23:58:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/21 23:58:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/01/21 23:58:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/01/21 23:58:08 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/01/21 23:58:08 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(f'{DATASET}')\n",
    "         .config('spark.sql.legacy.timeParserPolicy', 'LEGACY')\n",
    "         .config(\"spark.executor.memory\", \"8g\")\n",
    "         .config(\"spark.driver.memory\", \"8g\")\n",
    "         .config(\"spark.memory.offHeap.enabled\", True)\n",
    "         .config(\"spark.memory.offHeap.size\", \"16g\")\n",
    "         .getOrCreate())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(id=104, title='SWORD: workload-aware data placement and replica selection for cloud data management systems.', venue='vldb', year=2014, authors=['K. Ashwin Kumar', 'Abdul Quamar'], abstract='Cloud computing is increasingly being seen as a way to reduce infrastructure costs and add elasticity, and is being used by a wide range of organizations. Cloud data management systems today need to serve a range of different workloads, from analytical read-heavy workloads to transactional (OLTP) workloads. For both the service providers and the users, it is critical to minimize the consumption of resources like CPU, memory, communication bandwidth, and energy, without compromising on service-level agreements if any. In this article, we develop a workload-aware data placement and replication approach, called SWORD, for minimizing resource consumption in such an environment. Specifically, we monitor and model the expected workload as a hypergraph and develop partitioning techniques that minimize the average query span, i.e., the average number of machines involved in the execution of a query or a transaction. We empirically justify the use of query span as the metric to optimize, for both analytical and transactional workloads, and develop a series of replication and data placement algorithms by drawing connections to several well-studied graph theoretic concepts. We introduce a suite of novel techniques to achieve high scalability by reducing the overhead of partitioning and query routing. To deal with workload changes, we propose an incremental repartitioning technique that modifies data placement in small steps without resorting to complete repartitioning. We propose the use of fine-grained quorums defined at the level of groups of data items to control the cost of distributed updates, improve throughput, and adapt to different workloads. We empirically illustrate the benefits of our approach through a comprehensive experimental evaluation for two classes of workloads. For analytical read-only workloads, we show that our techniques result in significant reduction in total resource consumption. For OLTP workloads, we show that our approach improves transaction latencies and overall throughput by minimizing the number of distributed transactions.'),\n Row(id=106, title='A survey of large-scale analytical query processing in MapReduce.', venue='vldb', year=2014, authors=['Christos Doulkeridis', 'Kjetil N&oslash'], abstract='Enterprises today acquire vast volumes of data from different sources and leverage this information by means of data analysis to support effective decision-making and provide new functionality and services. The key requirement of data analytics is scalability, simply due to the immense volume of data that need to be extracted, processed, and analyzed in a timely fashion. Arguably the most popular framework for contemporary large-scale data analytics is MapReduce, mainly due to its salient features that include scalability, fault-tolerance, ease of programming, and flexibility. However, despite its merits, MapReduce has evident performance limitations in miscellaneous analytical tasks, and this has given rise to a significant body of research that aim at improving its efficiency, while maintaining its desirable properties. This survey aims to review the state of the art in improving the performance of parallel query processing using MapReduce. A set of the most significant weaknesses and limitations of MapReduce is discussed at a high level, along with solving techniques. A taxonomy is presented for categorizing existing research on MapReduce improvements according to the specific problem they target. Based on the proposed taxonomy, a classification of existing research is provided focusing on the optimization objective. Concluding, we outline interesting directions for future parallel data processing systems.'),\n Row(id=107, title='Taking the Big Picture: representative skylines based on significance and diversity.', venue='vldb', year=2014, authors=['Matteo Magnani', 'Ira Assent'], abstract='The skyline is a popular operator to extract records from a database when a record scoring function is not available. However, the result of a skyline query can be very large. The problem addressed in this paper is the automatic selection of a small number \\\\((k)\\\\) of representative skyline records. Existing approaches have only focused on partial aspects of this problem. Some try to identify sets of diverse records giving an overall approximation of the skyline. These techniques, however, are sensitive to the scaling of attributes or to the insertion of non-skyline records into the database. Others exploit some knowledge of the record scoring function to identify the most significant record, but not sets of records representative of the whole skyline. In this paper, we introduce a novel approach taking both the significance of all the records and their diversity into account, adapting to available knowledge of the scoring function, but also working under complete ignorance. We show the intractability of the problem and present approximate algorithms. We experimentally show that our approach is efficient, scalable and that it improves existing works in terms of the significance and diversity of the results.'),\n Row(id=108, title='Calibrating trajectory data for spatio-temporal similarity analysis.', venue='vldb', year=2015, authors=['Han Su', 'Kai Zheng'], abstract='Due to the prevalence of GPS-enabled devices and wireless communications technologies, spatial trajectories that describe the movement history of moving objects are being generated and accumulated at an unprecedented pace. Trajectory data in a database are intrinsically heterogeneous, as they represent discrete approximations of original continuous paths derived using different sampling strategies and different sampling rates. Such heterogeneity can have a negative impact on the effectiveness of trajectory similarity measures, which are the basis of many crucial trajectory processing tasks. In this paper, we pioneer a systematic approach to trajectory calibration that is a process to transform a heterogeneous trajectory dataset to one with (almost) unified sampling strategies. Specifically, we propose an anchor-based calibration system that aligns trajectories to a set of anchor points, which are fixed locations independent of trajectory data. After examining four different types of anchor points for the purpose of building a stable reference system, we propose a spatial-only geometry-based calibration approach that considers the spatial relationship between anchor points and trajectories. Then a more advanced spatial-only model-based calibration method is presented, which exploits the power of machine learning techniques to train inference models from historical trajectory data to improve calibration effectiveness. Afterward, since trajectory has temporal information, we extend these two spatial-only trajectory calibration algorithms to incorporate the temporal information, which can infer a proper time stamp to each anchor point of a calibrated trajectory. At last, we provide a solution to reduce cost, i.e., the number of trajectories that is necessary to be re-calibrated, of the updating of the reference system. Finally, we conduct extensive experiments using real trajectory datasets to demonstrate the effectiveness and efficiency of the proposed calibration system.'),\n Row(id=111, title='Windowed pq-grams for approximate joins of data-centric XML.', venue='vldb', year=2012, authors=['Nikolaus Augsten', 'Michael H. B&ouml'], abstract='In data integration applications, a join matches elements that are common to two data sources. Since elements are represented slightly different in each source, an approximate join must be used to do the matching. For XML data, most existing approximate join strategies are based on some ordered tree matching technique, such as the tree edit distance. In data-centric XML, however, the sibling order is irrelevant, and two elements should match even if their subelement order varies. Thus, approximate joins for data-centric XML must leverage unordered tree matching techniques. This is computationally hard since the algorithms cannot rely on a predefined sibling order. In this paper, we give a solution for approximate joins based on unordered tree matching. The core of our solution are windowed pq-grams which are small subtrees of a specific shape. We develop an efficient technique to generate windowed pq-grams in a three-step process: sort the tree, extend the sorted tree with dummy nodes, and decompose the extended tree into windowed pq-grams. The windowed pq-grams distance between two trees is the number of pq-grams that are in one tree decomposition only. We show that our distance is a pseudo-metric and empirically demonstrate that it effectively approximates the unordered tree edit distance. The approximate join using windowed pq-grams can be efficiently implemented as an equality join on strings, which avoids the costly computation of the distance between every pair of input trees. Experiments with synthetic and real world data confirm the analytic results and show the effectiveness and efficiency of our technique.'),\n Row(id=112, title='Sorting networks on FPGAs.', venue='vldb', year=2012, authors=['Ren&eacute', 'M&uuml'], abstract='Computer architectures are quickly changing toward heterogeneous many-core systems. Such a trend opens up interesting opportunities but also raises immense challenges since the efficient use of heterogeneous many-core systems is not a trivial problem. Software-configurable microprocessors and FPGAs add further diversity but also increase complexity. In this paper, we explore the use of sorting networks on field-programmable gate arrays (FPGAs). FPGAs are very versatile in terms of how they can be used and can also be added as additional processing units in standard CPU sockets. Our results indicate that efficient usage of FPGAs involves non-trivial aspects such as having the right computation model (a sorting network in this case); a careful implementation that balances all the design constraints in an FPGA; and the proper integration strategy to link the FPGA to the rest of the system. Once these issues are properly addressed, our experiments show that FPGAs exhibit performance figures competitive with those of modern general-purpose CPUs while offering significant advantages in terms of power consumption and parallel stream evaluation.'),\n Row(id=115, title='Query reverse engineering.', venue='vldb', year=2014, authors=['Quoc Trung Tran', 'Chee Yong Chan'], abstract=\"In this paper, we introduce a new problem termed query reverse engineering (QRE). Given a database \\\\(D\\\\) and a result table \\\\(T\\\\)—the output of some known or unknown query \\\\(Q\\\\) on \\\\(D\\\\)—the goal of QRE is to reverse-engineer a query \\\\(Q'\\\\) such that the output of query \\\\(Q'\\\\) on database \\\\(D\\\\) (denoted by \\\\(Q'(D)\\\\)) is equal to \\\\(T\\\\) (i.e., \\\\(Q(D)\\\\)). The QRE problem has useful applications in database usability, data analysis, and data security. In this work, we propose a data-driven approach, TALOS for Tree-based classifier with At Least One Semantics, that is based on a novel dynamic data classification formulation and extend the approach to efficiently support the three key dimensions of the QRE problem: whether the input query is known/unknown, supporting different query fragments, and supporting multiple database versions.\"),\n Row(id=116, title='Outsourcing shortest distance computing with privacy protection.', venue='vldb', year=2013, authors=['Jun Gao', 'Jeffrey Xu Yu'], abstract='With the advent of cloud computing, it becomes desirable to outsource graphs into cloud servers to efficiently perform complex operations without compromising their sensitive information. In this paper, we take the shortest distance computation as a case to investigate the technique issues in outsourcing graph operations. We first propose a parameter-free, edge-based 2-HOP delegation security model (shorten as 2-HOP delegation model), which can greatly reduce the chances of the structural pattern attack and the graph reconstruction attack. We then transform the original graph into a link graph \\\\(G_l\\\\) kept locally and a set of outsourced graphs \\\\(\\\\mathcal G _o\\\\). Our objectives include (i) ensuring each outsourced graph meeting the requirement of 2-HOP delegation model, (ii) making shortest distance queries be answered using \\\\(G_l\\\\) and \\\\(\\\\mathcal G _o\\\\), (iii) minimizing the space cost of \\\\(G_l\\\\). We devise a greedy method to produce \\\\(G_l\\\\) and \\\\(\\\\mathcal G _o\\\\), which can exactly answer shortest distance queries. We also develop an efficient transformation method to support approximate shortest distance answering under a given average additive error bound. The experimental results illustrate the effectiveness and efficiency of our method.'),\n Row(id=118, title='High efficiency and quality: large graphs matching.', venue='vldb', year=2013, authors=['Yuanyuan Zhu', 'Lu Qin'], abstract='Graph matching plays an essential role in many real applications. In this paper, we study how to match two large graphs by maximizing the number of matched edges, which is known as maximum common subgraph matching and is NP-hard. To find exact matching, it cannot a graph with more than 30 nodes. To find an approximate matching, the quality can be very poor. We propose a novel two-step approach that can efficiently match two large graphs over thousands of nodes with high matching quality. In the first step, we propose an anchor-selection/expansion approach to compute a good initial matching. In the second step, we propose a new approach to refine the initial matching. We give the optimality of our refinement and discuss how to randomly refine the matching with different combinations. We further show how to extend our solution to handle labeled graphs. We conducted extensive testing using real and synthetic datasets and report our findings in this paper.'),\n Row(id=120, title='Efficient processing of $$k$$ k -hop reachability queries.', venue='vldb', year=2014, authors=['James Cheng', 'Zechao Shang'], abstract='We study the problem of answering k-hop reachability queries in a directed graph, i.e., whether there exists a directed path of length \\\\(k\\\\), from a source query vertex to a target query vertex in the input graph. The problem of \\\\(k\\\\)-hop reachability is a general problem of the classic reachability (where \\\\(k=\\\\infty \\\\)). Existing indexes for processing classic reachability queries, as well as for processing shortest path distance queries, are not applicable or not efficient for processing \\\\(k\\\\)-hop reachability queries. We propose an efficient index for processing \\\\(k\\\\)-hop reachability queries. Our experimental results on a wide range of real datasets show that our method is efficient and scalable in terms of both index construction and query processing.')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField('id', T.IntegerType(), True),\n",
    "    T.StructField('title', T.StringType(), True),\n",
    "    T.StructField('venue', T.StringType(), True),\n",
    "    T.StructField('year', T.IntegerType(), True),\n",
    "    T.StructField('authors', T.StringType(), True),\n",
    "    T.StructField('abstract', T.StringType(), True),\n",
    "])\n",
    "\n",
    "df = (\n",
    "    spark.read.csv(DATASET.raw_str('author.txt'), schema=schema, sep='\\t\\t\\t')\n",
    "        .withColumn('authors', F.transform(F.split(F.col('authors'), ';'), F.trim))\n",
    ")\n",
    "\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5162\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(name='Abdul Quamar', id='Abdul Quamar'),\n Row(name='Karen Stepanyan', id='Karen Stepanyan'),\n Row(name='Wang-Pin Hsiung', id='Wang-Pin Hsiung'),\n Row(name='Hailong Sun', id='Hailong Sun'),\n Row(name='James Bailey', id='James Bailey')]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_authors = (\n",
    "    df.select(\n",
    "        F.explode(F.col('authors')).alias('name'),\n",
    "    ).withColumn('id', F.col('name')).distinct()\n",
    ")\n",
    "print(df_nodes_authors.count())\n",
    "df_nodes_authors.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(id='vldb', name='vldb'),\n Row(id='www', name='www'),\n Row(id='icde', name='icde'),\n Row(id='nips', name='nips'),\n Row(id='icdm', name='icdm')]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_venues = (\n",
    "    df.select(\n",
    "        F.col('venue').alias('id'),\n",
    "        F.col('venue').alias('name'),\n",
    "    ).filter(\"id != ''\").distinct()\n",
    ")\n",
    "print(df_nodes_venues.count())\n",
    "df_nodes_venues.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|2012|  844|\n",
      "|2013| 1370|\n",
      "|2014| 1336|\n",
      "|2015| 1562|\n",
      "|2016|  404|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('year').count().orderBy('year').show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5511\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(id=841, title='Metadata-as-a-Service.', authors=['Akon Dey', 'Gajanan S. Chinchwadkar'], venue='icde', year=2015, abstract='We present a vision of a technology and domain agnostic service that will store metadata that describes properties of the diverse data sets in an enterprise (or across several enterprises), and spread among heterogenous stores, such as relational databases, data warehouses, NoSQL or NewSQL cloud storage platforms, etc. The Metadata-as-a-Service will allow search over the metadata, so users and applications can find useful data sets, whether those are raw data or derived data. We make a preliminary proposal for the high-level architecture and API of such a service.', timestamp=datetime.datetime(2015, 1, 1, 0, 0)),\n Row(id=855, title='On random walk based graph sampling.', authors=['Rong-Hua Li', 'Jeffrey Xu Yu'], venue='icde', year=2015, abstract='Random walk based graph sampling has been recognized as a fundamental technique to collect uniform node samples from a large graph. In this paper, we first present a comprehensive analysis of the drawbacks of three widely-used random walk based graph sampling algorithms, called re-weighted random walk (RW) algorithm, Metropolis-Hastings random walk (MH) algorithm and maximum-degree random walk (MD) algorithm. Then, to address the limitations of these algorithms, we propose two general random walk based algorithms, named rejection-controlled Metropolis-Hastings (RCMH) algorithm and generalized maximum-degree random walk (GMD) algorithm. We show that RCMH balances the tradeoff between the limitations of RW and MH, and GMD balances the tradeoff between the drawbacks of RW and MD. To further improve the performance of our algorithms, we integrate the so-called delayed acceptance technique and the non-backtracking random walk technique into RCMH and GMD respectively. We conduct extensive experiments over four real-world datasets, and the results demonstrate the effectiveness of the proposed algorithms.', timestamp=datetime.datetime(2015, 1, 1, 0, 0)),\n Row(id=882, title='Matching heterogeneous events with patterns.', authors=['Xiaochen Zhu', 'Shaoxu Song'], venue='icde', year=2014, abstract='A large amount of heterogeneous event data are increasingly generated, e.g., in online systems for Web services or operational systems in enterprises. Owing to the difference between event data and traditional relational data, the matching of heterogeneous events is highly non-trivial. While event names are often opaque (e.g., merely with obscure IDs), the existing structure-based matching techniques for relational data also fail to perform owing to the poor discriminative power of dependency relationships between events. We note that interesting patterns exist in the occurrence of events, which may serve as discriminative features in event matching. In this paper, we formalize the problem of matching events with patterns. A generic pattern based matching framework is proposed, which is compatible with the existing structure based techniques. To improve the matching efficiency, we devise several bounds of matching scores for pruning. Since the exploration of patterns is costly and incrementally, our proposed techniques support matching in a pay-as-you-go style, i.e., incrementally update the matching results with the increase of available patterns. Finally, extensive experiments on both real and synthetic data demonstrate the effectiveness of our pattern based matching compared with approaches adapted from existing techniques, and the efficiency improved by the bounding/pruning methods.', timestamp=datetime.datetime(2014, 1, 1, 0, 0)),\n Row(id=1133, title='AntiqueData: A Proxy to Maintain Computational Transparency in Cloud.', authors=['Himel Dev', 'Mohammed Eunus Ali'], venue='dasfaa', year=2014, abstract='Cloud computing offers computing and software services to users on an on-demand basis. It facilitates users to use computing resources as utility with pay-per-usage billing, which allows users to acquire computational resources with low or no initial cost. Due to this greater level of flexibility, the cloud has become the breeding ground of a new generation of products and services. Since more and more people rely on the cloud with their data and computing, ensuring the trustworthiness of cloud services has become a major issue for both the users and cloud providers. Due to the black box nature of cloud, there has been a lack of trust among providers and users, which has become a major barrier to the widespread growth of cloud computing. One of the trust concerns of cloud is lack of computational transparency. In particular, in current cloud architecture a provider controls all the logging and auditing records corresponding to computation and users do not have access to these records. This is a big concern for many clients of cloud. In this paper, we first identify the risks associated with lack of transparency in cloud and propose a middleware service that eliminates these risks.', timestamp=datetime.datetime(2014, 1, 1, 0, 0)),\n Row(id=1436, title='Integration of Web Sources Under Uncertainty and Dependencies Using Probabilistic XML.', authors=['Mouhamadou Lamine Ba', 'S&eacute'], venue='dasfaa', year=2014, abstract='We study in this vision paper the problem of integrating several web data sources under uncertainty and dependencies. We present a concrete application with web sources about objects in the maritime domain where uncertainties and dependencies are omnipresent. Uncertainties are mainly caused by imprecise information trackers and imperfect human knowledge. Dependencies come from the recurrent copying relationships occurring among the sources. We answer the issue of data integration in such a setting by reformulating it as the merge of several uncertain versions of the same global XML document. As an initial result, we put forward a probabilistic XML data integration model by getting some intuitions from the versioning model with uncertain data we proposed in\\xa0[5]. We explain how this model can be used for materializing the integration outcome.', timestamp=datetime.datetime(2014, 1, 1, 0, 0))]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_papers = (\n",
    "    df.filter('year > 1900').select(\n",
    "        F.col('id').alias('id'),\n",
    "        'title',\n",
    "        'authors',\n",
    "        'venue',\n",
    "        'year',\n",
    "        'abstract',\n",
    "        F.to_timestamp(F.col('year').cast('string'), 'yyyy').alias('timestamp')\n",
    "    ).distinct()\n",
    ")\n",
    "print(df_nodes_papers.count())\n",
    "df_nodes_papers.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df_node_ids = (\n",
    "    df_nodes_authors.select('id')\n",
    "        .union(df_nodes_venues.select('id'))\n",
    "        .union(df_nodes_papers.select('id'))\n",
    "        .distinct()\n",
    ")\n",
    "\n",
    "def filter_node_ids(df):\n",
    "    return df.join(\n",
    "        df_node_ids,\n",
    "        F.col('src') == F.col('id'),\n",
    "        'inner'\n",
    "    ).drop(\n",
    "        'id'\n",
    "    ).join(\n",
    "        df_node_ids,\n",
    "        F.col('dst') == F.col('id'),\n",
    "        'inner'\n",
    "    ).drop('id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11022\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(src='Ashish Sabharwal', dst=16892),\n Row(src='Ashish Sabharwal', dst=5431),\n Row(src='Carsten Lutz', dst=17169),\n Row(src='Carsten Lutz', dst=16751),\n Row(src='Carsten Lutz', dst=16733)]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_edges_authored = filter_node_ids(\n",
    "    df.select(\n",
    "        F.explode(F.col('authors')).alias('src'),\n",
    "        F.col('id').alias('dst'),\n",
    "    ).distinct()\n",
    ")\n",
    "print(df_edges_authored.count())\n",
    "df_edges_authored.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5511\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(src=1436, dst='dasfaa'),\n Row(src=11078, dst='sdm'),\n Row(src=1090, dst='icde'),\n Row(src=9993, dst='nips'),\n Row(src=3210, dst='icdm')]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_edges_published_in = filter_node_ids(\n",
    "    df.select(\n",
    "        F.col('id').alias('src'),\n",
    "        F.col('venue').alias('dst'),\n",
    "    ).filter(\"dst != ''\").distinct()\n",
    ")\n",
    "print(df_edges_published_in.count())\n",
    "df_edges_published_in.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_nodes_authors.write.parquet(DATASET.processed_str('nodes_Author'), mode='overwrite')\n",
    "df_nodes_venues.write.parquet(DATASET.processed_str('nodes_Venue'), mode='overwrite')\n",
    "df_nodes_papers.write.parquet(DATASET.processed_str('nodes_Paper'), mode='overwrite')\n",
    "\n",
    "df_edges_authored.write.parquet(DATASET.processed_str('edges_AUTHORED'), mode='overwrite')\n",
    "df_edges_published_in.write.parquet(DATASET.processed_str('edges_PUBLISHED_IN'), mode='overwrite')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-01-21 23:58:31,397][/dd_volume/Development/Python/Thesis/code/datasets/datasets/build_schema.py][DEBUG] Merging old schema for DBLP-HCN\n"
     ]
    },
    {
     "data": {
      "text/plain": "DatasetSchema(name='DBLP-HCN', prefix='DBLP_HCN', database='DBLP-HCN', description='None', nodes=[NodeSchema(path='data/processed/DBLP-HCN/nodes_Author', properties=[Property(name='name', type='string', ignore=False, label=True, timestamp=False), Property(name='id', type='string', ignore=False, label=True, timestamp=False)], label='Author', interaction=False), NodeSchema(path='data/processed/DBLP-HCN/nodes_Venue', properties=[Property(name='id', type='string', ignore=False, label=False, timestamp=False), Property(name='name', type='string', ignore=False, label=True, timestamp=False)], label='Venue', interaction=False), NodeSchema(path='data/processed/DBLP-HCN/nodes_Paper', properties=[Property(name='id', type='int', ignore=False, label=False, timestamp=False), Property(name='title', type='string', ignore=False, label=True, timestamp=False), Property(name='authors', type='string[]', ignore=False, label=False, timestamp=False), Property(name='venue', type='string', ignore=False, label=False, timestamp=False), Property(name='year', type='int', ignore=False, label=False, timestamp=False), Property(name='abstract', type='string', ignore=False, label=False, timestamp=False), Property(name='timestamp', type='datetime', ignore=False, label=False, timestamp=True)], label='Paper', interaction=False)], edges=[EdgeSchema(path='data/processed/DBLP-HCN/edges_AUTHORED', properties=[Property(name='src', type='string', ignore=False, label=False, timestamp=False), Property(name='dst', type='int', ignore=False, label=False, timestamp=False)], type='AUTHORED', source='Author', target='Paper', directed=True, interaction=False), EdgeSchema(path='data/processed/DBLP-HCN/edges_PUBLISHED_IN', properties=[Property(name='src', type='int', ignore=False, label=False, timestamp=False), Property(name='dst', type='string', ignore=False, label=False, timestamp=False)], type='PUBLISHED_IN', source='Paper', target='Venue', directed=True, interaction=False)], ground_truth=None, versions={})"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.build_schema import build_schema\n",
    "\n",
    "build_schema(\n",
    "    spark,\n",
    "    name=str(DATASET),\n",
    "    nodes=[\n",
    "        ('Author', DATASET.processed_str('nodes_Author')),\n",
    "        ('Venue', DATASET.processed_str('nodes_Venue')),\n",
    "        ('Paper', DATASET.processed_str('nodes_Paper')),\n",
    "    ],\n",
    "    edges=[\n",
    "        ('Authored', 'Author', 'Paper', DATASET.processed_str('edges_AUTHORED')),\n",
    "        ('PublishedIn', 'Paper', 'Venue', DATASET.processed_str('edges_PUBLISHED_IN')),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ground truth communities\n",
    "Using same methodology as in:\n",
    "\n",
    "J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth,” in Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics, New York, NY, USA, Aug. 2012, pp. 1–8. doi: 10.1145/2350190.2350193.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_nodes_papers.groupby('venue').count().head(100))\n",
    "print(df_nodes_papers.filter(F.col('venue').isNull()).head(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(id='vldb', cid='vldb'),\n Row(id='www', cid='www'),\n Row(id='icde', cid='icde'),\n Row(id='nips', cid='nips'),\n Row(id='icdm', cid='icdm')]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_venues_comm = (\n",
    "    df_nodes_venues\n",
    "        .select(\n",
    "            F.col('id').alias('id'),\n",
    "            F.col('id').alias('cid'),\n",
    "        )\n",
    "        .distinct()\n",
    ")\n",
    "df_nodes_venues_comm.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(id=107, cid='vldb'),\n Row(id=977, cid='icde'),\n Row(id=1179, cid='dasfaa'),\n Row(id=1465, cid='dasfaa'),\n Row(id=1794, cid='pkdd')]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_papers_comm = (\n",
    "    df_nodes_papers\n",
    "        .select(\n",
    "            F.col('id').alias('id'),\n",
    "            F.col('venue').alias('cid'),\n",
    "        )\n",
    "        .distinct()\n",
    ")\n",
    "df_nodes_papers_comm.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(id='Bei Xu', cid='ijcai'),\n Row(id='Yue Wu', cid='icml'),\n Row(id='Reshef Meir', cid='aaai'),\n Row(id='Christina Teflioudi', cid='vldb'),\n Row(id='Hong Cheng', cid='icde')]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_authors_comm = (\n",
    "    df_nodes_authors\n",
    "        .join(df_edges_authored, F.col('id') == F.col('src'), 'left')\n",
    "        .drop('src')\n",
    "        .withColumnRenamed('dst', 'paper_id')\n",
    "        .join(df_edges_published_in, F.col('paper_id') == F.col('src'), 'left')\n",
    "        .drop('src')\n",
    "        .withColumnRenamed('dst', 'cid')\n",
    "        .select(\n",
    "            F.col('id').alias('id'),\n",
    "            F.col('cid').alias('cid'),\n",
    "        )\n",
    "        .distinct()\n",
    ")\n",
    "df_nodes_authors_comm.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(id='Bei Xu', cid='ijcai'),\n Row(id='Yue Wu', cid='icml'),\n Row(id='Reshef Meir', cid='aaai'),\n Row(id='Christina Teflioudi', cid='vldb'),\n Row(id='Hong Cheng', cid='icde')]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodes_comm = (\n",
    "    df_nodes_authors_comm\n",
    "        .union(df_nodes_papers_comm)\n",
    "        .union(df_nodes_venues_comm)\n",
    "        .distinct()\n",
    ")\n",
    "df_nodes_comm.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "df_nodes_comm.coalesce(1).write.csv(\n",
    "    DATASET.processed_str('ground_truth.comlist.tmp'),\n",
    "    sep='\\t',\n",
    "    mode='overwrite',\n",
    "    header=None,\n",
    "    quoteAll=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.move(\n",
    "    str(next(DATASET.processed('ground_truth.comlist.tmp').glob('*.csv'))),\n",
    "    DATASET.processed_str('ground_truth.comlist')\n",
    ")\n",
    "\n",
    "shutil.rmtree(DATASET.processed_str('ground_truth.comlist.tmp'), ignore_errors=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from shared.constants import DATASETS_PATH\n",
    "from datasets.schema import DatasetSchema\n",
    "\n",
    "schema = DatasetSchema.load_schema(str(DATASET))\n",
    "schema.ground_truth = DATASET.processed('ground_truth.comlist').relative_to(DATASETS_PATH)\n",
    "schema.save_schema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}